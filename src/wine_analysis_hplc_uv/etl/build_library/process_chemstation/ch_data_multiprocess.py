from typing import List, Tuple

counter = None
counter_lock = None


def ch_data_multiprocess(dirpath_list: List[str]) -> Tuple[List[dict], List[dict]]:
    """
    1. Create the metadata and data dicts from each .D file.
    2. check that the hash keys are unique.
    3. return uv_metadata_list and uv_data_list
    """
    if isinstance(dirpath_list, list):
        uv_file_pool = uv_extractor_pool(dirpath_list)

        try:
            uv_metadata_list, uv_data_list = zip(*uv_file_pool)
        except TypeError as e:
            print(f"Tried to unpack uv_file_pool but {e}")
            print(f"the datatype of uv_file_pool is {type(uv_file_pool)}")
            print("the contents of uv_file_pool is:")
            [print(file) for file in uv_file_pool]
            raise TypeError

        duplicate_hash_keys(uv_metadata_list)
    else:
        print(f"dirpath_list must be list, is {type(dirpath_list)}. Exiting.")
        raise TypeError

    return uv_metadata_list, uv_data_list


def duplicate_hash_keys(uv_metadata_list: List[dict]) -> None:
    # observe how many unique hash_keys were generated. duplicates are probably caused by duplicate files/filenames.
    num_unique_hash = len(set(d["hash_key"] for d in uv_metadata_list))
    print("num unqiue hash keys", num_unique_hash)
    print("size of metadata_list", len(uv_metadata_list))

    # print the UUIDs that occur more than once.
    list_of_keys = [d["hash_key"] for d in uv_metadata_list]
    uuid_counts = collections.Counter(list_of_keys)
    duplicates = [uuid for uuid, count in uuid_counts.items() if count > 1]
    print("Duplicate UUIDs:", len(duplicates))

    for uuid in duplicates:
        print(uuid)
        for metadata_dict in uv_metadata_list:
            if uuid == metadata_dict["hash_key"]:
                print(f"duplicate UUID generated by: {metadata_dict['path']}")
    return None
