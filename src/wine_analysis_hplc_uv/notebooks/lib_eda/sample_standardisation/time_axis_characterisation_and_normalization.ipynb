{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "date: 2023-08-23\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing and Normalizing Dataset Time Axis\n",
    "\n",
    "This notebook covers our efforts to normalize the time axes of the CUPRAC dataset in order to move toward a universal time index. This is needed as multivariate statistical models such as XGBoost require that the same feature (peak) is in the same column (time) for each sample. We will do this by treating the sample signals as time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "from wine_analysis_hplc_uv.etl.build_library.db_methods import get_data, pivot_wine_data\n",
    "import pandas as pd\n",
    "import duckdb as db\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import polars as pl\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "pd.options.display.width = None\n",
    "pd.options.display.max_colwidth = 50\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 15\n",
    "pd.options.display.colheader_justify = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "\n",
    "def fetch_dataset(con):\n",
    "    query = \"\"\"--sql\n",
    "    SELECT\n",
    "        *\n",
    "    FROM\n",
    "        chromatogram_spectra_long cs \n",
    "    LEFT JOIN\n",
    "        sample_metadata sm\n",
    "    USING\n",
    "        (id)\n",
    "    WHERE\n",
    "        detection='cuprac'\n",
    "    AND \n",
    "        wavelength=450\n",
    "    AND \n",
    "        varietal='shiraz'\n",
    "    ORDER BY\n",
    "        mins DESC\n",
    "    \"\"\"\n",
    "    # get_data.get_wine_data(con, detection=('cuprac',), wavelength=(450,), varietal=('shiraz',))\n",
    "    # df = pivot_wine_data.pivot_wine_data(con)\n",
    "    return con.sql(query).pl()\n",
    "\n",
    "\n",
    "con = db.connect(definitions.DB_PATH)\n",
    "df = fetch_dataset(con)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 154\n",
    "\n",
    "The sample used to explore the fundamentals of the time series is sample 154 - 2020 leeuwin estate shiraz art series, an Australian Shiraz from Margaret River, Western Australia. A Shiraz has been selected because they, at least anecdotally, have the highest peak intensity and signal complexity, meaning that patterns in the data should be easy to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 154\n",
    "\n",
    "\n",
    "def fetch_154(con):\n",
    "    query = \"\"\"--sql\n",
    "            SELECT\n",
    "                wine, mins, absorbance\n",
    "            FROM\n",
    "                chromatogram_spectra_long cs \n",
    "            LEFT JOIN\n",
    "                sample_metadata sm\n",
    "            USING\n",
    "                (id)\n",
    "            WHERE\n",
    "                samplecode='154'\n",
    "            AND \n",
    "                wavelength=450\n",
    "            ORDER BY\n",
    "                mins ASC\n",
    "            \"\"\"\n",
    "    return con.sql(query).pl()\n",
    "\n",
    "\n",
    "df_154 = fetch_154(con)\n",
    "viz = df_154.plot(x=\"mins\", y=\"absorbance\", title=\"154\")\n",
    "display(viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Sampling Frequency \n",
    "\n",
    "The use of of sampling methods requires a method of gauging the frequency, and regularity of frequency of observation of each dataset. Sampling frequency here is defined as the number of observations per second $\\frac{n \\space \\text{obs}}{m \\space \\text{seconds}}$. I am expecting the sampling frequency to equal 2.5Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sampling_frequency(df: pl.DataFrame):\n",
    "    \"\"\"\n",
    "    Calculate the average sampling frequency for an input chromatogram\n",
    "    \"\"\"\n",
    "    # TODO calculate the frequency in hertz, 1 / seconds * 1e6.(?)\n",
    "    # lag shifts the column back one relative to the column.\n",
    "    # lead shifts the column forward one relative to the column.\n",
    "    # to find the sampling frequency, use the foreward diff, i.e. diff between the column and the one ahead.\n",
    "    mean_hz = db.sql(\n",
    "        \"\"\"--sql\n",
    "        -- First get the time dimension, expressed in seconds\n",
    "        CREATE OR REPLACE TEMP TABLE hz_tbl\n",
    "        AS (\n",
    "            SELECT\n",
    "                mins*60.0 AS seconds\n",
    "            FROM\n",
    "                df\n",
    "            ORDER BY seconds ASC\n",
    "        );\n",
    "        -- shift the seconds column forward one such that a row contains time, time+1\n",
    "        CREATE OR REPLACE TEMP TABLE hz_tbl\n",
    "        AS \n",
    "        (\n",
    "        SELECT\n",
    "            seconds,\n",
    "            lag(seconds) OVER () as lag_seconds\n",
    "        FROM\n",
    "            hz_tbl\n",
    "        );\n",
    "        -- calculate the difference between time and time+1, the forward difference\n",
    "        CREATE OR REPLACE TEMP TABLE hz_tbl\n",
    "        AS\n",
    "        (\n",
    "        SELECT\n",
    "            seconds,\n",
    "            lag_seconds,\n",
    "            (seconds - lag_seconds) as forward_diff\n",
    "        FROM\n",
    "            hz_tbl\n",
    "        );\n",
    "        -- calculate the frequency of the difference, expressed in hertz\n",
    "        CREATE OR REPLACE TEMP TABLE hz_tbl\n",
    "        AS (\n",
    "            SELECT\n",
    "                seconds,\n",
    "                lag_seconds,\n",
    "                forward_diff,\n",
    "                (1 / forward_diff) as hz,\n",
    "            FROM\n",
    "                hz_tbl\n",
    "        );\n",
    "        -- express the average seconds difference and average hertz.\n",
    "        SELECT mean(forward_diff) as mean_diff, mean(hz) as mean_hertz FROM hz_tbl;\n",
    "        \"\"\"\n",
    "    ).pl()\n",
    "\n",
    "    display(mean_hz)\n",
    "\n",
    "\n",
    "calculate_sampling_frequency(df=df_154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the sampling frequency is one observation per 400 milliseconds, or 2.5 Hz, and that at least for this sample, the frequency is consistant. Thus, no extrenuous resampling is necessary beyond compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Maximum Time Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unfortunate side-effect of floating-point data types [@_d] is that for a given experimental variable observation, and depending on the numerical data type, there will be a higher number of digits stored in memory than the actual precision of the instrument. As one of my goals is to align all of my time series to one universal time axis, decimial digits beyond an identified level of precision can be treated as noise and discarded without further thought. Thus I need a method of identifying what an appropriate level of precision is. Agilent is not forthcoming with the rating of their DAD, so an internal analysis is required. In [determining_time_precision](./determining_time_precision.ipynb) I observed what effects changing the time scale had on the granularity of the data, and increased the time scale until I identified that a millisecond scale was the highest I could go without resulting in duplicates. A round-about way of approaching the problem, but an effective one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is question of what is the precision of the time points of my observations. For example, sample 154:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_154.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the second time point of this sample is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_num_sigfigs(df: pl.DataFrame) -> None:\n",
    "    obs = df.item(1, \"mins\")\n",
    "    display(obs)\n",
    "    display(f\"n sigfigs: {len(str(obs).split('.')[1])}\")\n",
    "\n",
    "\n",
    "observe_num_sigfigs(df=df_154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Unfortunately even the 'raw' data in my database has a precision of sometimes 18 digits, which could not possibly be correct, and must be a symptom of float datatypes in Python. To settle this once and for all, I could either make a decision of what is the minimum time scale that retains unique values in the time column, or check a .UV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_sig_figs_in_raw_file():\n",
    "    import rainbow as rb\n",
    "    import os\n",
    "\n",
    "    filepath = os.path.join(definitions.LIB_DIR, \"cuprac\", \"131.D\")\n",
    "    obs = rb.read(filepath).get_file(\"DAD1.UV\").xlabels[0]\n",
    "    display(obs)\n",
    "    display(\n",
    "        f\"n sigfigs: {len(str(obs).split('.')[1])}\",\n",
    "    )\n",
    "\n",
    "\n",
    "observe_sig_figs_in_raw_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well I have been vindicated, as rainbow is also returning 18 significant figures. Thus the second approach is required - identify an appropriate level of granularity by testing several time scales and seeing when duplicate values appear. Observe the millisecond ('L') and second ('S') scales (refer to [offset alias](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) for the symbology):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_minimum_precision_without_duplicates(pl_df: pl.DataFrame) -> None:\n",
    "    df = (\n",
    "        pl_df.to_pandas()\n",
    "        .set_index(\"mins\")\n",
    "        .pipe(lambda x: x.set_index(pd.to_timedelta(x.index, unit=\"minutes\")))\n",
    "    )\n",
    "    display(\n",
    "        \"num duplicates at millisecond scale:\",\n",
    "        len(df.index[df.index.round(freq=\"L\").duplicated()]),\n",
    "    )\n",
    "    display(\n",
    "        \"num duplicates at second scale:\",\n",
    "        len(df.index[df.index.round(freq=\"S\").duplicated()]),\n",
    "    )\n",
    "\n",
    "\n",
    "observe_minimum_precision_without_duplicates(pl_df=df_154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that no duplicates are detected at the millisecond scale ('L') , however at the second ('S') scale, over half the observation points are now duplicates. Thus we will continue at the millisecond scale.\n",
    "\n",
    "To reiterate, the time axis should be converted to a `timedelta_range` and then rounded to \"L\", or millisecond scale, in the following manner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reset the index to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_millisecond_timedelta(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    convert 'mins' to `timedelta_range` and round to millisecond precision\n",
    "    \"\"\"\n",
    "    df = df.pipe(lambda x: x.set_axis(pd.to_timedelta(x.index, unit=\"minutes\"))).pipe(\n",
    "        lambda x: x.set_axis(x.index.round(\"L\"))\n",
    "    )\n",
    "\n",
    "    display(df.index.dtype)\n",
    "    return df\n",
    "\n",
    "\n",
    "time_to_millisecond_timedelta(df=df_154.to_pandas().set_index(\"mins\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert to timedelta and round:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple operation has been added to [SignalProcessor](src/wine_analysis_hplc_uv/signal_processing/mindex_signal_processing.py) as `.adjust_timescale()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying and Removing Scalar Offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While determining the time precision, I noticed that once adjusted to a millisecond scale, it was clear that there was a scalar offset in sample 154 of 15 milliseconds at element zero. This is odd because we would expect the first observation to start at time zero.\n",
    "\n",
    " The first question to ask is whether there is a constant offset, as if not, resampling may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_offset(df: pl.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Observe the average observation time offset.\n",
    "\n",
    "    TODO: observe whether there is a consistent difference. This would be exemplified by\n",
    "      deviation from a monotonically increasing time series. The expected series would\n",
    "      start at zero and increase by the frequency every observation. In this case, 0.4 seconds.\n",
    "\n",
    "      To calculate this, one could first insert the column, then subtract the actual from the\n",
    "      ideal.\n",
    "    \"\"\"\n",
    "    db.sql(\n",
    "        \"\"\"--sql\n",
    "        -- define a custom rounding macro to handle floating point error.\n",
    "        CREATE OR REPLACE TEMP MACRO round_(a) AS round(a, 10);\n",
    "        -- create an incremementing series\n",
    "        CREATE OR REPLACE SEQUENCE serial START 1;\n",
    "        -- workspace table containing the time in seconds. Round to avoid floating point errors\n",
    "        CREATE OR REPLACE TEMP TABLE compare_seconds\n",
    "        AS (\n",
    "          SELECT\n",
    "            round_(mins * 60) as seconds_actual,\n",
    "            round_((row_number() OVER ()-1)*0.4)::DOUBLE AS seconds_ideal,\n",
    "          FROM\n",
    "            df\n",
    "        );\n",
    "        CREATE OR REPLACE TEMP TABLE\n",
    "          compare_seconds\n",
    "          AS (\n",
    "          SELECT\n",
    "            seconds_actual,\n",
    "            seconds_ideal,\n",
    "            round_(seconds_actual - seconds_ideal) as diff\n",
    "          FROM\n",
    "            compare_seconds\n",
    "        );\n",
    "\n",
    "        CREATE OR REPLACE TEMP TABLE\n",
    "          average_time_offset\n",
    "        AS (\n",
    "          SELECT\n",
    "            mode(diff)\n",
    "          FROM\n",
    "            compare_seconds\n",
    "        );\n",
    "        SELECT * FROM average_time_offset;\n",
    "        \"\"\"\n",
    "    ).show()\n",
    "\n",
    "\n",
    "observe_offset(df=df_154)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a constant difference of 0.15 seconds from the ideal.\n",
    "\n",
    "Is it the same for every sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_time_difference():\n",
    "    with open(\n",
    "        \"/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notebooks/lib_eda/sample_standardisation/get_time_frequency_all_samples.sql\",\n",
    "        \"r\",\n",
    "    ) as f:\n",
    "        query = f.read()\n",
    "        con.sql(query).show()\n",
    "\n",
    "\n",
    "observe_time_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly removing this\n",
    "def observe_frequency_all_samples(con: db.DuckDBPyConnection):\n",
    "    \"\"\"\n",
    "    Observe the time frequency across all samples in the database\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: get the time column for all samples. This requires selecting by sample and 1 wavelength\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    with open(Path(Path.cwd() / \"get_time_frequency_all_samples.sql\"), \"r\") as f:\n",
    "        query = f.read()\n",
    "\n",
    "        con.sql(query)\n",
    "        con.sql(\"SELECT * FROM wine_seconds\").show(max_width=200)\n",
    "        con.sql(\"SELECT * FROM average_mode_over_samples;\").show()\n",
    "        display(\n",
    "            con.sql(\"SELECT * FROM wine_hertz_agg\")\n",
    "            .pl()\n",
    "            .plot.hist(y=\"mode_diff_hertz\", title=\"distribution of \")\n",
    "        )\n",
    "        con.sql(\"FROM mode_counts\").show()\n",
    "\n",
    "\n",
    "observe_frequency_all_samples(con=con)\n",
    "# df = fetch_all_samples(con)\n",
    "# adf.head()\n",
    "con.sql(\"SELECT COUNT(DISTINCT sample_num) FROM wine_seconds\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is a consistant scalar observation per observation time difference of 400 milliseconds, which matches the expected frequency of 2.5Hz.\n",
    "\n",
    "Now what about the first observation offset, what is the trend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    adf.stack([\"samplecode\", \"wine\"])\n",
    "    .groupby([\"samplecode\", \"wine\"])[\"mins\"]\n",
    "    .first()\n",
    "    .plot(style=\".\", title=\"first time value per sample\", ylabel=\"time (mins)\")\n",
    ")\n",
    "plt.tick_params(axis=\"x\", bottom=False, labelbottom=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see without further analysis that there is a random spread of values, thus we can be confident in merely subtracting that value from the time column, aligning observation zero with time zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = (\n",
    "    adf.stack([\"samplecode\", \"wine\"])\n",
    "    .assign(\n",
    "        mins=lambda df: df.groupby([\"samplecode\", \"wine\"])[\"mins\"].transform(\n",
    "            lambda x: x - x.iloc[0]\n",
    "        )\n",
    "    )  # adjust time axis by initial value so they all start at 1\n",
    "    .unstack([\"samplecode\", \"wine\"])\n",
    "    .reorder_levels([\"samplecode\", \"wine\", \"vars\"], axis=1)\n",
    "    .sort_index(level=0, axis=1, sort_remaining=True)\n",
    "    .pipe(lambda df: df if display(df.head()) else df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    adf.stack([\"samplecode\", \"wine\"])\n",
    "    .groupby([\"samplecode\", \"wine\"])[\"mins\"]\n",
    "    .first()\n",
    "    .plot(\n",
    "        style=\".\",\n",
    "        title=\"first time value per sample, first value subtracted\",\n",
    "        ylabel=\"time (mins)\",\n",
    "    )\n",
    ")\n",
    "plt.tick_params(axis=\"x\", bottom=False, labelbottom=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's convincing enough for me. As of 2023-08-23 22:47:37 I am going to assume the full dataset follows the same pattern. In summary: all data time axes have a varying offset equal to the value of the first measurement. Subtracting the first value from the axis will align the data so that the first measurement is zero. The caveat is that the observation frequency must be the same for all samples.\n",
    "\n",
    "A method for correcting the offset has been created [here](src/wine_analysis_hplc_uv/signal_processing/mindex_signal_processing.py) under `.correct_offset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Universal Time Axis\n",
    "\n",
    "Now that the offset has been corrected for, and rounding to milliseconds, the sample on sample time columns are looking regular, and I suspect that we can now use 1 universal time column as an index. To determine whether this is true, we should compare all time columns and find any outliers. I will investigate this by treating each time element as a column and calculating the z-score for each row in that column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Select 'mins' column, convert to float (seconds), transpose, forward and backfill\n",
    "missing values to prepare for outlier detection\n",
    "\"\"\"\n",
    "\n",
    "adfT = (\n",
    "    adf.stack([\"samplecode\", \"wine\"])[\"mins\"]\n",
    "    .unstack([\"samplecode\", \"wine\"])\n",
    "    .apply(lambda x: x.dt.total_seconds())\n",
    "    .T.ffill()\n",
    "    .bfill()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate mean difference rounded to 3 decimal places (because floats), find those who \n",
    "are not equal to zero, then the total sum of True values\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mask = (adfT.apply(lambda x: np.round(x - x.mean(), 3)) != 0).sum().sum()\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting the mean of each column from its elements should act as a outlier detector, as we are expecting the values to be either all equal, or not. As we can see, useing the condition `!=0` results in a boolean frame, and calling `.sum().sum()` will calculate the total number of elements which are not equal to zero. As we can see, that number is zero, thus all the time columns are now equal, and we can use a universal time column, rather than a inter-sample column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Observing both sample 154 and the overall dataset has enabled me to investigate treating the signals as time series, normalization methods, and moving towards a universal time axis. Specifically, we found that `pd.timedelta` was an appropriate time series datatype. Firstly we observed that in sample 154 there was a sampling frequency of 2.5Hz, or one observation per 400 milliseconds and then later on found this consistant across the whole CUPRAC dataset. Initally the time axes of each sample looked unreconcilable, but after rounding to a millisecond scale and subtracting a scalar offset, we proved that there was infact a universal time scale that can be used for all samples. The only caveat is that the samples need to be recorded at the same frequency. That being said, simple resampling would rectify those differences. Finally, methods to move to the universal time axis (index) have been created in `m_index_signal_processing`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
