{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"My Document\"\n",
    "format: html\n",
    "bibliography: references.bib\n",
    "link-citations: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Time Alignment\n",
    "\n",
    "Due to systemic error, datasets of chromatogams of samples run under the same experimental conditions often will exhibit retention time shifting for the same compound peak. Inter-sample data analysis requires that features are aligned within the same vector. The magnitude and direction of run on run peak shift is unique for each peak for each run, within a distribution, creating a complex problem. A traditional approach is to reduce the dimensionality of the chromatograms through an aggregate measure such as peak area, discarding the time axis in favor of an element-wise ordering. This approach has two downsides - the first is the manual marking, grouping and ordering of peaks across the sampleset, which is subjective and often irreproducible [@zheng_2017], the second is the arbitrary loss of information fed to the statistical model, specifically the signal shapes [@nielsen_1998]. @bos-RecentApplicationsChemometrics-2020 lists correlation-optimized warping (COW), dynamic time warping (DTW), and correlation-optimized shifting (COSHIFT) as the most popular methods of alignment.\n",
    "\n",
    "## DTW\n",
    "\n",
    "Dynamic time warping is a method of aligning two time series, a reference series (to be aligned to) and a query series (to be aligned), originally developed in the context of speech recognition technology [@velichko_1970]. Alignment achieved by performing localizaed warping in the form of  stretching and compressing of the query series until a pre-defined level of alignment is reached, measured as the minimization of the distances between the two series. The distance between the two signals is measured as the sum of the distances between each series elementwise pairing [@giorgino_2009]. [@jiao_2015] has shown that DTW is an appropriate method of aligning organic sample chromatogram datasets. [@bork_2013] has discussed the importance of DTW for process monitoring. They describe how traditional DTW can alter y-axis values while aligning the x, however an extension to the method termed 'Derivative Dynamic Time Warping' [@keogh_2001] respects the shape of each series by observing their first derivatives, reducing unnecessary modifications.\n",
    "\n",
    "### Outputting Aligned Tensors Through DTW\n",
    "\n",
    "Tomasi discusses signal alignment through DTW in [@tomasi_2004, p. 7, sec. 2.3.3.] where they note that DTW does not itself produce aligned series of the same length, rather outputting shortened or lengthened series depending on the warping path taken. For stacking of sample signals into a tensor, the signals need to be the same length. This is not the intent of the design of the DTW algorithm, which rather is used to output the cost of aligning the series in the form of a distance metric. They do state that a desired synchronization can be achieved by either taking the mean value of intervals of stretching in the query (removing repeated time points in the warping), or an asymmetric warping algorithm which directly maps the query to the reference, but this can cause discontinuities in the warped signal, quoting @kassidas_1997.\n",
    "\n",
    "## COW\n",
    "\n",
    "@skov_2006 discussed the use of the COW algorithm for alignment of chromatographic data. The Correlation Optimized Warping (COW) algorithm was developed by Nielsen et al. [@nielsen_1998] for the purpose of data prepraration for multivariate statistical analysis.\n",
    "\n",
    "COW can be used on 2d and 3d data and the output can be fed directly into models such as PCA.[@nielsen_1998].\n",
    "\n",
    "COW is similar to DTW constrained to a number of windows along the time axis [@nielsen_1998].\n",
    "\n",
    "What is COW?\n",
    "\n",
    "COW aligns one signal onto another through localized linear stretching and compression of its time axis.\n",
    "COW was developed by Nielsen et al. who demonstrated its use on single and multi-channel HPLC-DAD chromatograms of fungal extracts [@nielsen_1998].\n",
    "\n",
    "The theory of COW is as follows (using notation from [@nielsen_1998]): for two signals, the 'target' (T) and the 'profile' (P), the two signals are divided up into a finite number of sections ($N=\\frac{L_p}{m}$), each of which is internally warped to maximise alignment, resulting in an aligned signal (P'). This operation is constrained to ensure that time ordering is retained. Within each section, warping can either stretch or compress the sections, and in the case of a length mismatch, P' is linearly interpolated to match the length of T. The warping magnitude is constrained through parameter $t$, called 'the slack'. For P and T of different length, slack is defined as lying within the range $\\Delta \\pm t$ where $\\Delta=\\frac{L_T}{N}-m$.  Warping is performed section by section, and for the correlation coefficient of each pair is calculated and the end-point of the process is the maximisation of the sum of correlation coefficients. The identification of the section warping optimization is identified through dynamic programming. The calculation only requires specification of the segment length and slack parameters. Nielsen et al. demonstrated that COW performed better when fitting 3D data than 2D as the 'spectral information' restricted overfitting. In their example, they showed that subsetting the dataset to the target interval and baseline correction improved alignment. They also recommended using the cubed correlation coefficient over the base form as it selects for optimzied alignment without compromise. [@nielsen_1998].\n",
    "\n",
    "DTW and COW were compared by Tomasi et al. [@tomasi_2004]. The MATLAB code the group wrote for COW can be found on the website of [Chemometrics Group of Copenhagen](https://ucphchemometrics.com/warping/).\n",
    "\n",
    "COW has been adapted for 2D chromtaography by several indepedant research groups [@zhang_2008, @gros_2012].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Algorithm\n",
    "\n",
    "notes from @giorgino_ComputingVisualizingDynamic_2009.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "$X$: the *test* or *query*\n",
    "$Y$: the *reference*\n",
    "\n",
    "*i*: index of $X$\n",
    "*j*: index of $Y$\n",
    "\n",
    "*f*: *local dissimilarity function* defined between pairs of $x_i$ and $y_j$. Non negative: $$d(i, j)=f(x_i, y_j) \\geq 0$$\n",
    "\n",
    "$d$: cross-distance matrix between $X$ and $Y$\n",
    "\n",
    "$\\phi(k)$: warping curve $$\\phi(k)=(\\phi_x(k), \\phi_y(k))$$\n",
    "\n",
    "Where $\\phi_x(k), \\phi_y(k)$ outputs an integer from 1 to N.\n",
    "\n",
    "$\\phi_x$ remaps $X$ time indices\n",
    "$\\phi_y$ remaps $Y$ time indices\n",
    "\n",
    "There is a average accumulated distortion between the warped $X$ and $Y$: $$d_\\phi(X, Y) = \\sum_{k=1}^{T} d(\\phi_x(k), \\phi_y(k)) \\frac{m_\\phi(k)}{M_\\phi}$$\n",
    "\n",
    "$m_\\phi(k)$: per-step weighting coefficient, \n",
    "$M_\\phi(k)$: normalizing constant of $m_\\phi(k)$\n",
    "\n",
    " $\\phi$ is constrained to ensure reasonable results. One constraint is monotonicity to ensure time ordering/avoid unnecessary loops: $$\\phi_x(k+1) \\geq \\phi_x(k)$$  $$\\phi_y(k+1) \\geq \\phi_y(k)$$\n",
    "\n",
    "The goal of DTW is to minimize the distance between $X$ and $Y$: $$D(X, Y)=min \\space d_\\phi(X, Y)$$\n",
    "\n",
    "Note: this mentions that Y is also deformed \"The deformation of the time axes of $X$ **and** $Y$\" (emphasis mine).\n",
    "\n",
    "DTW can be computed in $O(N \\cdot M)$ tiime.\n",
    "\n",
    "$D(X, Y)$: \"minimum global dissimilarity\", \"DTW distance\". Stretch insensitive measure of the 'inherent difference' between $X$ and $Y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtwalign\n",
    "\n",
    "`dtwalign` is a Python package that includes outputting the alignment path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pybaselines import Baseline\n",
    "from dtwalign import dtw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wine_analysis_hplc_uv.notebooks.dtw_methods import DTWNotebookMethods\n",
    "from wine_analysis_hplc_uv.signal_processing.mindex_signal_processing import (\n",
    "    SignalProcessor,\n",
    ")\n",
    "\n",
    "scipro = SignalProcessor()\n",
    "\n",
    "nb_mtds = DTWNotebookMethods()\n",
    "\n",
    "df = pd.read_parquet(definitions.XPRO_YPRO_DOWNSAMPLED_PARQ_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop an optimal alignment, we will focus on the alignment of 2021 John Duval Shiraz with 2021 Torbreck Struie. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join x, y and aligned x on index\n",
    "\n",
    "x = df.loc[:, [\"176\"]].pipe(\n",
    "    lambda df: df.set_axis(\n",
    "        axis=1,\n",
    "        labels=pd.MultiIndex.from_arrays(\n",
    "            [[\"176\"], [\"query\"], [\"NA\"], [\"abs\"]],\n",
    "            names=[\"sample\", \"status\", \"window_size\", \"unit\"],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "y = df.loc[:, [\"177\"]].pipe(\n",
    "    lambda df: df.set_axis(\n",
    "        axis=1,\n",
    "        labels=pd.MultiIndex.from_arrays(\n",
    "            [[\"177\"], [\"ref\"], [\"NA\"], [\"abs\"]],\n",
    "            names=[\"sample\", \"status\", \"window_size\", \"unit\"],\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align 176 on 177 without any constraints\n",
    "\n",
    "align_x = nb_mtds.dtw_align_series(x, y)\n",
    "\n",
    "align_x.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the alignment plane plot and series overlays, a significant section of the query series has been compressed and then interpolated as a flat line, losing a number of peaks in the process, see below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata, g = nb_mtds.query_ref_align_plot(x, y, align_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, numerous peaks are lost post-alignment, which is unacceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing\n",
    "\n",
    "When aligning a query and reference chromatogram of similar samples we have the expectation that peaks within $d_m$ distance, where $d_m$ is misalignment distance, are the same compound and will be aligned to the same retention time post warping. Conversely, we do not expect regions where the query has peaks but the reference does not to be altered. Unfortunatey, the algorithm does not by default contain that information, and without a global constraint it will drastically alter the query along the entire serie to minimize what it percieves as the distance between the two, including compressing peaks present in the query but not the reference:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cam see the query is modified by the algorithm to match the reference in both the x and y axes. This leads to an unexpectedly deformed query. What is needed is to restrict warping to localized regions in the form of a windowing operation with windows of specific geometry. One such is the Sakoe-Chiba band [@sakoe_1978] which restricts how far apart two elements can be when matched: $$|\\phi_x(k)-\\phi_y(k)| \\leq T_0$$ where $T_0$ is the absolute time deviation between two matched elements, specified by the user. As described by @giorgino_ComputingVisualizingDynamic_2009, this creates a boundary within the alignment plane within which the warping path can exist. For a window of size 10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the windowed aligned x, assign a regular frequency time index, rename axes\n",
    "\n",
    "\n",
    "sakoechiba_10_x_align = nb_mtds.dtw_align_series(\n",
    "    x, y, dict(window_type=\"sakoechiba\", window_size=10)\n",
    ")\n",
    "sakoechiba_10_x_align.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks promising, how does it compare to the query and the reference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata, __ = nb_mtds.query_ref_align_plot(x=x, y=y, x_align=sakoechiba_10_x_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    plotdata.loc[:, pd.IndexSlice[:, :, :, :, \"query and aligned query\"]]\n",
    "    .melt(ignore_index=False, value_name=\"mAU\")\n",
    "    .pipe(lambda df: df.set_index(keys=df.index.total_seconds() / 60))\n",
    "    .pipe(sns.lineplot, x=\"mins\", y=\"mAU\", hue=\"status\")\n",
    "    .set_title(\"query and aligned query\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Selecting a Sakoechiba window with size 10 dramaticaly alters the behavior of the warping, preserving peaks that were otherwise lost. It also appears as though the (in the default setting at least), baseline height differences affect the warp. It appears that reducing the distance/cost pre-warp will reduce overall warping errors. Before we continue we should examine the effects of baseline subtraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW with Subtracted Baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same x and y, assess effect of DTW on baseline subtracted series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([x, y], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename({\"abs\": \"value\"}, axis=1).pipe(\n",
    "    lambda df: df.set_axis(df.columns.set_names(\"signal\", level=\"unit\"), axis=1)\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lam of 1000 has been chosen as that appears to fit the baseline to the base of each peak without fitting to the internal area of the peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the baseline\n",
    "\n",
    "from pybaselines import Baseline\n",
    "\n",
    "\n",
    "def assign_baseline_correction(df) -> pd.DataFrame:\n",
    "    df = data.pipe(\n",
    "        lambda df: df.melt(ignore_index=False, value_name=\"raw\")\n",
    "        .drop(\"signal\", axis=1)\n",
    "        .pipe(\n",
    "            lambda df: df.groupby(\"sample\", group_keys=False).apply(\n",
    "                lambda grp: grp.assign(\n",
    "                    bline=Baseline(grp.index.total_seconds()).asls(grp.raw, lam=1000)[0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .pipe(\n",
    "            lambda df: df.groupby(\"sample\", group_keys=False).apply(\n",
    "                lambda grp: grp.assign(bcorr=grp.raw - grp.bline)\n",
    "            )\n",
    "        )\n",
    "        .pivot(\n",
    "            columns=[\"sample\", \"status\", \"window_size\"],\n",
    "            values=[\"raw\", \"bline\", \"bcorr\"],\n",
    "        )\n",
    "        .pipe(lambda df: df.set_axis(df.columns.set_names(\"signal\", level=0), axis=1))\n",
    "        .reorder_levels(order=[\"sample\", \"status\", \"window_size\", \"signal\"], axis=1)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    (\n",
    "        df.melt(ignore_index=False)\n",
    "        .pipe(sns.FacetGrid, col=\"sample\")\n",
    "        .map_dataframe(sns.lineplot, x=\"mins\", y=\"value\", hue=\"signal\")\n",
    "        .add_legend()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "data = assign_baseline_correction(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets observe how the warping behaves for the same parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_align_series = nb_mtds.dtw_align_series(\n",
    "    x=data.loc[:, pd.IndexSlice[\"176\", :, :, \"bcorr\"]],\n",
    "    y=data.loc[:, pd.IndexSlice[\"177\", :, :, \"bcorr\"]],\n",
    ")\n",
    "data = (\n",
    "    pd.concat([data.loc[:, pd.IndexSlice[:, :, :, \"bcorr\"]], dtw_align_series], axis=1)\n",
    "    .sort_index(axis=1)\n",
    "    .droplevel(axis=1, level=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "def plot_bcorr_dtw(data):\n",
    "    x = data.loc[:, idx[:, \"query\", :]]\n",
    "    y = data.loc[:, idx[:, \"ref\", :]]\n",
    "    x_align = data.loc[:, idx[:, \"aligned\", :]]\n",
    "\n",
    "    sp1 = data.loc[:, pd.IndexSlice[:, [\"query\", \"ref\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"query and reference\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # sp2 is aligned and ref\n",
    "\n",
    "    sp2 = data.loc[:, pd.IndexSlice[:, [\"aligned\", \"ref\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"aligned query and reference\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## sp3 is query and aligned\n",
    "\n",
    "    sp3 = data.loc[:, pd.IndexSlice[:, [\"query\", \"aligned\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"query and aligned query\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # concatenate the three subplot dataframes, melt, renaming the value column to 'mAU', convert the date time index to a minutes float, create a column-wise facetgrid on 'subplot', map a lineplot to the facetgrids of 'mins', 'mAU', 'status' for hue, set the subplot titles to subplot value.\n",
    "\n",
    "    plotdata = pd.concat([sp1, sp2, sp3], axis=1)\n",
    "\n",
    "    g = (\n",
    "        plotdata.melt(ignore_index=False, value_name=\"mAU\")\n",
    "        .pipe(lambda df: df.set_index(df.index.total_seconds() / 60))\n",
    "        .pipe(\n",
    "            lambda df: sns.FacetGrid(df, col=\"subplot\")\n",
    "            .map_dataframe(sns.lineplot, x=\"mins\", y=\"mAU\", hue=\"status\")\n",
    "            .set_titles(col_template=\"{col_name}\")\n",
    "            .add_legend()\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "plot_bcorr_dtw(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would call that successful without any further modifications to the dtw algorithm. At this point in time we need a number of tools -\n",
    "\n",
    "1. a method of evaluating alignment\n",
    "2. produce a matrix of subplots for each sample in the set row wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline correct all samples\n",
    "\n",
    "df = (\n",
    "    df.melt(ignore_index=False, value_name=\"raw\")\n",
    "    .groupby(\"samplecode\", group_keys=False)\n",
    "    .apply(\n",
    "        lambda grp: grp.assign(\n",
    "            bline=Baseline(grp.index.total_seconds()).asls(grp.raw, lam=1000)[0]\n",
    "        )\n",
    "    )\n",
    "    .assign(bcorr=lambda df: df.raw - df.bline)\n",
    "    .pivot(columns=[\"samplecode\", \"wine\"], values=[\"raw\", \"bline\", \"bcorr\"])\n",
    "    .pipe(lambda df: df.set_axis(df.columns.set_names(level=0, names=\"signal\"), axis=1))\n",
    "    .reorder_levels(axis=1, order=[\"samplecode\", \"wine\", \"signal\"])\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 2x2 plot grid of each sample with an overlay of the raw, the baseline and\n",
    "# the bcorr signals\n",
    "\n",
    "(\n",
    "    df.melt(ignore_index=False, value_name=\"mAU\")\n",
    "    #  .pipe(lambda df: df.set_index())\n",
    "    .pipe(sns.FacetGrid, col=\"wine\", col_wrap=2)\n",
    "    .map_dataframe(sns.lineplot, hue=\"signal\", x=\"mins\", y=\"mAU\")\n",
    "    .set_titles(col_template=\"{col_name}\")\n",
    "    .add_legend()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline corrected signals appear to be acceptable to me. Does it modify which is the reference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = scipro.most_correlated(df)\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add '(ref)' suffix to the reference wine of the dataset\n",
    "\n",
    "\n",
    "def label_reference(df, ref):\n",
    "    \"\"\"\n",
    "    add suffix '(ref)' to wine string of reference sample\n",
    "    \"\"\"\n",
    "    oname = df[ref].columns.get_level_values(\"wine\")[0]\n",
    "    new_name = oname + \" (ref)\"\n",
    "\n",
    "    return df.rename({oname: new_name}, axis=1)\n",
    "\n",
    "\n",
    "df = label_reference(df, ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does, 176 is now the most correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, idx[ref, :, \"bcorr\"]]\n",
    "\n",
    "# add aligned series to original df through concatenation. Need to subset the original\n",
    "# with the warping path, reindex to 2S timedelta range, rename signal level to 'aligned\n",
    "# then concatentate with original df\n",
    "\n",
    "df = (\n",
    "    df.loc[:, idx[:, :, \"bcorr\"]]\n",
    "    .pipe(\n",
    "        lambda df: df.groupby([\"wine\"], group_keys=False, axis=1).apply(\n",
    "            lambda df: pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    df.iloc[dtw(x=df, y=y).get_warping_path(), :].pipe(\n",
    "                        lambda df: df.set_index(\n",
    "                            pd.timedelta_range(\n",
    "                                start=df.index[0], end=df.index[-1], freq=\"2S\"\n",
    "                            )\n",
    "                        )\n",
    "                        .rename_axis(\"mins\")\n",
    "                        .rename({\"bcorr\": \"aligned\"}, axis=1)\n",
    "                    ),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .pipe(\n",
    "        lambda df: df.reindex(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame()\n",
    "                .assign(role=\"query\")\n",
    "                .assign(\n",
    "                    role=lambda df: df.loc[:, \"role\"].where(\n",
    "                        ~(df.samplecode.isin(ref)), \"ref\"\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    .reorder_levels(axis=1, order=[\"samplecode\", \"wine\", \"role\", \"signal\"])\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to form the sets of three again.\n",
    "\"\"\"\n",
    "we are expecting to create a column 'subplot' with a pattern of 1,2,3 and for each\n",
    "sample a combination of ('query','bcorr'), ('query','aligned'), and ('ref','bcorr').\n",
    "\n",
    "specifically,\n",
    "\n",
    "1: ('query','bcorr'), ('ref','bcorr')\n",
    "2: ('query','aligned'), ('ref','bcorr')\n",
    "3: ('query','bcorr'), ('query','aligned')\n",
    "\n",
    "For each we're expecting 2 rows per sample, 8 rows for subplot 1, 8 rows for subplot 2, 8 rows for subplot 3.\n",
    "We need combinations with repeats using the cartesian product (?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# get the reference\n",
    "\n",
    "new_index_ = (\n",
    "    df.columns.to_frame(index=False)\n",
    "    .assign(\n",
    "        state=lambda df: df.signal.where(~(df.signal == \"aligned\"), \"aligned\")\n",
    "        .where(~(df.signal == \"bcorr\"), \"query\")\n",
    "        .where(~(df.role == \"ref\"), \"ref\")\n",
    "    )\n",
    "    .drop([\"role\", \"signal\"], axis=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index = pd.MultiIndex.from_frame(new_index_)\n",
    "df = df.set_axis(new_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " column_index_df = df.columns.to_frame(index=False).set_index([\"samplecode\", \"wine\", \"state\"]).loc[lambda df: ~df.index.duplicated(keep='first'),:]\n",
    "display(column_index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPlotDFBuilder:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Handle the assignment and duplication process and logic to form a relplot df index\n",
    "        in the following fashion:\n",
    "\n",
    "        | sample  | row  | col  | role  |\n",
    "        |---------|------|------|-------|\n",
    "        | sample1 | row1 | col1 | query |\n",
    "        | sample1 | row1 | col1 | ref   |\n",
    "        | sample1 | row1 | col2 | align |\n",
    "        | sample1 | row1 | col2 | ref   |\n",
    "        | sample1 | row1 | col3 | query |\n",
    "        | sample1 | row1 | col3 | align |\n",
    "\n",
    "        do for each row then concat horizontally.\n",
    "\n",
    "        Expects a tidy df of column index with levels (for example):\n",
    "\n",
    "        |    | samplecode      | wine                                  | state   |\n",
    "        |---:|:----------------|:--------------------------------------|:--------|\n",
    "        |  0 | 154             | 2020 leeuwin estate shiraz art series | aligned |\n",
    "        |  1 | 154             | 2020 leeuwin estate shiraz art series | query   |\n",
    "        |  2 | 176             | 2021 john duval wines shiraz concilio | ref     |\n",
    "        |  3 | 176             | 2021 john duval wines shiraz concilio | ref     |\n",
    "        |  4 | 177             | 2021 torbreck shiraz the struie 1     | aligned |\n",
    "        |  5 | 177             | 2021 torbreck shiraz the struie 1     | query   |\n",
    "        |  6 | torbreck-struie | 2021 torbreck shiraz the struie 2     | aligned |\n",
    "        |  7 | torbreck-struie | 2021 torbreck shiraz the struie 2     | query   |\n",
    "\n",
    "        It is generalized enough to handle any names for the levels, and any values\n",
    "        (as long as the pattern is consistant), however the 'samplecode' and 'state'\n",
    "        levels must be in the same order, seperated by 1 level.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        # create a df out of the df multiindex, drop any duplicates.\n",
    "        self.column_index_df = (\n",
    "            df.columns.to_frame(index=False)\n",
    "            .set_index([\"samplecode\", \"wine\", \"state\"])\n",
    "            .loc[lambda df: ~df.index.duplicated(keep=\"first\"), :]\n",
    "        )\n",
    "\n",
    "        # get the samplecodes as an iterable\n",
    "        samples = self.column_index_df.index.get_level_values(\"samplecode\").unique()\n",
    "\n",
    "        # for each sample form a df of 'query' series and 'ref' series, then concat\n",
    "        # them together\n",
    "        col1 = pd.concat(\n",
    "            [self.build_col(sample, \"176\", \"query\", \"ref\", 1) for sample in samples]\n",
    "        ).assign(row=lambda df: df.groupby(\"wine\").ngroup() + 1)\n",
    "\n",
    "        # for each sample form a df of 'aligned', 'ref', then concat together\n",
    "        col2 = pd.concat(\n",
    "            [self.build_col(sample, \"176\", \"aligned\", \"ref\", 2) for sample in samples]\n",
    "        ).assign(row=lambda df: df.groupby(\"wine\").ngroup() + 1)\n",
    "\n",
    "        # for each sample form a df of 'query', 'aligned' for the same sample, then concat\n",
    "        col3 = pd.concat(\n",
    "            [\n",
    "                self.build_col3(sample, \"176\", \"query\", \"aligned\", 3)\n",
    "                for sample in samples\n",
    "            ]\n",
    "        ).assign(row=lambda df: df.groupby(\"wine\").ngroup() + 1)\n",
    "\n",
    "        # combine all the col dfs\n",
    "        self.index_df = pd.concat([col1, col2, col3])\n",
    "\n",
    "        self.test_index_df()\n",
    "\n",
    "        self.join_df = self.join_df_index_df()\n",
    "\n",
    "        self.test_join_df()\n",
    "\n",
    "    def build_col(self, samplecode_1, samplecode_2, state_val_1, state_val_2, colnum):\n",
    "        \"\"\"\n",
    "        combine query and reference for overlaying in col1\n",
    "        samplecode_1 is the base sample, samplecode_2 is the overlay, or comparison.\n",
    "        state_val_1 and state_val_2 correspond to the respective samplecode.\n",
    "\n",
    "        Used for column 1 and column 2.\n",
    "        \"\"\"\n",
    "\n",
    "        if samplecode_1 == samplecode_2:\n",
    "            sample = self.column_index_df.loc[idx[samplecode_2, :, state_val_2], :]\n",
    "\n",
    "        else:\n",
    "            # get sample row\n",
    "            sample = self.column_index_df.loc[idx[samplecode_1, :, state_val_1], :]\n",
    "\n",
    "        # get the reference row, reindex it so its 'wine' (row) is s1\n",
    "        if samplecode_1 == samplecode_2:\n",
    "            ref = self.column_index_df.loc[idx[samplecode_2, :, state_val_2], :]\n",
    "\n",
    "        else:\n",
    "            wine = sample.index.get_level_values(\"wine\")\n",
    "            ref = self.column_index_df.loc[idx[samplecode_2, :, state_val_2], :].pipe(\n",
    "                lambda df: df.set_axis(\n",
    "                    df.index.remove_unused_levels().set_levels(\n",
    "                        level=[\"wine\"], levels=[wine]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # assign row and column identifier to reference and sample rows\n",
    "        col1 = pd.concat([sample, ref]).assign(col=colnum)\n",
    "\n",
    "        return col1\n",
    "\n",
    "    def build_col3(self, samplecode, ref_samplecode, state_val_1, state_val_2, colnum):\n",
    "        \"\"\"\n",
    "        combined query and aligned. Refer to build_col for parameter descriptions\n",
    "        Cant use 'build_col' because the reference sample doesnt have the same state\n",
    "        values as the other samples.\n",
    "\n",
    "        Maybe we can modify how the reference sample is handled. Maybe seperate prior\n",
    "        to initializing the concatenations in __init__\n",
    "        \"\"\"\n",
    "\n",
    "        if samplecode == ref_samplecode:\n",
    "            col3 = self.column_index_df.loc[[samplecode]].assign(col=3)\n",
    "\n",
    "        else:\n",
    "            col3 = self.column_index_df.loc[\n",
    "                idx[samplecode, :, [state_val_1, state_val_2]], :\n",
    "            ].assign(col=colnum)\n",
    "\n",
    "        return col3\n",
    "\n",
    "    def test_index_df(self):\n",
    "        \"\"\"\n",
    "        test whether the output ready index_df matches the expected content and structure\n",
    "        \"\"\"\n",
    "        # the expected output of RelPlotIndexBuilder.index_df. orient = 'tight' retains multiindex\n",
    "\n",
    "        left = pd.DataFrame.from_dict(\n",
    "            {\n",
    "                \"index\": [\n",
    "                    (\"154\", \"2020 leeuwin estate shiraz art series\", \"query\"),\n",
    "                    (\"176\", \"2020 leeuwin estate shiraz art series\", \"ref\"),\n",
    "                    (\"176\", \"2021 john duval wines shiraz concilio (ref)\", \"ref\"),\n",
    "                    (\"176\", \"2021 john duval wines shiraz concilio (ref)\", \"ref\"),\n",
    "                    (\"177\", \"2021 torbreck shiraz the struie 1\", \"query\"),\n",
    "                    (\"176\", \"2021 torbreck shiraz the struie 1\", \"ref\"),\n",
    "                    (\"torbreck-struie\", \"2021 torbreck shiraz the struie 2\", \"query\"),\n",
    "                    (\"176\", \"2021 torbreck shiraz the struie 2\", \"ref\"),\n",
    "                    (\"154\", \"2020 leeuwin estate shiraz art series\", \"aligned\"),\n",
    "                    (\"176\", \"2020 leeuwin estate shiraz art series\", \"ref\"),\n",
    "                    (\"176\", \"2021 john duval wines shiraz concilio (ref)\", \"ref\"),\n",
    "                    (\"176\", \"2021 john duval wines shiraz concilio (ref)\", \"ref\"),\n",
    "                    (\"177\", \"2021 torbreck shiraz the struie 1\", \"aligned\"),\n",
    "                    (\"176\", \"2021 torbreck shiraz the struie 1\", \"ref\"),\n",
    "                    (\"torbreck-struie\", \"2021 torbreck shiraz the struie 2\", \"aligned\"),\n",
    "                    (\"176\", \"2021 torbreck shiraz the struie 2\", \"ref\"),\n",
    "                    (\"154\", \"2020 leeuwin estate shiraz art series\", \"query\"),\n",
    "                    (\"154\", \"2020 leeuwin estate shiraz art series\", \"aligned\"),\n",
    "                    (\"176\", \"2021 john duval wines shiraz concilio (ref)\", \"ref\"),\n",
    "                    (\"177\", \"2021 torbreck shiraz the struie 1\", \"query\"),\n",
    "                    (\"177\", \"2021 torbreck shiraz the struie 1\", \"aligned\"),\n",
    "                    (\"torbreck-struie\", \"2021 torbreck shiraz the struie 2\", \"query\"),\n",
    "                    (\"torbreck-struie\", \"2021 torbreck shiraz the struie 2\", \"aligned\"),\n",
    "                ],\n",
    "                \"columns\": [\"col\", \"row\"],\n",
    "                \"data\": [\n",
    "                    [1, 1],\n",
    "                    [1, 1],\n",
    "                    [1, 2],\n",
    "                    [1, 2],\n",
    "                    [1, 3],\n",
    "                    [1, 3],\n",
    "                    [1, 4],\n",
    "                    [1, 4],\n",
    "                    [2, 1],\n",
    "                    [2, 1],\n",
    "                    [2, 2],\n",
    "                    [2, 2],\n",
    "                    [2, 3],\n",
    "                    [2, 3],\n",
    "                    [2, 4],\n",
    "                    [2, 4],\n",
    "                    [3, 1],\n",
    "                    [3, 1],\n",
    "                    [3, 2],\n",
    "                    [3, 3],\n",
    "                    [3, 3],\n",
    "                    [3, 4],\n",
    "                    [3, 4],\n",
    "                ],\n",
    "                \"index_names\": [\"samplecode\", \"wine\", \"state\"],\n",
    "                \"column_names\": [None],\n",
    "            },\n",
    "            orient=\"tight\",\n",
    "        )\n",
    "\n",
    "        pd.testing.assert_frame_equal(left=left, right=self.index_df)\n",
    "\n",
    "    def join_df_index_df(self):\n",
    "        \"\"\"\n",
    "        massage df and index_df to left join onto index_df\n",
    "        \"\"\"\n",
    "\n",
    "        pdf = (\n",
    "            self.df.melt(ignore_index=False, value_name=\"mAU\")\n",
    "            .reset_index()\n",
    "            .set_index([\"samplecode\", \"state\"])\n",
    "            .drop(\"wine\", axis=1)\n",
    "        )\n",
    "\n",
    "        pindex_df = self.index_df.reset_index().set_index([\"samplecode\", \"state\"])\n",
    "        join = pindex_df.join(pdf, how=\"left\").dropna()\n",
    "\n",
    "        return join\n",
    "\n",
    "    def test_join_df(self) -> None:\n",
    "        \"\"\"\n",
    "        Take the source df and join df and check whether the join is as expected. Specifically\n",
    "        make sure that the relationship between the labels and series values has been maintained.\n",
    "\n",
    "        Calculates the mean value of each series in both the base df and join_df then join\n",
    "        the two on the mean column. Then checks for any mismatch by checking for NAs in\n",
    "        result.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate base df mean rounded to 10 digits and modify index to the 'mean' column\n",
    "        df_means = (\n",
    "            self.df.mean().to_frame(\"mean\").round(10).reset_index().set_index(\"mean\")\n",
    "        )\n",
    "\n",
    "        # calculate join df mean rounded to 10 digits and modify index to the 'mean' column\n",
    "        post_join_means = (\n",
    "            self.join_df.groupby([\"samplecode\", \"wine\", \"state\", \"row\", \"col\"])[\"mAU\"]\n",
    "            .apply(lambda df: df.mean().round(10))\n",
    "            .to_frame(name=\"mean\")\n",
    "            .reorder_levels([\"row\", \"col\", \"samplecode\", \"wine\", \"state\"])\n",
    "            .reset_index()\n",
    "            .set_index(\"mean\")\n",
    "        )\n",
    "\n",
    "        # join base df and join df on 'mean'\n",
    "        mean_join = (\n",
    "            post_join_means.join(\n",
    "                df_means.loc[lambda df: ~(df.index.duplicated(keep=\"first\"))],\n",
    "                how=\"left\",\n",
    "                rsuffix=\"right\",\n",
    "                validate=\"many_to_one\",\n",
    "            )\n",
    "            .reset_index()\n",
    "            .set_index([\"row\", \"col\", \"samplecode\", \"wine\", \"state\"])\n",
    "            .sort_index()\n",
    "        )\n",
    "\n",
    "        # test whether any NA in df, indicating a failed join. If assertion fails, outputs\n",
    "        # rows with NA - use to identify mismatching join keys.\n",
    "        assert ~mean_join.isna().all().all(), (\n",
    "            \"NAs in join, failed. Found in the\"\n",
    "            f\" following\\n{mean_join[mean_join.isna().any(axis=1)]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "relplotdata = RelPlotDFBuilder(df).join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relplotdata = (\n",
    "    relplotdata.assign(mins=lambda df: df.mins.dt.total_seconds() / 60)\n",
    "    .assign(\n",
    "        col_label=lambda df: df.col.replace(\n",
    "            {\n",
    "                1: \"query and ref\",\n",
    "                2: \"aligned and ref\",\n",
    "                3: \"query and aligned\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .assign(\n",
    "        col_label=lambda df: pd.Categorical(\n",
    "            values=df.col_label,\n",
    "            categories=[\n",
    "                \"query and ref\",\n",
    "                \"aligned and ref\",\n",
    "                \"query and aligned\",\n",
    "            ],\n",
    "            ordered=True,\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    "    .set_index([\"row\", \"col\", \"col_label\", \"samplecode\", \"state\", \"wine\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now display it..\n",
    "\n",
    "relplot = sns.relplot(\n",
    "    relplotdata,\n",
    "    col=\"col_label\",\n",
    "    row=\"wine\",\n",
    "    x=\"mins\",\n",
    "    y=\"mAU\",\n",
    "    hue=\"state\",\n",
    "    kind=\"line\",\n",
    "    legend=\"full\",\n",
    "    facet_kws=dict(margin_titles=True, subplot_kws=dict(alpha=0.95)),\n",
    "    errorbar=None,\n",
    "    palette=sns.color_palette(n_colors=3, palette=\"colorblind\"),\n",
    ").set_titles(\n",
    "    col_template=\"{col_name}\",\n",
    "    row_template=\"{row_name}\",\n",
    ")\n",
    "relplot.fig.suptitle(\n",
    "    \"Comparison of Query and Reference Aligned Before and After DTW\", fontsize=16\n",
    ")\n",
    "relplot.fig.subplots_adjust(top=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the alignment is nominally successful for the samples within the set, remarkably even sample 177, which was the most divergent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have visually shown the effectiveness of DTW with a Sakoe-Chiba global constraint and prior baseline-correction with asls for aligning CUPRAC detected shiraz wine chromatograms. Further investigation will require that I identify appropriate alignment and distance metrics rather than utilizing a visual check. Specifically, we've shown that peaks can be matched, however it is not evident if a mismatch is occuring - suspiciously, no peak remains unmatched in any sample.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
