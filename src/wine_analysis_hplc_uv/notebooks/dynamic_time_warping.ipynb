{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"My Document\"\n",
    "format: html\n",
    "bibliography: references.bib\n",
    "link-citations: true\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Time Alignment\n",
    "\n",
    "Due to systemic error, datasets of chromatogams of samples run under the same experimental conditions often will exhibit retention time shifting for the same compound peak. Inter-sample data analysis requires that features are aligned within the same vector. The magnitude and direction of run on run peak shift is unique for each peak for each run, within a distribution, creating a complex problem. A traditional approach is to reduce the dimensionality of the chromatograms through an aggregate measure such as peak area, discarding the time axis in favor of an element-wise ordering. This approach has two downsides - the first is the manual marking, grouping and ordering of peaks across the sampleset, which is subjective and often irreproducible [@zheng_2017], the second is the arbitrary loss of information fed to the statistical model, specifically the signal shapes [@nielsen_1998]. @bos-RecentApplicationsChemometrics-2020 lists correlation-optimized warping (COW), dynamic time warping (DTW), and correlation-optimized shifting (COSHIFT) as the most popular methods of alignment.\n",
    "\n",
    "## DTW\n",
    "\n",
    "Dynamic time warping is a method of aligning two time series, a reference series (to be aligned to) and a query series (to be aligned), originally developed in the context of speech recognition technology [@velichko_1970]. Alignment achieved by performing localizaed warping in the form of  stretching and compressing of the query series until a pre-defined level of alignment is reached, measured as the minimization of the distances between the two series. The distance between the two signals is measured as the sum of the distances between each series elementwise pairing [@giorgino_2009]. [@jiao_2015] has shown that DTW is an appropriate method of aligning organic sample chromatogram datasets. [@bork_2013] has discussed the importance of DTW for process monitoring. They describe how traditional DTW can alter y-axis values while aligning the x, however an extension to the method termed 'Derivative Dynamic Time Warping' [@keogh_2001] respects the shape of each series by observing their first derivatives, reducing unnecessary modifications.\n",
    "\n",
    "### Outputting Aligned Tensors Through DTW\n",
    "\n",
    "Tomasi discusses signal alignment through DTW in [@tomasi_2004, p. 7, sec. 2.3.3.] where they note that DTW does not itself produce aligned series of the same length, rather outputting shortened or lengthened series depending on the warping path taken. For stacking of sample signals into a tensor, the signals need to be the same length. This is not the intent of the design of the DTW algorithm, which rather is used to output the cost of aligning the series in the form of a distance metric. They do state that a desired synchronization can be achieved by either taking the mean value of intervals of stretching in the query (removing repeated time points in the warping), or an asymmetric warping algorithm which directly maps the query to the reference, but this can cause discontinuities in the warped signal, quoting @kassidas_1997.\n",
    "\n",
    "## COW\n",
    "\n",
    "@skov_2006 discussed the use of the COW algorithm for alignment of chromatographic data. The Correlation Optimized Warping (COW) algorithm was developed by Nielsen et al. [@nielsen_1998] for the purpose of data prepraration for multivariate statistical analysis.\n",
    "\n",
    "COW can be used on 2d and 3d data and the output can be fed directly into models such as PCA.[@nielsen_1998].\n",
    "\n",
    "COW is similar to DTW constrained to a number of windows along the time axis [@nielsen_1998].\n",
    "\n",
    "What is COW?\n",
    "\n",
    "COW aligns one signal onto another through localized linear stretching and compression of its time axis.\n",
    "COW was developed by Nielsen et al. who demonstrated its use on single and multi-channel HPLC-DAD chromatograms of fungal extracts [@nielsen_1998].\n",
    "\n",
    "The theory of COW is as follows (using notation from [@nielsen_1998]): for two signals, the 'target' (T) and the 'profile' (P), the two signals are divided up into a finite number of sections ($N=\\frac{L_p}{m}$), each of which is internally warped to maximise alignment, resulting in an aligned signal (P'). This operation is constrained to ensure that time ordering is retained. Within each section, warping can either stretch or compress the sections, and in the case of a length mismatch, P' is linearly interpolated to match the length of T. The warping magnitude is constrained through parameter $t$, called 'the slack'. For P and T of different length, slack is defined as lying within the range $\\Delta \\pm t$ where $\\Delta=\\frac{L_T}{N}-m$.  Warping is performed section by section, and for the correlation coefficient of each pair is calculated and the end-point of the process is the maximisation of the sum of correlation coefficients. The identification of the section warping optimization is identified through dynamic programming. The calculation only requires specification of the segment length and slack parameters. Nielsen et al. demonstrated that COW performed better when fitting 3D data than 2D as the 'spectral information' restricted overfitting. In their example, they showed that subsetting the dataset to the target interval and baseline correction improved alignment. They also recommended using the cubed correlation coefficient over the base form as it selects for optimzied alignment without compromise. [@nielsen_1998].\n",
    "\n",
    "DTW and COW were compared by Tomasi et al. [@tomasi_2004]. The MATLAB code the group wrote for COW can be found on the website of [Chemometrics Group of Copenhagen](https://ucphchemometrics.com/warping/).\n",
    "\n",
    "COW has been adapted for 2D chromtaography by several indepedant research groups [@zhang_2008, @gros_2012].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW Algorithm\n",
    "\n",
    "notes from @giorgino_ComputingVisualizingDynamic_2009.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "$X$: the *test* or *query*\n",
    "$Y$: the *reference*\n",
    "\n",
    "*i*: index of $X$\n",
    "*j*: index of $Y$\n",
    "\n",
    "*f*: *local dissimilarity function* defined between pairs of $x_i$ and $y_j$. Non negative: $$d(i, j)=f(x_i, y_j) \\geq 0$$\n",
    "\n",
    "$d$: cross-distance matrix between $X$ and $Y$\n",
    "\n",
    "$\\phi(k)$: warping curve $$\\phi(k)=(\\phi_x(k), \\phi_y(k))$$\n",
    "\n",
    "Where $\\phi_x(k), \\phi_y(k)$ outputs an integer from 1 to N.\n",
    "\n",
    "$\\phi_x$ remaps $X$ time indices\n",
    "$\\phi_y$ remaps $Y$ time indices\n",
    "\n",
    "There is a average accumulated distortion between the warped $X$ and $Y$: $$d_\\phi(X, Y) = \\sum_{k=1}^{T} d(\\phi_x(k), \\phi_y(k)) \\frac{m_\\phi(k)}{M_\\phi}$$\n",
    "\n",
    "$m_\\phi(k)$: per-step weighting coefficient, \n",
    "$M_\\phi(k)$: normalizing constant of $m_\\phi(k)$\n",
    "\n",
    " $\\phi$ is constrained to ensure reasonable results. One constraint is monotonicity to ensure time ordering/avoid unnecessary loops: $$\\phi_x(k+1) \\geq \\phi_x(k)$$  $$\\phi_y(k+1) \\geq \\phi_y(k)$$\n",
    "\n",
    "The goal of DTW is to minimize the distance between $X$ and $Y$: $$D(X, Y)=min \\space d_\\phi(X, Y)$$\n",
    "\n",
    "Note: this mentions that Y is also deformed \"The deformation of the time axes of $X$ **and** $Y$\" (emphasis mine).\n",
    "\n",
    "DTW can be computed in $O(N \\cdot M)$ tiime.\n",
    "\n",
    "$D(X, Y)$: \"minimum global dissimilarity\", \"DTW distance\". Stretch insensitive measure of the 'inherent difference' between $X$ and $Y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dtwalign\n",
    "\n",
    "`dtwalign` is a Python package that includes outputting the alignment path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pybaselines import Baseline\n",
    "from dtwalign import dtw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wine_analysis_hplc_uv.notebooks.dtw_methods import DTWNotebookMethods\n",
    "from wine_analysis_hplc_uv.signal_processing.mindex_signal_processing import (\n",
    "    SignalProcessor,\n",
    ")\n",
    "\n",
    "scipro = SignalProcessor()\n",
    "\n",
    "nb_mtds = DTWNotebookMethods()\n",
    "\n",
    "df = pd.read_parquet(definitions.XPRO_YPRO_DOWNSAMPLED_PARQ_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop an optimal alignment, we will focus on the alignment of 2021 John Duval Shiraz with 2021 Torbreck Struie. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join x, y and aligned x on index\n",
    "\n",
    "x = df.loc[:, [\"176\"]].pipe(\n",
    "    lambda df: df.set_axis(\n",
    "        axis=1,\n",
    "        labels=pd.MultiIndex.from_arrays(\n",
    "            [[\"176\"], [\"query\"], [\"NA\"], [\"abs\"]],\n",
    "            names=[\"sample\", \"status\", \"window_size\", \"unit\"],\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "y = df.loc[:, [\"177\"]].pipe(\n",
    "    lambda df: df.set_axis(\n",
    "        axis=1,\n",
    "        labels=pd.MultiIndex.from_arrays(\n",
    "            [[\"177\"], [\"ref\"], [\"NA\"], [\"abs\"]],\n",
    "            names=[\"sample\", \"status\", \"window_size\", \"unit\"],\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align 176 on 177 without any constraints\n",
    "\n",
    "align_x = nb_mtds.dtw_align_series(x, y)\n",
    "\n",
    "align_x.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the alignment plane plot and series overlays, a significant section of the query series has been compressed and then interpolated as a flat line, losing a number of peaks in the process, see below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata, g = nb_mtds.query_ref_align_plot(x, y, align_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, numerous peaks are lost post-alignment, which is unacceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing\n",
    "\n",
    "When aligning a query and reference chromatogram of similar samples we have the expectation that peaks within $d_m$ distance, where $d_m$ is misalignment distance, are the same compound and will be aligned to the same retention time post warping. Conversely, we do not expect regions where the query has peaks but the reference does not to be altered. Unfortunatey, the algorithm does not by default contain that information, and without a global constraint it will drastically alter the query along the entire serie to minimize what it percieves as the distance between the two, including compressing peaks present in the query but not the reference:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we cam see the query is modified by the algorithm to match the reference in both the x and y axes. This leads to an unexpectedly deformed query. What is needed is to restrict warping to localized regions in the form of a windowing operation with windows of specific geometry. One such is the Sakoe-Chiba band [@sakoe_1978] which restricts how far apart two elements can be when matched: $$|\\phi_x(k)-\\phi_y(k)| \\leq T_0$$ where $T_0$ is the absolute time deviation between two matched elements, specified by the user. As described by @giorgino_ComputingVisualizingDynamic_2009, this creates a boundary within the alignment plane within which the warping path can exist. For a window of size 10:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the windowed aligned x, assign a regular frequency time index, rename axes\n",
    "\n",
    "\n",
    "sakoechiba_10_x_align = nb_mtds.dtw_align_series(\n",
    "    x, y, dict(window_type=\"sakoechiba\", window_size=10)\n",
    ")\n",
    "sakoechiba_10_x_align.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks promising, how does it compare to the query and the reference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata, __ = nb_mtds.query_ref_align_plot(x=x, y=y, x_align=sakoechiba_10_x_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    plotdata.loc[:, pd.IndexSlice[:, :, :, :, \"query and aligned query\"]]\n",
    "    .melt(ignore_index=False, value_name=\"mAU\")\n",
    "    .pipe(lambda df: df.set_index(keys=df.index.total_seconds() / 60))\n",
    "    .pipe(sns.lineplot, x=\"mins\", y=\"mAU\", hue=\"status\")\n",
    "    .set_title(\"query and aligned query\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Selecting a Sakoechiba window with size 10 dramaticaly alters the behavior of the warping, preserving peaks that were otherwise lost. It also appears as though the (in the default setting at least), baseline height differences affect the warp. It appears that reducing the distance/cost pre-warp will reduce overall warping errors. Before we continue we should examine the effects of baseline subtraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTW with Subtracted Baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same x and y, assess effect of DTW on baseline subtracted series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([x, y], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename({\"abs\": \"value\"}, axis=1).pipe(\n",
    "    lambda df: df.set_axis(df.columns.set_names(\"signal\", level=\"unit\"), axis=1)\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lam of 1000 has been chosen as that appears to fit the baseline to the base of each peak without fitting to the internal area of the peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the baseline\n",
    "\n",
    "from pybaselines import Baseline\n",
    "\n",
    "\n",
    "def assign_baseline_correction(df) -> pd.DataFrame:\n",
    "    df = data.pipe(\n",
    "        lambda df: df.melt(ignore_index=False, value_name=\"raw\")\n",
    "        .drop(\"signal\", axis=1)\n",
    "        .pipe(\n",
    "            lambda df: df.groupby(\"sample\", group_keys=False).apply(\n",
    "                lambda grp: grp.assign(\n",
    "                    bline=Baseline(grp.index.total_seconds()).asls(grp.raw, lam=1000)[0]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .pipe(\n",
    "            lambda df: df.groupby(\"sample\", group_keys=False).apply(\n",
    "                lambda grp: grp.assign(bcorr=grp.raw - grp.bline)\n",
    "            )\n",
    "        )\n",
    "        .pivot(\n",
    "            columns=[\"sample\", \"status\", \"window_size\"],\n",
    "            values=[\"raw\", \"bline\", \"bcorr\"],\n",
    "        )\n",
    "        .pipe(lambda df: df.set_axis(df.columns.set_names(\"signal\", level=0), axis=1))\n",
    "        .reorder_levels(order=[\"sample\", \"status\", \"window_size\", \"signal\"], axis=1)\n",
    "        .sort_index(axis=1)\n",
    "    )\n",
    "    (\n",
    "        df.melt(ignore_index=False)\n",
    "        .pipe(sns.FacetGrid, col=\"sample\")\n",
    "        .map_dataframe(sns.lineplot, x=\"mins\", y=\"value\", hue=\"signal\")\n",
    "        .add_legend()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "data = assign_baseline_correction(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets observe how the warping behaves for the same parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_align_series = nb_mtds.dtw_align_series(\n",
    "    x=data.loc[:, pd.IndexSlice[\"176\", :, :, \"bcorr\"]],\n",
    "    y=data.loc[:, pd.IndexSlice[\"177\", :, :, \"bcorr\"]],\n",
    ")\n",
    "data = (\n",
    "    pd.concat([data.loc[:, pd.IndexSlice[:, :, :, \"bcorr\"]], dtw_align_series], axis=1)\n",
    "    .sort_index(axis=1)\n",
    "    .droplevel(axis=1, level=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "def plot_bcorr_dtw(data):\n",
    "    x = data.loc[:, idx[:, \"query\", :]]\n",
    "    y = data.loc[:, idx[:, \"ref\", :]]\n",
    "    x_align = data.loc[:, idx[:, \"aligned\", :]]\n",
    "\n",
    "    sp1 = data.loc[:, pd.IndexSlice[:, [\"query\", \"ref\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"query and reference\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # sp2 is aligned and ref\n",
    "\n",
    "    sp2 = data.loc[:, pd.IndexSlice[:, [\"aligned\", \"ref\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"aligned query and reference\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ## sp3 is query and aligned\n",
    "\n",
    "    sp3 = data.loc[:, pd.IndexSlice[:, [\"query\", \"aligned\"]]].pipe(\n",
    "        lambda df: df.set_axis(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame().assign(subplot=\"query and aligned query\")\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # concatenate the three subplot dataframes, melt, renaming the value column to 'mAU', convert the date time index to a minutes float, create a column-wise facetgrid on 'subplot', map a lineplot to the facetgrids of 'mins', 'mAU', 'status' for hue, set the subplot titles to subplot value.\n",
    "\n",
    "    plotdata = pd.concat([sp1, sp2, sp3], axis=1)\n",
    "\n",
    "    g = (\n",
    "        plotdata.melt(ignore_index=False, value_name=\"mAU\")\n",
    "        .pipe(lambda df: df.set_index(df.index.total_seconds() / 60))\n",
    "        .pipe(\n",
    "            lambda df: sns.FacetGrid(df, col=\"subplot\")\n",
    "            .map_dataframe(sns.lineplot, x=\"mins\", y=\"mAU\", hue=\"status\")\n",
    "            .set_titles(col_template=\"{col_name}\")\n",
    "            .add_legend()\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "plot_bcorr_dtw(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would call that successful without any further modifications to the dtw algorithm. At this point in time we need a number of tools -\n",
    "\n",
    "1. a method of evaluating alignment\n",
    "2. produce a matrix of subplots for each sample in the set row wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a way to construct the subplot dataset through groupby on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a 2x2 plot grid of each sample with an overlay of the raw, the baseline and\n",
    "# the bcorr signals\n",
    "\n",
    "df = (\n",
    "    df.melt(ignore_index=False, value_name=\"raw\")\n",
    "    .groupby(\"samplecode\", group_keys=False)\n",
    "    .apply(\n",
    "        lambda grp: grp.assign(\n",
    "            bline=Baseline(grp.index.total_seconds()).asls(grp.raw, lam=1000)[0]\n",
    "        )\n",
    "    )\n",
    "    .assign(bcorr=lambda df: df.raw - df.bline)\n",
    "    .pivot(columns=[\"samplecode\", \"wine\"], values=[\"raw\", \"bline\", \"bcorr\"])\n",
    "    .pipe(lambda df: df.set_axis(df.columns.set_names(level=0, names=\"signal\"), axis=1))\n",
    "    .reorder_levels(axis=1, order=[\"samplecode\", \"wine\", \"signal\"])\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "\n",
    "(\n",
    "    df.melt(ignore_index=False, value_name=\"mAU\")\n",
    "    #  .pipe(lambda df: df.set_index())\n",
    "    .pipe(sns.FacetGrid, col=\"wine\", col_wrap=2)\n",
    "    .map_dataframe(sns.lineplot, hue=\"signal\", x=\"mins\", y=\"mAU\")\n",
    "    .set_titles(col_template=\"{col_name}\")\n",
    "    .add_legend()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline corrected signals appear to be acceptable to me. Does it modify which is the reference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = scipro.most_correlated(df)\n",
    "ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does, 176 is now the most correlated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, idx[ref, :, \"bcorr\"]]\n",
    "\n",
    "# add aligned series to original df through concatenation. Need to subset the original\n",
    "# with the warping path, reindex to 2S timedelta range, rename signal level to 'aligned\n",
    "# then concatentate with original df\n",
    "\n",
    "df = (\n",
    "    df.loc[:, idx[:, :, \"bcorr\"]]\n",
    "    .pipe(\n",
    "        lambda df: df.groupby([\"wine\"], group_keys=False, axis=1).apply(\n",
    "            lambda df: pd.concat(\n",
    "                [\n",
    "                    df,\n",
    "                    df.iloc[dtw(x=df, y=y).get_warping_path(), :].pipe(\n",
    "                        lambda df: df.set_index(\n",
    "                            pd.timedelta_range(\n",
    "                                start=df.index[0], end=df.index[-1], freq=\"2S\"\n",
    "                            )\n",
    "                        )\n",
    "                        .rename_axis(\"mins\")\n",
    "                        .rename({\"bcorr\": \"aligned\"}, axis=1)\n",
    "                    ),\n",
    "                ],\n",
    "                axis=1,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .pipe(\n",
    "        lambda df: df.reindex(\n",
    "            axis=1,\n",
    "            labels=pd.MultiIndex.from_frame(\n",
    "                df.columns.to_frame()\n",
    "                .assign(role=\"query\")\n",
    "                .assign(\n",
    "                    role=lambda df: df.loc[:, \"role\"].where(\n",
    "                        ~(df.samplecode.isin(ref)), \"ref\"\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    .reorder_levels(axis=1, order=[\"samplecode\", \"wine\", \"role\", \"signal\"])\n",
    "    .sort_index(axis=1)\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to form the sets of three again.\n",
    "\"\"\"\n",
    "we are expecting to create a column 'subplot' with a pattern of 1,2,3 and for each\n",
    "sample a combination of ('query','bcorr'), ('query','aligned'), and ('ref','bcorr').\n",
    "\n",
    "specifically,\n",
    "\n",
    "1: ('query','bcorr'), ('ref','bcorr')\n",
    "2: ('query','aligned'), ('ref','bcorr')\n",
    "3: ('query','bcorr'), ('query','aligned')\n",
    "\n",
    "For each we're expecting 2 rows per sample, 8 rows for subplot 1, 8 rows for subplot 2, 8 rows for subplot 3.\n",
    "We need combinations with repeats using the cartesian product (?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# get the reference\n",
    "\n",
    "\n",
    "\n",
    "new_index_ = (df\n",
    " .columns\n",
    " .to_frame(index=False)\n",
    " .assign(state=lambda df: df\n",
    "         .signal\n",
    "         .where(~(df.signal=='aligned'),'aligned')\n",
    "         .where(~(df.signal=='bcorr'),'query')\n",
    "         .where(~(df.role=='ref'),'ref')\n",
    "         )\n",
    " .drop(['role','signal'],axis=1)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index=pd.MultiIndex.from_frame(new_index_)\n",
    "df = df.set_axis(new_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = df.columns.to_frame(index=False).set_index([\"samplecode\", \"wine\", \"state\"]).loc[~a.index.duplicated(keep='first'),:]\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_idx = a.loc[idx[:, :, 'ref']].assign(subplot=\"1\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"         \n",
    "| sample  | row  | col  | role  |\n",
    "|---------|------|------|-------|\n",
    "| sample1 | row1 | col1 | query |\n",
    "| sample1 | row1 | col1 | ref   |\n",
    "| sample1 | row1 | col2 | align |\n",
    "| sample1 | row1 | col2 | ref   |\n",
    "| sample1 | row1 | col3 | query |\n",
    "| sample1 | row1 | col3 | align |\n",
    "\n",
    "do for each, then concat horizontally.\n",
    "\n",
    "The iteration pattern is:\n",
    "\n",
    "sample 1, 2, 3\n",
    "row 1, 2, 3\n",
    "column 1, 2, 3\n",
    "\n",
    "for sample in samples:\n",
    "    for column in range(0:2):\n",
    "        col1 = \n",
    "\"\"\"\n",
    "\n",
    "samples = a.index.get_level_values(\"samplecode\").unique()\n",
    "\n",
    "\n",
    "# get sample row\n",
    "def build_col1(df, sample_id):\n",
    "    \"\"\"\n",
    "    combine query and reference for overlaying in col1\n",
    "    \"\"\"\n",
    "    \n",
    "    if sample_id=='176':\n",
    "        sample = df.loc[idx[sample_id, :, \"ref\"], :]\n",
    "    \n",
    "    else:\n",
    "        sample = df.loc[idx[sample_id, :, \"query\"], :]\n",
    "\n",
    "\n",
    "    # get the reference row, reindex it so its 'wine' (row) is s1\n",
    "    \n",
    "    if sample_id=='176':\n",
    "        ref=df.loc[idx[:,:,'ref'],:]\n",
    "    \n",
    "    else:\n",
    "        wine = sample.index.get_level_values(\"wine\")\n",
    "        ref = df.loc[idx[:, :, \"ref\"], :].pipe(\n",
    "            lambda df: df.set_axis(\n",
    "                df.index.remove_unused_levels().set_levels(level=[\"wine\"], levels=[wine])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # assign row and column identifier to reference and sample rows\n",
    "    col1 = pd.concat([sample, ref]).assign(col=1)\n",
    "\n",
    "    return col1\n",
    "\n",
    "\n",
    "def build_col2(df, sample_id):\n",
    "    \"\"\"\n",
    "    combine aligned and reference for overlaying in col1\n",
    "    \"\"\"\n",
    "    if sample_id=='176':\n",
    "        sample = df.loc[idx[sample_id, :, \"ref\"], :]\n",
    "    \n",
    "    else:\n",
    "        sample = df.loc[idx[sample_id, :, \"aligned\"], :]\n",
    "\n",
    "\n",
    "    # get the reference row, reindex it so its 'wine' (row) is s1\n",
    "    \n",
    "    if sample_id=='176':\n",
    "        ref=sample.copy()\n",
    "    \n",
    "    else:\n",
    "        wine = sample.index.get_level_values(\"wine\")\n",
    "        ref = df.loc[idx[:, :, \"ref\"], :].pipe(\n",
    "            lambda df: df.set_axis(\n",
    "                df.index.remove_unused_levels().set_levels(level=[\"wine\"], levels=[wine])\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # assign row and column identifier to reference and sample rows\n",
    "    \n",
    "    col2 = pd.concat([sample, ref]).assign(col=2)\n",
    "    \n",
    "    return col2\n",
    "\n",
    "\n",
    "def build_col3(df, samplecode):\n",
    "    \"\"\"\n",
    "    combined query and aligned\n",
    "    \"\"\"\n",
    "\n",
    "    if samplecode=='176':\n",
    "        col3 = df.loc[[samplecode]].assign(col=3)\n",
    "        \n",
    "    else:\n",
    "        col3 = df.loc[idx[samplecode, :, [\"query\", \"aligned\"]], :].assign(col=3)\n",
    "\n",
    "    return col3\n",
    "\n",
    "\n",
    "col1 = pd.concat([build_col1(a, sample) for sample in samples])\n",
    "col1 = col1.assign(row=col1.groupby(\"wine\").ngroup() + 1)\n",
    "col2 = pd.concat([build_col2(a, sample) for sample in samples]).assign(\n",
    "    row=lambda df: df.groupby(\"wine\").ngroup() + 1\n",
    ")\n",
    "col3 = pd.concat([build_col3(a, sample) for sample in samples]).assign(\n",
    "    row=lambda df: df.groupby(\"wine\").ngroup() + 1\n",
    ")\n",
    "\n",
    "index_df = pd.concat([col1, col2, col3])\n",
    "# index_df = index_df[[\"row\", \"col\"]]\n",
    "\n",
    "\n",
    "\n",
    "index_df.set_index([\"row\", \"col\"], append=True).reorder_levels(\n",
    "    [\"row\", \"col\", \"wine\", \"state\",\"samplecode\"]\n",
    ").sort_index(level=[\"row\", \"col\", \"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we just reindex the original df?\n",
    "\n",
    "\n",
    "def join_df_index_df(df, index_df):\n",
    "    \"\"\"\n",
    "    massage df and index_df to left join onto index_df\n",
    "    \"\"\"\n",
    "\n",
    "    pdf =(\n",
    "        df\n",
    "        .melt(ignore_index=False)\n",
    "        .reset_index()\n",
    "        .set_index(['samplecode','state'])\n",
    "        .drop('wine',axis=1) \n",
    "    )\n",
    "\n",
    "    pindex_df = index_df.reset_index().set_index([\"samplecode\",'state'])\n",
    "    display('pindex_df')\n",
    "    display(pindex_df)\n",
    "    display('pdf')\n",
    "    display(pdf)\n",
    "    join = pindex_df.join(pdf, how='left').dropna()\n",
    "\n",
    "    return join\n",
    "\n",
    "\n",
    "relplotdata = join_df_index_df(df, index_df)\n",
    "display(relplotdata)\n",
    "display(relplotdata.describe())\n",
    "display(relplotdata.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert timedelta to float\n",
    "\n",
    "relplotdata = relplotdata.assign(mins=lambda df: df.mins.dt.total_seconds() / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means = df.mean().to_frame(\"mean\").reset_index().set_index(\"mean\")\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if joins acted as expected by calulating an aggregation statisic, say mean.\n",
    "\n",
    "\n",
    "post_join_means = (\n",
    "    relplotdata.groupby([\"samplecode\", \"wine\", \"state\", \"row\", \"col\"])[\"value\"]\n",
    "    .mean()\n",
    "    .to_frame(name=\"mean\")\n",
    ")\n",
    "\n",
    "post_join_means = (\n",
    "    post_join_means.reorder_levels([\"row\", \"col\", \"samplecode\", \"wine\", \"state\"])\n",
    "    .reset_index()\n",
    "    .set_index(\"mean\")\n",
    "    .\n",
    ")\n",
    "post_join_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_means.loc[lambda df: ~df.index.duplicated(keep='first'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_join_means.join(df_means, lsuffix=\"left\", rsuffix=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([df_means, post_join_means],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    relplotdata.set_index([\"row\", \"col\"], append=True)\n",
    "    .loc[idx[:, 1, 3]]\n",
    "    .pipe(lambda df: df if display(df) else df)  # display df\n",
    "    #  .reset_index()\n",
    "    .pipe(sns.relplot, x=\"mins\", y=\"value\", hue=\"state\", legend=\"full\", kind=\"line\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now display it..\n",
    "\n",
    "sns.relplot(\n",
    "    relplotdata,\n",
    "    col=\"col\",\n",
    "    row=\"row\",\n",
    "    x=\"mins\",\n",
    "    y=\"value\",\n",
    "    hue=\"state\",\n",
    "    kind=\"line\",\n",
    "    legend=\"full\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = df.loc[idx[:, :, [\"query\", \"ref\"], \"bcorr\"], :].assign(col=\"col1\")\n",
    "display(col1)\n",
    "\n",
    "col2 = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df.loc[idx[:, :, \"query\", \"aligned\"], :],\n",
    "            df.loc[idx[:, :, \"ref\", \"bcorr\"], :],\n",
    "        ]\n",
    "    )\n",
    "    .sort_index()\n",
    "    .assign(col=\"col2\")\n",
    ")\n",
    "\n",
    "display(col2)\n",
    "\n",
    "col3 = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            df.loc[idx[:, :, \"query\", \"bcorr\"], :],\n",
    "            df.loc[idx[:, :, \"query\", \"aligned\"], :],\n",
    "        ]\n",
    "    )\n",
    "    .sort_index()\n",
    "    .assign(col=\"col3\")\n",
    ")\n",
    "\n",
    "display(col3)\n",
    "\n",
    "pdata = pd.concat([col1, col2, col3]).sort_index()\n",
    "pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp3 = a[(a['role']=='query') & (a['signal']=='bcorr') | (a['role']=='query') & (a['signal']=='aligned')].assign(subplot='3')\n",
    "\n",
    "# display(sp3)\n",
    "\n",
    "# subplots_index = pd.concat([sp1,sp2,sp3], ignore_index=True)\n",
    "\n",
    "# subplots_index.set_index(['samplecode','wine','role','signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use `relplot` rather than FacetGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = df.reindex(pd.MultiIndex.from_frame(subplots_index), axis=1)\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    plot_df.melt(ignore_index=False, value_name=\"mAU\").pipe(\n",
    "        lambda df: sns.FacetGrid(df, col=\"subplot\", row=\"wine\")\n",
    "        .map_dataframe(sns.lineplot, x=\"mins\", y=\"mAU\", hue=\"role\")\n",
    "        .add_legend()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sns.relplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
