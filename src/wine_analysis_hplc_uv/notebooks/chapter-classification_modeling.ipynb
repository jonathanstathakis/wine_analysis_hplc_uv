{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Untitled\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML in Chemistry\n",
    "\n",
    "ML applications are becoming more mainstream [@joshi_2023, @baum_2021].\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "- An improvement to the decision tree model first published by Tianqi Chen and Carolos Guestrin of the University of Washington in 2016 [@chen_2016].\n",
    "\n",
    "## XGBoost in Chemometrics\n",
    "\n",
    "- @mustapha_2016 thoroughly reviewed the application of contemporary classification models on seven different biomolecule datasets and found that XGBoost performed best with an accuracy ranging from 94.47% - 98.49%.\n",
    "\n",
    "### XGBoost in Chromatography\n",
    "\n",
    "- @tian_2021 used XGBoost to classify liquor and colon cancer datasets (seperately) by observing K-means clustering extracted features, with 100% success rate.\n",
    "- @guan_2023 used XGBoost with LC-MS/MS amino acid data fused with patient demographic information to predict lung cancer occurance with an accuracy of 75.29%\n",
    "\n",
    "### XGBoost in Spectroscopy\n",
    "\n",
    "- most recently @vanwyngaard_2023 compared the combination of Infrared Spectroscopy techniques with XGBoost to predict properties of grapevine organs.\n",
    "- @yokoyama_2022 compared the use of NMR spectroscopy and a selection of ML models including XGBoost to predict chemical compound effects on aquaculture membranes, with XGBoost the most performant.\n",
    "- @zou2023 profiled peanut seeds with hyperspectral imaging and constructed an XGBoost classification model with 80% accuracy.\n",
    "- @ranaweera-AuthenticationGeographicalOrigin-2021 modelled Cabernet Sauvignon wine A-TEEM data to classify by geographic origin with 100% accuracy.\n",
    "\n",
    "## XGBoost in Python\n",
    "\n",
    "XGBoost is available as a stand-alone Python package [xgboost](https://xgboost.readthedocs.io/en/stable/install.html).\n",
    "\n",
    "It provides an interface through Scikit-Learn as described [here](https://xgboost.readthedocs.io/en/stable/python/sklearn_estimator.html) and provides regression, classificaton, and ranking. According to [this page](https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator) the primary motivation for this is integration with Scikit-Learn's `model_selection.GridSearchCV` and `pipeline.Pipeline` tools, as well as cross-library compatibility as other third-party libraries rely on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost For Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "A CUPRAC red wine dataset at 450nm has been constructed and stored to parquet file in [processing_cup_rw_dset](./processing_cup_rw_dset.ipynb). The filepath to the data file is stored in `definitons.RW_CUP_450_PROCESSED`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Run Classification model\n",
    "\n",
    "A first attempt at a classification model will be undertaken in [xgboost_modeling](./xgboost_modeling.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
