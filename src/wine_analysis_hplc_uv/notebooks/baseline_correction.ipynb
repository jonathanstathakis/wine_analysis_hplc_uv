{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Background Correction\n",
    "\n",
    "[@kensart_2021] developed a ANN written in Python (Tensorflow) that is able to perform unsupervised background correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Niezen Solution\n",
    "\n",
    "[@niezen_2022] performed an in-depth analysis of a number of background correction methods and concluded that:\n",
    "\n",
    "| method          | outcome                          |\n",
    "|-----------------|----------------------------------|\n",
    "| SASS and arPLS  | lowest RMSE, best looking result |\n",
    "| SASS and LMV    | smallest error in peak area      |\n",
    "| backcor and LMV | fastest drift correction         |\n",
    "| arPLS           | slowest drift correction         |\n",
    "\n",
    "LMV = Local Minimum Values. A method developed by [@fu_2016]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Baseline Algorithms\n",
    "\n",
    "TODO:\n",
    "- [ ] complete notes on:\n",
    "  - [ ] splines\n",
    "  - [ ] smoothing\n",
    "  - [ ] classification\n",
    "  - [ ] optimizer\n",
    "  - [ ] misc\n",
    "- [ ] complete top level table describing each class\n",
    "- [ ] justify why iasls has been selected\n",
    "- [ ] \n",
    "\n",
    "According to PyBaselines, there are the following categories of baseline calculating algorithms:\n",
    "* polynomial\n",
    "* Whittaker-smoothing\n",
    "* morphological\n",
    "* spline\n",
    "* smoothing\n",
    "* baseline/peak classification\n",
    "* optimizers\n",
    "* misc.\n",
    "\n",
    "| CLASS | DESCRIPTION |\n",
    "|---|---|\n",
    "| Polynomial | Relies on minimizing the least squares. |\n",
    "| Whittaker | Also utilizes least squares, but includes cost functions that reward smoothness |\n",
    "| Spline | Combines least squares methods with localised optimizations |\n",
    "| Smoothing |test | \n",
    "| Classification |test|\n",
    "| Optimizer |test|\n",
    "| Misc. |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whittaker Baselines\n",
    "\n",
    "* Whittaker-smoothing-based algorithms are also known as weighted least squares, penalized least squares, or asymmetric least squares .\n",
    "* They work by making the baseline match the data while penalizing roughness to avoid overfitting.\n",
    "* The core function is $$\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$$ and the linear equation to solve the minimization is: $$(W + \\lambda D_d^{\\top} D_d) z = W y$$\n",
    "* It is generally recommended to use the 2nd order difference matrix, but some adaptions use both the first and second order.\n",
    "* The algorithm works iteratively by solving for the baseline $z$, updating the weights and solving for $z$ again, so on and so forth until a preset criteria is reached. (JS - how are the weights reset?, how is the criteria determined?)\n",
    "\n",
    "| Abbreviation | Full Name                | Equation                                                                                 | Weighting | \n",
    "|--------------|--------------------------|------------------------------------------------------------------------------------------|---|\n",
    "| asls         | Asymmetric Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ |$w_i = \\begin{array}{cr}p & y_i > z_i \\\\1 - p & y_i \\le z_i\\end{array}$ |\n",
    "| iasls | Improved Assymmetric Least Squares | $\\sum\\limits_{i}^N (w_i (y_i - z_i))^2 + \\lambda \\sum\\limits_{i}^{N - 2} (\\Delta^2 z_i)^2 + \\lambda_1 \\sum\\limits_{i}^{N - 1} (\\Delta^1 (y_i - z_i))^2$ | $w_i = \\begin{array}{cr}p & y_i > z_i \\\\1 - p & y_i \\le z_i\\end{array}$ |\n",
    "| airpls | Adaptive Iteratively Reweighted Penalized Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ | $w_i = \\begin{array}{cr}0 & y_i \\ge z_i \\\\exp{(\\frac{t (y_i - z_i)}{\\|\\mathbf{r}^-\\|}} & y_i < z_i\\end{array}$ |\n",
    "| arpls | Asymmetrically Reweighted Penalized Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ | $w_i = \\frac{1}{1 + exp{\\left(\\frac{2(r_i - (-\\mu^- + 2 \\sigma^-))}{\\sigma^-}\\right)}}$ |\n",
    "| drpls | Doubly Reweighted Penalized Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2+ \\lambda \\sum\\limits_{i}^{N - 2}(1 - \\eta w_i) (\\Delta^2 z_i)^2+ \\sum\\limits_{i}^{N - 1} (\\Delta^1 (z_i))^2$ | $w_i = \\frac{1}{2}\\left(1 -\\frac{exp(t)(r_i - (-\\mu^- + 2 \\sigma^-))/\\sigma^-}{1 + abs[exp(t)(r_i - (-\\mu^- + 2 \\sigma^-))/\\sigma^-]}\\right)$ |\n",
    "| iarpls | Improved Asymmetrically Reweighted Penalized Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ | $w_i = \\frac{1}{2}\\left(1 -\\frac{exp(t)(r_i - 2 \\sigma^-)/\\sigma^-}{\\sqrt{1 + [exp(t)(r_i - 2 \\sigma^-)/\\sigma^-]^2}}\\right)$ |\n",
    "| aspls | Adaptive Smoothness Penalized Least Squares | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2+ \\lambda \\sum\\limits_{i}^{N - d} \\alpha_i (\\Delta^d z_i)^2$ | $w_i = \\frac{1}{1 + exp{\\left(\\frac{0.5 (r_i - \\sigma^-)}{\\sigma^-}\\right)}}$ |\n",
    "| psalsa | Peaked Signal's Asymmetric Least Squares Algorithm |  $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ | $w_i = \\begin{array}{cr} p \\cdot exp{\\left(\\frac{-(y_i - z_i)}{k}\\right)} & y_i > z_i \\\\ 1 - p & y_i \\le z_i \\end{array}$ |\n",
    "| derpsalsa | Derivative Peak-screening Asymmetric Least Squares Algorithm | $\\sum\\limits_{i}^N w_i (y_i - z_i)^2 + \\lambda \\sum\\limits_{i}^{N - d} (\\Delta^d z_i)^2$ | $w_i = w_{0i} * w_{1i} * w_{2i}$* |\n",
    "\n",
    "* $w_{0i} = \\begin{array}{cr}p \\cdot exp{\\left(\\frac{-[(y_i - z_i)/k]^2}{2}\\right)} & y_i > z_i \\\\1 - p & y_i \\le z_i\\end{array}$, $w_{1i} = exp{\\left(\\frac{-[y_{sm_i}' / rms(y_{sm}')]^2}{2}\\right)}$, $w_{2i} = exp{\\left(\\frac{-[y_{sm_i}'' / rms(y_{sm}'')]^2}{2}\\right)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## polynomial\n",
    "\n",
    "### Fitting a polynomial\n",
    "\n",
    "For the polynomial: $$p(x) = \\beta_0 x^0 + \\beta_1 x^1 + \\beta_2 x^2 + ... + \\beta_m x^m = \\sum\\limits_{j = 0}^m {\\beta_j x^j}$$\n",
    "\n",
    "Fitting the polynomial can be achieved by minimizing the least-squares: $$\\sum\\limits_{i}^N w_i^2 (y_i - p(x_i))^2$$ \n",
    "\n",
    "where $x_i$, $y_i$ is the measured data, $p(x_i)$ is the polynomial estimate at $x_i$, and $w_i$ is the weighting. \n",
    "\n",
    "### Fitting a baseline\n",
    "\n",
    "To fit only the baseline of a polynomial, the least-squares algorithm must be altered to disregard peaks. This can be achieved through selective masking, thresholding or penalizing outliers.\n",
    "\n",
    "#### Selective masking\n",
    "\n",
    "The oldest method of fitting a baseline, it involves removing all peak regions from the dataset then fitting the remaining points. This is not a recommended approach as the manual selection of the baseline region is time-consuming and subjective, reducing the reproducibility of results.\n",
    "\n",
    "#### thresholding\n",
    "\n",
    "Thresholding is a two-fold process where a least-squares fit is established then a comparison is made between the fit and each data point in an iterative process. In each iteration The minimum between the datapoint and the fit is found, and the fit is adjusted. This continues until an exit criteria is reached.\n",
    "\n",
    "#### Penalizing outliers\n",
    "\n",
    "This approach lends less weight to outliers (i.e. peaks) when fitting the baseline.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "| abbrev. | Full Name | Description |\n",
    "|---|---|---|\n",
    "| poly | Regular Polynomial | Least squares polynomial fitting, use with selective masking |\n",
    "| modpoly/ModPolyFit | Modified Polynomial | polynomial fitting with thresholding |\n",
    "| imodpoly/IModPolyFit | Improved Modified Polynomial | adaption of modpoly for noisy data by including the stdev of the residual during thresholding |\n",
    "| penalized_poly/backcor | Penalized Polynomial |  uses non-squadratic cost functions - Huber, truncated-quaratic, indec |\n",
    "| loess/rbe | Locally Estimated Scatterplot Smoothing/Robust Baseline Estimate | element-wise calculation of baseline by applying polynomial regression on the k-nearest neighbours of the element. Outliers are reduced by iterative reweighting |\n",
    "| quant_reg | Quantile Regression | Uses quantile regression |\n",
    "| goldinc | Goldindec Method | Uses asyummetric non-quadratic cost functions |\n",
    "\n",
    "Source [bybaselines](https://pybaselines.readthedocs.io/en/latest/algorithms/polynomial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline Baselines\n",
    "\n",
    "- PyBaselines uses 'B-splines'\n",
    "\n",
    "$$z(x) = \\sum\\limits_{i}^N \\sum\\limits_{j}^M {B_j(x_i) c_j}$$\n",
    "\n",
    "where $N$ is the number of points in $x$, $M$ is the number of spline basis functions, $B_j(x_i)$ is the j-th basis function evaluated at $x_i$, and $c_j$ is the coefficient for the j-th basis (which is analogous to the height of the j-th basis). $M$ is calculated as the number of knots plus the spline degree minus 1.\n",
    "\n",
    "- A zBspline is formed by minimizing the least squares: $$\\sum\\limits_{i}^N w_i (y_i - \\sum\\limits_{j}^M {B_j(x_i) c_j})^2$$\n",
    "\n",
    "- T control smoothness of baseline, a penalty is added in the form of a cost function. These kinds of B-splines are called P-splines: $$\\sum\\limits_{i}^N w_i (y_i - \\sum\\limits_{j}^M {B_j(x_i) c_j})^2+ \\lambda \\sum\\limits_{i}^{M - d} (\\Delta^d c_i)^2$$ where $\\lambda$ is the penalty scale factor, $\\Delta^d$ is the finite difference operator of order $d$. This is solved with the linear equation: $$(B^{\\top} W B + \\lambda D_d^{\\top} D_d) c = B^{\\top} W y$$ where $W$ is the diagonal matrix of the weights, $B$ is the matrix containing all of the spline basis functions, $D_d$ is the matrix version of $\\Delta^d$.\n",
    "\n",
    "- P-splines can be thought of as an adaption of Whittaker smoothing, a P-spline with $M=N$ and $d=0$ has an equation equal to that of the solution to Whittaker smoothing.\n",
    "\n",
    "### Algorithms\n",
    "\n",
    "| Abbrev. | Full Name | Description |\n",
    "|---|---|---|\n",
    "| mixture_model | Mixture Model | test |\n",
    "| irsqr | Iterative Reweighted Spline Quantile Regression | Quantile regression through P-splines with iterative reweighted least squares |\n",
    "| corner_cutting | Corner-Cutting Method | test |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
