---
title: 'MRes Logbook'
jupyter: python3
toc: true
toc-depth: 3
toc-expand: 2
---

## Links

[mres homepage](../../001_obsidian_vault/homepages/homepage_wsu_research.md)

[dp paper logbook](../../001_obsidian_vault/mres_logbook/dp_paper_logbook/dp_paper_logbook.md)

<!--  contents -->

## 2023-01-20

### 2023 Q1 Funding Request

\$2500

2 x Avantor chromatography columns \$1600

PEEK step down units \$500

Research samples \$400

## 2023-01-23

### Wine Analysis

-   [x] outline analysis methods.
-   [x] prepare column.
-   [x] program instrument.
-   [x] run.
-   [x] observe run, make adjustments to program.
-   [x] run.
-   [x] set to run overnight.

As per \[\[2023-01-18#wine method development\|The method outlined earlier\]\] we will be using H2O and methanol as out phases, and a C18 column.p

1321 :

After getting C18 column from King's, I aimed to wet it to prepare it for HPLC use, at 0.1mL/min flow rate. After wetting the column, flow rate was ramped up to 1mL/min, and run for approximately 1h to flush contaminants and equilibriate the bed. The next thing to do is test the column performance.

The performance test will consist of 3 x 500ppm phenol standard at 1mL/min flow rate, 40% methanol isocratic. We will measure the variance of these samples. anything below 5% variance is acceptable.

In the mean time I will filter the wine samples in preparation for the runs.

#### Wine Sample Table

| vial | wine_name                                  | description                                                                  | sampler_pos |
|:-----------|:--------------|:---------------------------------|:-----------|
| z1   | 2016 Zema Estate Cabernet Sauvignon        | Fridge. Sampled at 21:20 20230122, stored for 10h before transport to lab.   |             |
| z2   | 2016 Zema Estate Cabernet Sauvignon        | Freezer.  Sampled at 21:20 20230122, stored for 10h before transport to lab. |             |
| z3   | 2016 Zema Estate Cabernet Sauvignon        | Ambient.  Sampled at 21:20 20230122.                                         |             |
| 1    | 2022 William Downie 'Cathedral' Pinot Noir | Ambient 2 weeks. Sampled 20230111.                                           |             |
| 2    | 2021 Babo Chianti                          | Ambient 2 weeks. Sampled 20230111.                                           |             |

#### Column Integrity Verification

The phenol sample repeatedly failed to produce a peak, so we resorted to a capsule coffee, which was able to produce a full sample peak. It was decided to complete three replicates of the first 5 minutes of the coffee separation and use the maxima for the variance measurement. As before, conditions were: 1ml/min, 60% H2O 40% methanol, ambient temperature.

| idx           | peak height (mAU) | peak time (min) |
|:--------------|:------------------|:----------------|
| 1             | 283.833           | 3.333           |
| 2             | 287.470           | 3.334           |
| 3             | 284.719           | 3.327           |
| **variance:** | 2.398             | 9.5e-06         |

Peak 2 was the regional maxima, with good geometry, minimal tailing and no shoulders.

Unknown coffee sample, silver body, black foil cap and back, no markings.

Variance in peak time is obviously very acceptable, and, if the acceptable variation on peak height is 10% = \~28.5 mAU, then the peak height variance is acceptable as well. instrument is GTG.

1524:

First wine test was successful, now to develop an acceptable gradient method. It needs to be quick, with adequate peak resolution.

First attempt:

| time (min) | %B       |
|:-----------|:---------|
| 0          | 5%       |
| 8          | 100%<br> |
| 10         | 100%     |
| 11         | 5%       |
| 13         | 5%       |

File: `~\2023-01-23_wine_gradient_1.{}`

Where:

`{}` is `M` or `D`,

and:

`~\` is `C:\Chem32\1\METHODS\0_jono_{}`,

Where:

`{}` is `methods`, `sequences` or `data`.

First attempt failed, injection volume was too small, run time set to 5mins. file has been rectified.

Note: Ensure 3min 100% methanol flush runs between runs to clear the column.

I should set up a wash method to swap to to clear the column.

1558:

Done.

Column Clean Method:

| Time (min) | %A  | %B  | Flow (mL/min) |
|:-----------|:----|:---:|:-------------:|
| 0          | 0   | 100 |      0.1      |
| 1          | 0   | 100 |      0.2      |
| 2          | 0   | 100 |      0.4      |
| 3          | 0   | 100 |      0.6      |
| 4          | 0   | 100 |      0.8      |
| 5          | 0   | 100 |      1.0      |
| 6          | 0   | 100 |      1.0      |

Found in `~\0_jono_column_clean_method.M`

Problem tho, changing the method doesnt change the sample. 1609:

### Method Development

The first method, `../2023-01-25_WINE_GRADIENT_1.M` had an 11 minute ramp up time, i.e. 8.63%/min. The profile can be seen in this \[\[20230123_WINE_TEST_GRAD_1.pdf\|report\]\]. Even at this short time, ther was some seperation, hoewever the baseline is high, and the peaks are poorly resolved.

I then upped the gradient length to 14 mins, a 6.7% gradient, and the report can be found \[\[20230123_WINE_TEST_GRAD_2.pdf\|here\]\]. Oddly, the peak maxima is almost 3 x as intense, otherwise the profile remains somewhat constant while accounting for the increased time, baseline is lower and peaks are overall more resolved.

## 2023-01-25

Today I continued the wine separation method development begun yesturday.

After a 14min gradient not achieving adequate separation, I have doubled the gradient to 32mins, or 2.96%/min. Method can be found in `~\2023-01-25_WINE_GRADIENT_3.M` and respective the data file. The results were better, but still not achieving complete separation. A report has not been obtained, the data was possibly deleted.

The next run can be found in `~\2023-01-25_WINE_GRADIENT_4.M`, with an increase in gradient time to 38mins, or 2.5%/min. The 254nm wavelength band appeared quite sparse, but separation was good, average baseline height is approaching zero.

Following this run, `~\2023-01-25_WINE_GRADIENT_5.M` was performed, and the report can be found \[\[20230123_WINE_TEST_GRAD_5.pdf\|here\]\], with the addition of wavelengths ranging from 200 to 320 at 20nm increments, 4nm band widths. Results good, majority of the separation has maximal absorbance at 240nm, however 220nm was best for the first 20 minutes.

Thats it for today, next thing to do would be to look for a more precise maximal absorbance around 220 and around 240nm, also look into focusing on segments of the chromatogram.

## 2023-01-30

Today I am going to continue refining my wine gradient and run different samples of wine. Today's sample is a Nocturne Cabernets 2021, a producer description can be found [here](https://www.nocturnewines.com.au/wines) under "2021 Nocturne Cabernet". It is a Margaret River blend of Cabernet, Merlot, and Malbec. It was sampled 2023-01-30 at 22:30pm and frozen immediately.

Another run of Z3 was conducted, filename `20230123_WINE_TEST_GRAD_6.D` (renamed from `..GRAD_7.D`)

### PCD

benchtop - wine with CUPRAC reagent resulted in a cloudy solution, indicating that the wine samples at hand, (Z3 and NC-4) are unsuitable for PCD at the given concentration range.

As Andrew needs consultation from this point, I will instead run the remaining wine samples.

### Other Wine Samples

I will run z1, z2, 1, 2 in a 4h sequence. Sequence will be called: `.\20230130_wines.S`

## 2023-02-01

After running my first run of the day, `..\2023-02-01_z_second_run_of_the_day.D`

TODO: - \[x\] change `..\2023-02-01_z_second_run_of_the_day.D` to `..\2023-02-01_z3_second_run_of_the_day.D`

today i have had troubles using open chrom to batch process my signals as you are unable to point the batch processor at a directory amd process - doing this only converts the first file in the dir. so i sterted exploring other options. one found was chromConverter, an R package for utilising a number of parsing softwares in one interface. unfortunately the unfortunately the software has very little documentation, however, I am getting pretty close but it seems as though though openchrom problem, so to use it i will do a clean reinstall of openchrom.

my laboratory activities today I have continued to run the Z3 Sample over four hours or so. Data analysis of these signals could reveal the effects of use, such as thermal effects.

to do: - \[x\] Identify a wavelength Maxima by comparing the samples over the wavelengths observed. ✅ 2023-02-05 - \[x\] grab sample vials, label them same as injection vials. - \[x\] Next Monday collect samples by going into work at 8 am store in freezer. ✅ 2023-02-08

Possibilitirs for fixing the chromconveter provlen include: - identifying the expected installation path of openchrom

Project is here: \[\[project_chromatogram_converter\]\].

## 2023-02-02

Continued working on this: \[\[project_chromatogram_converter\]\].

### meeting with andrew

#### talking points

-   [x] Signals. ✅ 2023-02-02
-   [x] Kings? ✅ 2023-02-02
-   [x] PCD ✅ 2023-02-02
-   [x] Sample protocol. ✅ 2023-02-02
-   Danil
-   Caffeine
-   wine PCD
-   referee reviewer 2 response.

change column to avantor column.

shorter column to reduce run time.

dont reduce runtime, unecessary

wine hplc-uv end of feb.

cuprac after.

decide which pcd reagent to use, i.e. dpph, phenolics.

After my meeting with andrew I have come out with the following:

Aim - build a wine chromatogram library, primarily PCD-based but with hplc-uv/vis as well. Following construction of the library, various methods will be attempted on the data to produce statistically significant models for differentiation categorization and differentiation. Potentially novel research can come from the method development itself.

Method: only using the agilent system for hplc-uv/vis and PCD.

Timeline:

End of feb: finish building uv/vis library. End of march: finish building PCD library, End of April: finish data analysis. End of May: finish writing thesis. End of June: submit thesis.

Samples will be taken primarily from Shellhouse, potentially Dining Room and Menzies. Aim for 150 samples, but acknowledge limitations.

Samples will be takein \~30mL volumes from work. Will need to label the vials.

## 2023-02-06

Designed a file parser workflow here: \[\[project_chromatogram_converter#2023-02-06\]\]

## 2023-02-07

### HPLC-DAD project file naming conventions

Setting a new sample naming format using the chemstation naming patterns:

`<Date>-<Time>-<SampleName>-<Counter>`

Today I will be running the samples I collected yesturday. This will be done on the avantor column that Andrew gave me last week:

Avantor ACE

Catalogue No: EXL-121-1046U

Material: ACE Excel 5 C18

Batch: DV21-4114

Dimensions: 100 x 4.6 mm

I need a method naming convention. I had one before for the SFC, I just need to find it. It is in \[\[20220825_logbook\]\] and is as follows, adapted for HPLC-DAD:

`<name>_<column name><column comp><columndim>_<A>_<B>_<grad>`

where `<name>` is a signifier, ie. "wine", just to give context, `<column>` is the column name, `<A>` is the A phase, `<B>` is the B phase, `<grad>` is the % gradient. i.e. 95 / total length of gradient, such as 46mins. Thus a typical 46 minute gradient will be $95 / 46 = 2.1$.

To give a full example, a wine focused method on an avantor 100 x 4.6mm C18 with H2O:MeOH with 46min gradient will be:

wine-avantor100x4_6C18-H2O-MeOH-2_1

Where the underscore signifys a period for decimal place.

### Laboratory Activities

Today I need to break in the new column and get through my samples.

Column has been tested on a blank run of MeOH, method: `2023-02-07_BLANK.M` data: `2023-02-07-11-22-37-BLANK-4.D`. No peaks were present, suggesting that the column is clean.

Next is to run a wine sample, Z3, on the new column:

Method file: `WINE-AVANTOR100X4_6C18-H2O-MEOH-2_1.M`

Data file: `2023-02-07-12-01-43-Z3-1.D`

{{add report}}

The profile looked good, but obviously was different to the previous days recordings because of the changed column and additive in the phase. Next is to run the column without FA to be able to determine the effect of changing the column and changing the column + adding formic acid.

After changing over the solvents, I have run the Z3 sample again:

method file: `AVANTOR100X4_6C18-H2O-MEOH-2_1-NO-FA.M`

data file: `2023-02-07-14-10-47-Z3-NO-FORMIC-ACID-1.M`

In the mean time, I will continue working on the file parser \[\[project_chromatogram_converter#2023-02-07\]\].

Run done.

{{add report}}

Comparison between with and without FA shows no major advantages to introducing FA. It produces a higher baseline and more intense peaks in the first 5 mins, with no major difference in the lower end of the separation. Thus I will resolve to remain with a H2O MeOH composition without any modifiers.

Samples have all been filtered and placed into new injection vials. Equipment was washed with H2O, MeoH and acetone to dry between use.

To determine ideal wavelength to target, I ran a sample at 240 - 260 at 4nm increments AND included 'full spectrum option'. I get the feeling the full spectrum records everything between 190 and 400nm at 2nm steps, but will have to verify. produces a `.uv` file.

Method file: `AVANTOR100X4_6C18-H2O-MEOH-2_1-1.M`

Data file: `2023-02-07-18-30-07-Z3-1D_NM_ABS_MAX.D`

Results inconclusive, will need further study. Thus, I have set up a sequence to run recording the same signals and spectrum:

Sequence Template File: `2023-02-07-WINES.S`

Data Folder: `2023-02-07-WINES 2023-02-07 19-41-18`

Thats all, will pack it up in the morning and start data analyses for the next two days.

## 2023-02-08

Yesturdays sequence did not run successfully because the method was drawing from A2 reservioir, rather than A1. As A2 was not connected to anything, this meant that the instrument was drawing air for 17 hours. After disconnecting the column flushing out air bubbles with 100% H2O at 5ml/min, then observing the pressure signal, then reconnecting, we confirmed that the instrument was A OK.

Issue could have been caught with a low pressure limit in the bin pump method. Comparing run `2023-02-07-17-08-25-0051.D` with `2023-02-07-Wines 2023-02-07 199-41-18/005-0101.D`:

**Normal Pressure Trace**

\[\[2023-02-07-17-08-25-0051_pressure.png\]\]

**Pressure Trace without A Phase Solution**

\[\[2023-02-07-WINES 2023-02-07 19-41-18 009-0501.png\]\]

Observing the first plot, we can see that the rough minimum for pressure is \~800 psi. Thus a very safe shutoff minimum limit would be 400 psi, well below normal operating pressure.

At 14:43 I will run Z3 another time to observe performance after this incident. It will be compared with `2023-02-07-18-30-07-Z3_ID_NM_ABS_MAX.D`. The new file is `2023-02-08_14-48-37-Z3.D`.

## 2023-02-09

Data Analysis needs to get done today.

1.  [x] Collect all data together. ✅ 2023-02-28
2.  [x] Build data objects in Python. ✅ 2023-02-28
3.  [x] Construct a Table with characteristics of each wine. ✅ 2023-04-26
4.  [ ] Identify:
    1.  [x] wavelength of abs maxima. ✅ 2023-03-16
    2.  [ ] Characteristic regions of spectra.
    3.  [x] Is time alignment necessary - ID time variance.. peak on peak? ✅ 2023-03-28

### Column Check

After the luckluster results from yesturday<!--todo: paste Z3 reports from yesturday in 2023-02-08_logbook and link to from here -->I determined to run a espresso through the column, essentially using it as a standard. The method is `2023-02-09_COLUMN_TEST_COFFEE_SHORT.M` and had a 9.5% gradient a link to the report and the profile are below:

<!--todo: add report and profile  -->

The resulting profile looked hunky-dory! I thus hypothesise that the issue was that the Z3 sample had degraded. To confirm this, I am running the Z3 sample again with the same method. If thats fine, I'll run the full method from yesturday.

## 2023-02-14

Today I have two objectives: - \[ \] tabulate experiments done so far. provide comments. - \[x\] determine whether instrument performance has been acceptable. ✅ 2023-02-24

## 2023-02-15

### Phenolic Assay

Going to do the \[\[phenolics_assay\|phenolics assay\]\].

Method based on [this paper](zotero://select/items/@selim_TwocomponentPostcolumnDerivatisation_2014).

#### Phenolics Reagent

> The ammonium acetate (0.1 M) buffer solution (adjusted to pH 9 with ammonia) was used to prepare the post-column derivatisation reagents: 1) the 4-aminoantiprine solution (150 mg/100 mL) and 2) the potassium ferricyanide solution (150 mg/100 mL in line with the procedure outlined by Bigley and Grob \[16\].

-   0.1 M ammonium acetate at pH 9 adjusted with ammonia.
-   150mg/100mL 4-aminoantipyrine.
-   150mg/100mL potassium ferricyanide.

#### Phenolics Assay Reagent Procedure

[Bigley and Grob](zotero://select/items/@bigley_DeterminationPhenolsWater_1985)

-   0.1 M ammonium acetate buffer formed by adding 7.70g ammonium acetate to \~900mL H2O then pH adjusted to ammonium hydroxide.

### PCD setup

> The post column derivatisation process was optimised using the phenol standard compound, at various reagent flow rate ratios of the two derivatisation reagents. The phenol standard was tested at a wavelength of 500 nm, and the mobile phase was kept constant at 1 mL/min. The optimum flow rates of the PCD reagents: 4-aminoantipyrine and potassium ferricyanide were found to be 0.5 mL/min and 0.25 mL/min respectively, as it resulted in the best signal to noise (S:N) response (see supporting information Table S1).

-   2 PCD pumps required.
-   main flow rate = 1 mL/min.
-   4-aminoantipyrine = 0.5 mL/min.
-   potassium ferricyanide = 0.25 mL/min.

## 2023-02-16

Have run my 16 samples from 2 weeks ago for the first REAL time to observe sample deg. Sequence `2023-02-15_WINES_2023-02-15_15-19-53`<!--TODO: stop dating the sequence files if its gna auto suffix them -->Only 0181 had a reasonably high abs response with a very impure peak from 2.45 to 2.9min at 700 mAU. Unfortunately I do not have a record of what sample is.

Regardless, compared with the 0051 (william downie 'cathedral' pinot noir) response on 2023-02-07, baseline was higher relative to the gradient, samples had generally lost the 20 min maxima, 240nm had become the dominant wavelength in the 10 min region, and the 0 - 5min region had become much more populated and intense.

i.e. those original compounds had broken down and what was left was being retained less strongly under the current separation conditions.

Q: do we try and adapt to those conditions?

Q: Is there still phenols to react with neocuproine?

It is still surprising that the samples broke down so rapidly considering they were kept in a -22oC freezer.

Regardless, we should push on with our 'new' samples and attempt to get some decent fresh signals.

Todo: - \[x\] enter new samples into tracker. - \[x\] label vials with new sample ids - \[x\] filter samples, place into new vials. IMMEDIATELY back into freezer. - \[x\] run each sample, one by one if there is no duplicate. Anything with a duplicate, run as a sequence over night. ✅ 2023-03-05 - \[x\] Reorganise vial trays, 1 duplicate per tray, rather than duplicates together. ✅ 2023-03-21

I will run the following before leaving today, in sequences. These will be portioned out before placing in the instrument:

| Vial | ID  | Wine                            |     |     |     |     |     |
|------|-----|---------------------------------|-----|-----|-----|-----|-----|
| 1    | 22  | kuenhof 2019                    |     |     |     |     |     |
| 2    | 24  | stargazer riesling              |     |     |     |     |     |
| 3    | 25  | ulysse colin les perriers       |     |     |     |     |     |
| 4    | 26  | 2016 anna maria obbona nebbiolo |     |     |     |     |     |
| 5    | 27  | mystery, possibly protero nebb. |     |     |     |     |     |
| 6    | 30  | st hugo cabernet 2009           |     |     |     |     |     |

And we'll see how they go. Current hypothesis is leaning toward the 0051 sample being false, rather than all of the wines degrading to a very similar profile comparatively. Sequence file is: `2023-02-16_WINES_2023-02-16_13-46-32`.

2023-02-22 - Result: No wines, red, white, aged, youthful, have the expected chromatographic profile. Time to seriously question the instrumentation setup, the column, etc.

## 2023-02-22

I have grabbed 6 new samples<!--todo: add sample names -->of fresh wines ready to be run in order to verify the instrumentation and past wine profiles. plan of attack:

1.  [x] Run a coffee to verify the instrument.
2.  [x] While coffee is running, filter wine samples. Seperate 1 runs worth of sample for injection, leave rest in a vial.
3.  [x] Run each sample over 6 hours. only take the sample to be measured out just before injection, i.e. during calibration time after the previous run.

While those runs are being performed, finalise your bokeh setup.

### Wines

<!-- todo: add wines to sample table   -->

Koerner Nellucio, Stoney Rise PN, Crawford Cabernet, Hey Malbec.

### Column Check with Coffee

I have run a L'OR ristretto capsule coffee, and I've noted that the retention under current conditions is not great and doesnt match the retention on earlier runs. There is now too much uncertainty to continue without system checks. The first question is, how do I verify column performance? The secend question is, how can I verify the detector.

### Koerner Nellucio

Currently at 15 mins, and the profile is consistant with the previous samples, but again not with the samples from two weeks ago. at 254nm, the volatile region has a maximum absorbance of 150mAU.

I need to: 2. \[x\] Figure out how to parse .UV files. ✅ 2023-02-23 3. \[x\] Ensure that I am looking at the right wavelengths. ✅ 2023-02-23 4. \[x\] Verify why maximum absorbance might change (?). ✅ 2023-02-24

### 3d surface plotting

I have produced a proof-of-concept of accessing the .UV files [here](https://github.com/jonathanstathakis/wine_analysis_hplc_uv/blob/main/2023-02-22_parsing_uv_files.ipynb) using rainbow-api, and after some research, and checking old notes \[\[20220706_logbook\|here\]\] I decided against Bokeh, and instead use Plotly. Quick googling led me to discover [cufflinks](https://github.com/santosjorge/cufflinks) which makes plotly a lot easier to use with pandas in notebook environments. A preliminary plot has been produced [here](https://github.com/jonathanstathakis/wine_analysis_hplc_uv/blob/main/2023-02-22_parsing_uv_files.ipynb) and I will now need to develop the interface for clearer use, understanding and flexibility.

3 software are at play to produce the plot linked above, pandas, plotly, and cufflinks (not to mention jupyter lab). Need to understand how they are interacting.

1353 - Cufflinks surfaces are conceptually difficult to plot for my data, I will need to finely control how the software interpolates from the data. My previous attempts took advantage of 2d line plots on a 3d surface spaced by wavelength interval, which I will try to recreate. [this](https://stackoverflow.com/questions/58034958/how-to-plot-multiple-3d-lines-with-plotly-express) stack overflow post is asking the same thing AND is using chromatographic data as their example. The accepted answer says that the data needs to be in a "tidy" format, which is defined [here](https://www.jeannicholashould.com/tidy-data-in-python.html). I would say that the .UV data is already tidy, each wavelength is a variable with absorbance values and each moment of obsevation is labelled as a portion of a minute.

1439 - After looking at examples of 3d line plotting in plotly [here](https://plotly.com/python/3d-line-plots/) I decided that I needed to melt my .UV data with `RT (min)` as `id_vars` and `wavelength` and `mAU` as values. This produced the desired result of each row labelled with each wavelength value, however it massively expanded the size of the dataset from 4000 or so rows<!--double check this -->to 200,000. Plotting this result is not working in the cufflinks API or plotly.line_3d though. And its massively slowing down jupyter lab. I will continue working on this for an hour, then spend an hour cleaning up my data files on the instrument computer to standardise their formats.

1443 - To continue to develop my plotting software, I need a mock dataset that is small yet representative of full sized chromatograms.

Also, I need to keep running samples. Lets do the cheap wine next, on both columns. De Bertoli Sacred Hill 2021 Cabernet Merlot. 5 replicates have been put on ice, 1 has been run at 1449.

1610 - The De Bertoli wine on the avantor 10cm column had a profile similar to all other wines run lately. The profile can be found in the saved report. I have now run the same sample again (1h of deg) on the Halo 15cm column. Unfortunately! the data file was overwritten due to careless thumbs, so I will need to run the avantor again afterwards to be able to produce a comparison report.

1753 - During the avantor run a leak developed in the top of the connector. Due to the late time in the day i have decided to call it here. Will run again first thing in the morning, with 'fresh' sample.

2023-02-24_10-15 - It wasnt mentioned earlier, but I did "clean" up my data files on the agilent PC to be more usable. The basic format is {information-block-1}\_{information_block_2}\_.. etc. where and information-block might be the date, the time, the sample name. The idea is to be able to cleanly seperate each by targetting underscores while avoiding any use of whitespace.

## 2023-02-23

### Wines in CUPRAC

#### Bench Testing CUPRAC

Prior to injecting the wine in the Shimadzu for CUPRAC PCD using Jake's 10cm Avantor RP C18 PCD column, bench tested to make sure no precipitate would form. As long as no precipitate formed within a minute of mixing, the instrument will be safe.

wine: Debertoli sacred hill<!--todo: fill in info  -->

0.3mL reagent. 1mL mobile phase 15uL of wine.

After 5 mins, no precipitate formed for these concentrations, therefore safe to use.

Injected in Shimadzu at 12:13pm.

Note: after 1h, some precipitate has formed.

2023-02-24_09-50: 2 x Shimadzu De Bertoli runs are done.

Also, 3d .UV data plotting has been achieved, as has .UV to .csv conversion. The prototypes can be found [here](https://github.com/jonathanstathakis/wine_analysis_hplc_uv/blob/main/2023-02-22_optimising_3d_specta_plots.ipynb) and [here](https://github.com/jonathanstathakis/wine_analysis_hplc_uv/blob/main/uv_to_csv.py), respectively.

## 2023-02-24

Successful day yesturday, achieving 3d plotting and modularizing a uv to csv converter. Now I have a bunch of TODO's to get through, primarily a descriptive analysis of data collected thusfar. The best way to get through them is to think about how to assemble the data together in Python.. The most sensible approach would be to produce a class objects of Sequence and Data Directory, and wrap them around rainbow-api data directory objects as much as possible. The reason for this is because I want to be able to access as much of the meta data as possible. Once this is complete, I can proceed to perform descriptive statistics analyses on the dataset.

So.

-   [x] Barebones sample data run class will have:
    -   [x] the data
    -   [x] name of the sample
    -   [x] date-time of acquisition.
    -   [x] method object.

The method object class will consist of:

-   [ ] method name.
    -   [ ] injection volume.
    -   [ ] mobile phases.
    -   [ ] gradient profile.
    -   [ ] detector settings.

Can probably just start with the raw text available in the `.M` dir then go from there.

## 2023-02-28

Been hard at work building up a codebase, mostly in notebooks found at `/Users/jonathan/002_wine_analysis_hplc_uv/notebooks` . I've got a data tabulator and uv_data accessor at `/Users/jonathan/002_wine_analysis_hplc_uv/scripts/data_interface.py` and a 3d line plotter in `/Users/jonathan/002_wine_analysis_hplc_uv/scripts/hplc_dad_plots.py`.

A surface plot was attempted notebooks found at `/Users/jonathan/002_wine_analysis_hplc_uv/` `2023-02-27_surface_plots_3d.ipynb` and `2023-02-28_trying_to_model_3d_surfaces.ipynb`, however we found that the computer memory was being overwhelmed because the dataset is too large for direct surface generation, and will either need to downsample the dataset or set up a data stream.

### Interacting with chromatograms with SciPy

The next thing to do is figure out how to interact with the chromatograms programmatically to generate descriptive data and from there descriptive statistics. The frist point of contact will be scipy.

The corresponding notebook on scipy can be found here: `/Users/jonathan/002_wine_analysis_hplc_uv/notebooks/2023-02-28_scipy.ipynb`

19-43: The above notebook contains prototype code for a scipy-assisted peak plot and calculations of 'peak prominance'. `/Users/jonathan/002_wine_analysis_hplc_uv/notebooks/2023-02-28_playing_with_sci_peak_plots.ipynb` contains some minor explorations using this plot, whose code has been functionalised in `/Users/jonathan/wine_analysis_hplc_uv/scripts/hplc_dad_plots.py`.

It will be useful to further develop a chromatogram description interface with:

-   [ ] Functionalised Chromatogram Display
    -   [ ] number of peaks.
    -   [ ] peak prominance of each peak.
    -   [ ] baseline description.
    -   [ ] maxima value.
    -   [ ] peak symmetries.

But that's all for today. I should also start mapping out the project structure and what is where.

## 2023-03-01

Today I will clear as much of my backlog of todo's as possible, and run several reproductions of the runs from last week to get some reproducibility data. Building a quick dashboard would not go astray either.

### Wavelength of Absorbance Maxima for each sample

Now, the problem with this is how to define 'Absorbance Maxima', as the baseline is exponential inversely porportional to wavelength.

To observe if this is true, I will need to identify a "quiet" region of each samples curve, and then take the baseline measurement for each wavelength.

So, a few questions there. Firstly, how to handle baselines in Python, or indeed, at all?

And then more than that, what about chromatograms in general? It will be good to take a step back and start to organise my notes on:

-   chromatography
-   RP-HPLC
-   HPLC-DAD
-   wine analysis.
-   wine authentication.
-   PCD.

And then build notes up from there. Before doing any of that I should produce some plots I can send to Andrew, as that is first priority. Get that in the works, get his interest, then start your own research.

To send the plots to Andrew I need to get them into the "origin" format I used in the past, and send them as power points.

-   [ ] Origin styled plots for Andrew:
    -   [ ] locate the code that formatted the plots as in Origin. ✅ 2023-03-01
    -   [ ] plug that into my data.
    -   [ ] produce a report to send to Andrew.. powerpoint? Send him the powerpoints and the .csv files.

### Slideshow generator

The repo is [here](https://github.com/jonathanstathakis/plot-slideshow-generator.git) and has been currently cloned to `/Users/jonathan/plot-slideshow-generator`.

The package uses

### Obtaining the void volume of a chromatography column

To get the void volume of a column, you can inject a sample of non-retained yet detectable compound - typically small, polar compounds like urea or aceteone.

The void volume of a column will vary based on: - Column dimensions. - Packing conditions. - Temperature. - Mobile phase composition. - sample volume.

Note: Does'nt appear to be possible to add column info to a method/sample. Will address it later, but atm just include it in 'method information'.

To measure the void volume of the system with my avantor C18 100 mm column, I will inject acteone. Acetone has a abs maxima at around 270nm[^1]. Multiplication of the flow rate by the time of the acetone peak.

[^1]: [Acetone](https://webbook.nist.gov/cgi/cbook.cgi?ID=C67641&Mask=400#UV-Vis-Spec)

Have set up a 6 sample sequence with decreasing concentrations of acetone produced by lowering the injection volume from 10uL to 8, 6, 4, 2, and 1 uL. I'm not sure whether injection volume itself will vary the void volume and I will currently assume that it does not.

### Organising Chromatography Notes

What about Plate counts?

14-11 - Creating a chromatography homepage \[\[homepage_chromatography\]\]

14-43 - Homepage is setup, I still need to populate it with notes that didnt turn up in the initial query. In the mean time I need to plan a dead volume experiment using acetone, as per above, but with a concentration gradient.

### Basic Chromatography Definitions

The following are sourced from [dong_HPLCUHPLCPracticing_2019](zotero://select/items/@dong_HPLCUHPLCPracticing_2019):

-   \[\[chromatography_retention-time\]\]

-   \[\[chromatography_void-time\]\]

-   \[\[chromatography_peak-height\]\]

-   \[\[chromatography_peak-width\]\]

-   \[\[chromatography_retention-volume\]\]

-   \[\[chromatography_void-volume\]\]

-   \[\[chromatography_peak-volume\]\]

-   \[\[chromatography_retention-factor\]\]

17-42: got as far as retention factor, found on page 18 (40 in pdf terms). Starting to get a bit tired though, will continue tomorrow.

I will continue with plugging the powerpoint generator into my data generator so I can send something to Andrew to review.

## 2023-03-02

I've gotten \[\[unix_rsync\|rsync\]\] working now, there was some weird bug when trying to sync the oold `0_jono_data` dir, so i wiped it and created a new one populated by rsync. See how it goes. To get the same functionality on the instrument side, I can either install cygwin on the computer (very tempting at this point) or use robocopy.

Today I desperately need to:

-   [ ] Today's Tasks
    -   [x] send some data to Andrew to give him an update on how I'm going. ✅ 2023-03-28
    -   [x] analyse the void volume acetone data. ✅ 2023-03-16
    -   [x] and read up on how to measure column plate counts. ✅ 2023-03-16

### Sending Data to Andrew

What data? I want to send him representative data.

To do that, I should build a panel-based dashboard that will:

-   [ ] dashboard
    -   [ ] let me cycle through each dataset with the same format graphical representation.
    -   [ ] include a 3d line plot
    -   [ ] and a 2d plot with peak detection
    -   [ ] a read out of the peak detection parameters and values.
    -   [ ] It should also include some sort of baseline description.

What is my workflow?

The codebase is currently:

``` javascript
notebooks
├── 2023-02-08_eda.ipynb
├── 2023-02-11_playing_with_rainbow_api.ipynb
├── 2023-02-21_testing-plotting.ipynb
├── 2023-02-22_optimising_3d_specta_plots.ipynb
├── 2023-02-22_parsing_uv_files.ipynb
├── 2023-02-23_learning_plotly_api.ipynb
├── 2023-02-24_learning_python_classes.ipynb
├── 2023-02-24_prototyping_agilent_data_method_classes.ipynb
├── 2023-02-25_exploring-rainbow-api.ipynb
├── 2023-02-27_surface_plots_3d.ipynb
├── 2023-02-27_testing-UI-modules.ipynb
├── 2023-02-28_scipy.ipynb
├── 2023-02-28_trying_to_model_3d_surfaces.ipynb
├── __init__.py
├── bokeh_getting_working.ipynb
├── learning_python_regex.ipynb
├── prototyping_filename_cleaning.ipynb
└── untitled.py
scripts
├── 2023-02-28_playing_with_sci_peak_plots.ipynb
├── __init__.py
├── combo_dif.py
├── data_interface.py
├── data_manipulators.py
├── dir_contents_dict_builder.py
├── dir_ext_set_getter.py
├── example_getopt.py
├── hplc_dad_plots.py
├── playing_with_argpase.py
└── uv_to_csv.py
```

I need to structure the scripts dir more..

``` javascript
notebooks
├── 2023-02-08_eda.ipynb
├── 2023-02-11_playing_with_rainbow_api.ipynb
├── 2023-02-21_testing-plotting.ipynb
├── 2023-02-22_optimising_3d_specta_plots.ipynb
├── 2023-02-22_parsing_uv_files.ipynb
├── 2023-02-23_learning_plotly_api.ipynb
├── 2023-02-24_learning_python_classes.ipynb
├── 2023-02-24_prototyping_agilent_data_method_classes.ipynb
├── 2023-02-25_exploring-rainbow-api.ipynb
├── 2023-02-27_surface_plots_3d.ipynb
├── 2023-02-27_testing-UI-modules.ipynb
├── 2023-02-28_playing_with_sci_peak_plots.ipynb
├── 2023-02-28_scipy.ipynb
├── 2023-02-28_trying_to_model_3d_surfaces.ipynb
├── __init__.py
├── bokeh_getting_working.ipynb
├── learning_python_regex.ipynb
└── prototyping_filename_cleaning.ipynb
scripts
├── __init__.py
├── core_scripts
│   ├── __init__.py
│   ├── data_interface.py
│   ├── data_manipulators.py
│   └── hplc_dad_plots.py
└── old_scripts
    ├── __init__.py
    ├── combo_dif.py
    ├── dir_contents_dict_builder.py
    ├── dir_ext_set_getter.py
    ├── example_getopt.py
    ├── playing_with_argpase.py
    └── uv_to_csv.py
```

Better.

Now, the workflow from file --\> plot is:

\[\[2023-03-02_workflow_to_plot_uv_data.excalidraw\|250\]\]

Cool.

Now, it would be nice to have a dynamic df table of current data including sequences. Sequences as a class will be ignored and the individual data files will be the focus, apart from the inclusion of a sequence column. If the data file was part of a sequence, the column will state the name of the sequence, if not, it will contain 'single run' as per the nomenclature of Chemstation.

On second thought.. I really need the OOP to be able to work with the data structures..

## 2023-03-07

\[\[2023-03-02_logbook\|Last time I was in the lab\]\] I determined that a OOP approach was necessary for more complex analyses. This has been done, encapsulated within a dir titled "agilette".

After calculating the void volume, I would like to investigate plate counts for the column. To do this I will follow the method in [this webpage](https://www.pharmaguideline.com/2011/07/hplc-column-performance-evaluation-and.html) . It recommends to run 0.1% toulene and observe the peak.

[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6470734/#:~:text=The%20absorption%20spectra%20of%20toluene,of%20250%20to%20270%20nm.) states that toulene has a abs maxima between 250 and 270nm with a band of 10nm centered on 260nm.

Creating a 1mL solution at 0.1% would be..

1000uL 100uL = 10% 10uL = 1% 1uL = 0.1%

too low to measure.

10mL would be 10uL, so 990uL water, 10uL toluene.

Note: toluene is immiscible in water.

The above link directs us to use an acetonitrile : water (60 : 40) mobile phase, and make the toluene up in the same.

What is acetonitrile? \[\[acetonitrile\]\].

What is toulene? \[\[toluene\]\].

So, we can see now that toluene is not miscible in water, therefore the solution prepared earlier is not what was desired. It should instead be a 60:40 acetonitrile : water solution.

To prepare 5mL mobile phase solution..

On second thought, I will assume that for now, acetone is a sufficient standard.

Furthermore, on second second thought, I want to verify my void volume calculations by having a constant injection volume and prediluting the samples.

In \[\[2023-03-01_logbook\]\] I assumed that an injection volume decrease of 10uL, 8uL, 6uL, 4uL, 2uL and 1uL would produce a decrease in concentration of 1, 0.8, 0.6, 0.4, 0.2, 0.1. To verify that, we'd make up 6 samples following the same.

| vial | amount of acetone (mL) | amount of water (mL) |
|------|------------------------|----------------------|
| 1    | 1                      | 0                    |
| 2    | 0.8                    | 0.2                  |
| 3    | 0.6                    | 0.4                  |
| 4    | 0.4                    | 0.6                  |
| 5    | 0.2                    | 0.8                  |
| 6    | 0.1                    | 0.9                  |
|      |                        |                      |

Scrap it all, just do some wine runs. Going to run the frozen duplicates of debertoli and see how it goes.

### New New Plan

Use uracil as a void marker in all samples then calculate retention factors.

[Alejandro Martin](https://www.researchgate.net/post/How-do-you-solubilize-uracil) says that he prepares it in 2mg/mL and stores it at -20oC, also says that you can't get higher concentrations than that to dissolve.r

We will add a quanity of uracil. Preferably we'd measure 1mL wine then add a given quantity of uracil.

Uracil has an abs max at 258.5nm according to \[J. Li\](https://omlc.org/spectra/PhotochemCAD/html/093.html.

)

first run 10uL injection, very big peak at 260nm. 0.5 mg/mL. 50mg/100mL, 3300 mAU peak at 1.76mins.

Second will be a 1uL injection at the same concentration.

A run in Debertoli wine has been completed with a uracil peak (possibly) at 1.72 mins at approx 620 mAU in 260 nm.

0.5mL wine to 50uL uracil at 0.5mg/mL.

### Wines with Uracil Sequence

6 wine samples have been made up with Uracil:

0281 0233 0182 0072 0300 0092

500uL of wine sample, 50uL of Uracil aquous solution.

We'll see how these go for identifying the uracil peak.

### Double Check Uracil Dilution Calculation

Recommended concentration of uracil is 0.5mg/mL.

In 100mL, 50mg.

We've added 50uL of 0.5mg/mL uracil to 500uL of wine.

$$ c1v1 = c2v2 $$ $$c2 = c1v1/v2$$

v2 = 550uL v1 = 50uL c1 = 500 ug/uL

$$ c2 = 550 * 50 / 500$$ $$c2 = 55ug/uL$$

which is equivalent to 55mg/mL/

## 2023-03-08

Today let's answer 1 research question:

```         
For each wine studied on the Avantor 150mm column, what was the maximal wavelength outside of the methanol region? i.e. for minimal baseline, what is maximal wavelength?
```

There are multiple components to this question:

-   [x] Isolate the sequences and runs which used the avantor column. ✅ 2023-03-11
-   [x] Identify the wavelength region of 'minimal' baseline. ✅ 2023-03-16
-   [x] Identify the single wavelength with maximal absorbance within that region.

For the first, I need to identify the date upon which I swapped to the avantor column, because while I have been noting in the run description at some point, I wasn't always.

We can view this in the data table, presumably.

This investigation will be undertaken in [2023-03-08_investigating_wavelength_maxima](/Users/jonathan/wine_analysis_hplc_uv/notebooks/2023-03-08_investigating_wavelength_maxima.ipynb)<!--note: path link doesnt work for some reason..  -->

**17-37**

Fairly productive day, depending on how you measure it. Ironed out a bunch of creases in Agilette, many more to go. Started getting familar with my available data set and how to manipulate it. There is much more of that to go as well.

Tomorrow I want to establish a method of baseline analysis before work. I will also need to go in and run the sequence I set up yesturday to test uracil. I will also also need to do more reading on Uracil.

To note, in terms of further analysis, grouping data by method may be very important.

## 2023-03-09

According to the [rainbow documentation](https://rainbow-api.readthedocs.io/en/latest/examples.html), the multiprocessing package can be used to speed up the processing of .D dirs. I tested this in `../2023-03-09_testing_multiprocessing.ipynb` and found that it took 5 seconds to process all of them with this method, compared to the 30 - 40 seconds to process before.

Now, integration into my already existing data structure will have to be explored, but I am way too far behind on data processing to explore this further atm. Something to think about though.

## 2023-03-12

### MRES Project Outline

1.  data acquisition.
2.  data treatment.
3.  modelling.
4.  optimization
5.  verification.

There are two main routes of analysis: 1. Select an 'ideal' wavelength where baseline is minimized and peak height is maximised, baseline correction, peak alignment, load into machine model. 2. 3 way data analysis using MCR-ALS, PARAFAC, etc.

I should develop a pipeline for the first option first, then approach the 3 way if I have time.

## 2023-03-14

Note: Gradient calculations have been wrong, been calculating on the end time of the run, not gradient max. Actual gradient is $0 \rightarrow 40$ mins, the % gradient per min will then be $95/40 = 2.375\%$.

There was a supposed block in the flow cell which was remedied by reversing the flow until a peak was detected then waiting. Seems to have been fixed.

Following this, a linear downward drift in baseline was observed over the course of the gradient until about 27 mins where at 255 the measured absorbance was -127 mAU. After this it increased back to -25 mAU, then dipped again. Note, this was the first of the 44min runs, so 30 mins is the point of maximum methanol, at 80%.

To check this, a 52 min run at 2.5% was performed with the same sample, immediately afterwards, but the same behavior was observed, lowest point within the gradient at the same wavelength was \~108 mAU at \~22 mins, except this time the dip was slightly lower at 120 mAU.

A third run was performed then aborted when the downward shift behavior was observed, then the mobile phase water was replaced with a fresh quantity, and the baseline behavior was observed over the course of two hours. A estimated rate of reduction in baseline was said to be 5mAU per min, but then when the air conditioning was changed from 'cool' to 'auto' (heat) the downward gradient reduced to about 1mAU per min, and now its even less at about -0.036 mAU / min.

I have replaced the majority of the capillary with shorter lengths to allow teh column to rest inside the column oven AND reduce the total dead volume post column pre detector. The column now rests in the column oven which is set to 25oC.

[How Does Column Temperature Affect HPLC Resolution?](https://www.chromtech.com/how-does-column-temperature-affect-hplc-resolution) says that there may be issues with ambient temp mobile phase entering hotter columns, so we will keep it below 25oC atm.

Now it is time to prep the samples and set up a sequence.

Sample prep: 1. Clean instruments. 2. Prepare injection vials. 3. filter samples. 4. assemble in autosampler. 5. program sequence. 6. Begin sequence.

## 2023-03-16

Yesturday's wine dup run was only partially successful because I'd messed up the locations and entered 11 -\> 24 instead of 21 -\> 34.

Therefore, In that run:

| Vial Number | Entered Sample | Actual Sample |
|-------------|----------------|---------------|
| 11          | 32             | 56            |
| 12          | 33             | 57            |
| 13          | 34             | Null          |
| 14          | 35             | Null          |
| 15          | 36             | Null          |
| 16          | 37             | Null          |
| 17          | 38             | Null          |
| 18          | 39             | Null          |
| 19          | 40             | Null          |
| 20          | 41             | Null          |
| 21          | 42             | 32            |
| 22          | 43             | 33            |
| 23          | 44             | 34            |
| 24          | 45             | 35            |

Can I rename samples in Agilent Chemstation?

Yes. To do this you need to edit the sequence table then reprocess. You cannot delete lines but you can change sample names. Advisable to do it in offline mode as the reprocess will run post-sequence macos.

Today I will run:

5 samples at 44 min 2.5% gradient.

5 runs of no injection.

### Messed Up Sample Labeling.

The more I look at the samples from monday, the more doubtful I am that they are labelled correctly.

50 claims to be a sweet riesling, but it is a red wine.

56 claims to be a rose, but it is a fruity white wine.

I will have to sample again next week and compare the chromatograms. If there is a variance, drop this weeks one.

Note: Does not appear that I can program an empty run as a sequence. Will have to do it manually.

### Iterating over A DataFrame for Operations

`pd.DataFrame.itterrows()`. Returns a two-item tuple of (index number, Series) where the Series is the row contents.

Note: Somehow! I ended up running the '44min' sequence with the 52 min method.. fucks sake.

Putting on a 15 sample run to start after the current one. Havent labelled the samples, will need to post run. A random mix from the sample selection just to get some time tracking.

## 2023-03-21

new samples to sample.

On bumping in, I managed to uncouple the sintered filter and the inlet capillary. Replacing it proved difficult when I realised that simply putting filter back on introduced a lot of air, which went into the capillary. My attempts to remove the air bubbles rapidly led to me uncoupling the capillary from the binary pump inlet, which was very difficult to get back on after syringing in methanol to remove bubbles from the capillary. The threads are not 100% matching, but it does not appear to be leaking. I will connect to the detector shortly and look for air bubbles.

## 2023-03-30

Been very busy reworking agilette into a usable state. The codebase is more modular, more straightforward, and generally less spaghetti.

This week:

1.  Cleaned up Sample Tracker Table.
2.  Entered Samples into Cellar Tracker.
3.  Joined Sample Tracker Table with Cellar Tracker Metadata Table.
4.  Planned a sample degredation paper.
5.  Reworked Agilette into a cleaner, modular, more intuitive program.
6.  Began summerizing sampling progress thus far.
7.  Began thinking about how to surmize quality of a chromatogram to identify good signals from bad.

Is there a way of joining jupyter notebooks together into a chronological order, or building a table of contents?

In the mean time, I need to progress to the next stage of the project.

### Investigating Progress of Data Collection Thus Far

Make a statement on the progress of the data collection thus far.

How?

For the subset of all runs, identify: 1. those appropriate for analysis. 2. of those, observe ratio of peak prominance to area under the baseline for 255nm.

For the first point, runs which: 1. were on the Avantor 10cm column. 2. contain UV spectra. 3. Not uracil, acetone or coffee runs.

That is encapsulated by the following masking:

``` python
df = meta_df[(meta_df['uv_filenames']!='') & (meta_df['acq_method'].str.contains('AVANTOR')) & ~(meta_df['name'].str.contains('uracil')) & ~(meta_df['name'].str.contains('coffee')) & ~(meta_df['name'].str.contains('lor'))]
```

Lets start this investigation under the following moniker: "sample-data-quality-investigation".

## 2023-03-31

23-30

While setting up for the investigation planned yesturday, I initialised the Library object. On the dataset from yesturday and previously, everything was fine, and I had about 32 unique runs. I think that dataset extended from 02/07 to 03/07 or something close to that. However, after adding the files from since then to last week, I've run into a terminal error "cannot access local variable 'signal_wavelength' where it is not associated with a value" which then causes 'run' in 'runs_list' to be a PosixPath rather than a Run_Dir object.

Evidently there is an issue with reading the signal wavelength during Run_Dir initialization. However I am too tired to look into it now, but will do so first thing tomorrow morning AFTER dropping off Antonios things.

------------------------------------------------------------------------

10-25

Beginning diagnosis.

1.  Error begins in `get_metadata_list` in `Library` during initialization.
    -   `get_metadata_list` accepts a list and calls `.metadata_to_list` on all elements in that list.
    -   Currently encountering an error on '2023-03-07_2021-DEBORTOLI-CABERNET-MERLOT_AVANTOR.D' which is present as a PosixPath rather than Run_Dir. So the problem is the contents of runs_list.
2.  `runs_list` is initialized by `load_runs` which is passed an argument of `self.path` which can be a `str`, `Path`, or `list`. `load_runs`.
3.  `path` is passed to `run_input_validation` which returns a 'validated' list of filepaths as `PosixPaths`.

The bug must be in `run_dir_obj_loader` which is somehow passing back PosixPath objects rather than Run_Dir?

22-50

In `Library.load_runs` I have added an Try Except block around the `loaded_runs` list initializer to at least catch any problems there, but I will need to add one within `Run_Dir.init` as well in order to get to the bottom of the problem.

I have commited this change. Before proceeding to testing `Run_Dir.init` I want to try out the `inspect` module. ChatGPT says this is a good way of getting the function name of the current function of runtime. This will be integrated into any error messages. This is the example code:

``` python
import inspect

def my_function():
    current_function_name = inspect.currentframe().f_code.co_name
    print("The name of the current function is:", current_function_name)
```

And that works great. So, 1. `import inspect` 2. `inspect.currentframe().f_code.co_name`. added to `Library.run_dir_obj_loader` and commited.

Now to investigate the `signal_wavelength` error. `signal_wavelength` is only present in "acaml_read".py, within the function `acaml_read.get_single_signal_info`. It is intended to contain the wavelength as a string, which is parsed from "acq.macaml" within the ".D" directory, and it does this from within `acaml_read.signal_metadata` function. This function itself is called by `Run_Dir.get_signal_metadata`.

In order: 1. `run_dir.Run_Dir.get_signal_metadata`. 2. `acaml_read.signal_metadata`. 3. `acaml_read.get_single_signal_info`. 4. `signal_wavelength`.

So we should start with a simple test on `signal_wavelength` to define its behavior better. What was the actual error again?

"Error occured in run_dir_obj_loader in Library: cannot access local variable 'signal_wavelength' where it is not associated with a value"

That means that `signal_wavelength` is not being initialised. Currently it is initialised within an if statement:

``` python
if "Signals_Signal_Wavelength" in parameter.find('ID):
   signal_wavelength = f"{parameter.Value.text} {parameter.Unit.text}" 
```

With no else statement. Seems like an easy fix. Add an else statement `else: signal_wavelength = None` and same for bandwidth.

00-32

That did it. Able to generate the files, and files where there was no single recorded, that column is left empty. I'll commit that change now then investigate which files have no signals.

3 runs were found to contain no signals. In order to save time, I opted to simply delete them from the folder rather than investigating further.

00-57

Now back to the original question:

For the subset of all runs, identify: 1. those appropriate for analysis. 2. of those, observe ratio of peak prominance to area under the baseline for 255nm.

This will be undertaken in [2023-03-30_testing-loading-full-library-spectrum](../wine_analysis_hplc_uv/notebooks/2023-03-30_testing-loading-full-library-spectrum.ipynb), please refer to it for the investigation process. Notes on the process are continued in:

[2023-04-02_logbook](file:///Users/jonathan/001_obsidian_vault/2023-04-02_logbook.md)

[test](file:///Users/jonathan/001_obsidian_vault/2023-04-02_logbook.md)

## 2023-04-02

Continuing from [2023-03-31_logbook](file:///Users/jonathan/001_obsidian_vault/2023-04-02_logbook.md) working on [2023-03-30_testing-loading-full-library-spectrum.ipynb](../wine_analysis_hplc_uv/notebooks/2023-03-30_testing-loading-full-library-spectrum.ipynb).

I have been concerned about reproducibility. Early on in the investigation I found that 3 .D did not contain signals (most likely due to run abortion) and were causing an error when loading the Library. I fixed the error, but as I didn't see the point of retaining the files, I deleted them. However that rendered the section of the notebook addressing this problem unusable. This raised the concern - how do I retain reproducibility while editing my metadata table to join with the others? The apparently obvious step was to disconnect the metadata table from the files themselves and keep it as a seperate file, linked to the files through filepath fields. This only introduced more questions regarding how to introduce new files etc etc.

After some thought, I remembered that pipelines existed. Specifically `pd.DataFrame.pipe`.

Going forward my workflow will be pipeline based, leaving the raw code untouched, but requiring that I maintain the pipelines themselves CAREFULLY. It's probably better this way, certainly for reproducibility, but also for rigorous understanding and critique of my process.

I have tested the syntax such that I feel comfortable applying it. But it will change the way I approach problems. All operations will have to be contained within functions which take a target dataframe as input and output a dataframe. The first problem will be rectifying the 'name', or 'id' column in the metadata table to match the 'id' column in sample tracker table. Prior to the join.

## 2023-04-04

18 samples to run.

18 \* 52 = 936 mins, 15h.

-   [x] 1. Prepare instrument
-   [ ] 2. Prepare samples.
    -   [x] Enter samples into tracker. ✅ 2023-04-10
    -   [x] filter and revial samples. ✅ 2023-04-10
        -   [x] prepare glassware.
-   [x] 3. Prepare sequence.
-   [x] 4. Start sequence.
-   [x] 5. add wines to cellar tracker

## 2023-04-05

Yesterdays sequence of 17 samples ran successfully. "2023-04-04_WINES_2023_04_04_12-01-53".

I am clearing out old empty sequences including those from 2023-03-14 and 2023-02-07, placing old experiments into subfolders, namely the caffeine quant and void-time studies.

Now I will spend half an hour fixing up my 'cleaner' module so as to be able to move on to the next stage..

\[\[2023-04-05_logbook_depicting-progress-thus-far\]\]

Note: to fix horrendous highlighting of dir and file names in zsh iterm on ls, use `chmod -R 755 0_jono_data`. The reason this is happening is because at some point during the transfer process the permission of each dir and file in that directory is being set to 777, which my terminal/shell do not like. changing them to 755 makes everything happy again.

Ran into an error when looking at number of sequences vs single runs for a given condition - I'd set the agilette code up to flag a file as a sequence file if its parent directory had '.sequence' in the name. However, this was a suffix added by me, and was lost when I rsync'd the data dir today.

Best way to flag it is to read each dir's contents and if it contains a .S, note as sequence. Do it with a glob? - Correct, this was the best way.

Here I will describe the data thus far. This will be done in 2hrs.

1.  [x] Load the data.
2.  [x] Describe the data.
    a.  [x] how many single runs.
    b.  [x] how many sequences.
        i.  [x] what's in each sequence.
    c.  [x] How many samples in total on avantor column with uv spectra.
    d.  [x] Describe groups by variety.
3.  [ ] Of the subset of project-specific samples, rate the chromatogram 'quality'.

See [2023-04-05_description-of-dataset-thus-far.ipynb](/Users/jonathan/wine_analysis_hplc_uv/notebooks/2023-04-05_description-of-dataset-thus-far.ipynb) for the report.

So far we have:

Number of Avantor runs = 197, running from 2023-02-07 to 2023-04-05 Number of Halo runs = 3, running from 2023-02-22 to 2023-03-1.

That can't be right. Regardless, disregard halo in general from this point.

### Progress at 2023-04-06-12:29

Thus far I have identified that there are 78 unique samples within my dataset that are potentially useful. There was also a 44 min run: - '2023-03-16_red-wines-44min_2023-03-16_12-08-23' which was designed to test the compatibility between 52 min and 44 min runs, and 2 repeat runs: - '2023-03-15_wine_dups_2023-03-15_22-17-47' - '2023-03-16_random_wines_repeat_44min_run_2023-03-16_17-30-55'

Now that I have a better idea of the timeline, I need to start looking at what my samples actually are. Three questions:

1.  Group samples by variety.
2.  Number of sample repeats.
3.  Of sample repeats, how many repeats, over what lengths of time?

To do this, I will need to join the metadata table with tracker table, but I will also need to join the tracker table with a cellartracker metadata table. Lots of work.

202304101144

1 and 2 are done.

To progress with 3. I need to build a generalised function to apply transformation operations to dataframes and output new ones.

In pandas it is the same to output many new dataframes as it is to add many dataframes as a Series to a overrarching dataframe.

## 2023-04-06

Continuing the exploration in \[\[2023-04-05_logbook_depicting-progress-thus-far\]\].

I am at a stage in \[\[2023-04-05_logbook_depicting-progress-thus-far\]\] where I need to start integrating the cellartracker table and sample tracker tables.

I need to have these as functions that I can call on a dataframe, workflow being:

1.  Get metadata table subset.

2.  join with sample_tracker table on id.

3.  join with cellartracker table on sample_tracker-sourced vintage, name.

4.  output a joined df with agilent-metadata-table + sample_tracker-table + cellartracker-table

5.  and 3. can happen in a unexposed function, all I need is df input, joined_df output.

This function can be found [here](/Users/jonathan/wine_analysis_hplc_uv/agilette/modules/sample_tracker_cellartracker_join.py)

## 2023-04-10

Following from [2023-04-06_logbook](2023-04-06_logbook.md), [2023-04-05_logbook_depicting-progress-thus-far.md](2023-04-05_logbook_depicting-progress-thus-far.md) [metadata_sampletracker_cellartracker_join.py](../../wine_analysis_hplc_uv/agilette/modules/metadata_sampletracker_cellartracker_join.py) is now done and ready for import.

We now want to characterise the spectrum of each run. We will do this by calculating the ratio of the area under the baseline and peak prominance.

1.  Fit baseline.
2.  Calculate area under baseline.
3.  calculate peak prominance.
4.  calculate ratio of the two.
5.  plot a scatter plot.

For this, interesting statistics are: - distribution of area under baseline. - distribution of peak prominance. - distribution of ratio.

Previous files that may be of use:

[2023-03-07_prototyping_extracting_detector_wavelength](../../wine_analysis_hplc_uv/notebooks/2023-03-07_prototyping_extracting_detector_wavelength.py) [2023-03-08_investigating_wavelength_maxima](../../wine_analysis_hplc_uv/notebooks/2023-03-08_investigating_wavelength_maxima.ipynb) [2023-03-10_baselines](../../wine_analysis_hplc_uv/notebooks/2023-03-10_baselines.ipynb) [2023-03-14_identifying_optimal_wavelength](../../wine_analysis_hplc_uv/notebooks/2023-03-14_identifying_optimal_wavelength.ipynb) [2023-03-15_investigating-optimal-wavelength-for-all-runs](../../wine_analysis_hplc_uv/notebooks/2023-03-15_investigating-optimal-wavelength-for-all-runs.ipynb) [2023-03-22_testing_baseline_subtraction](../../wine_analysis_hplc_uv/notebooks/2023-03-22_testing_baseline_subtraction.py) [2023-03-23_starting_optimal_wavelength_analysisv2](../../wine_analysis_hplc_uv/notebooks/2023-03-23_starting_optimal_wavelength_analysisv2.py) [2023-03-28_tabulating-past-experiments](../../wine_analysis_hplc_uv/notebooks/2023-03-28_tabulating-past-experiments.ipynb)

The workflow proceeds as follows:

1.  Generate a Library object

2023-04-10-14-22

As I expected, I had to make a lot of minor edits to how the program handles the spectrum loading. At the moment the only function I know works is `Spectrum.extract_spectrum()`, but eventually I would like to integrate all of the functions developed during this task into the Spectrum object, and have `extract_spectrum()` be somewhat hidden from the users view. Essentially I want to avoid a 'load all spectrums and store in memory' step, and rather be able to chain operations such as baseline computations. Hard to say how practical this is atm though.

Currently I am focusing on 1 wavelength as I do not know how to calculate 3d baselines, I'm sure its not difficult to extend these methods to 3d though.

2023-04-10-19-21

Half way, I have made it as far as finding the peaks for a set prominance. Importantly, I've established a dataframe `.apply` workflow AND built a streamlit app that integrates a slider for a function variable, in this case setting peak_prominence as a variable in find_peaks.

That's all for today, continue with more of the same tomorrow.

## 2023-04-11

### Wine Degredation Study

3 wines.

1 set of replicates in ambient. 1 set of replicates in normal freezer. 1 set of replicates in -80oC freezer.

Course of experiment: 2 weeks, or until no more change observed.

ambient: 1 run every hour for first day, evaluate rate of change, then program for further observations. 2 replicate backups also in ambient conditions.

normal freezer: make 14 replicates of each wine. first measure 1h after placing in freezer, 1 every day for 14 days.

-80oC freezer: make 7 replicates. First measure 2h after placing in freezer, 1 every 2 days for 14 days.

Don't automate except in short bursts.

Total replicates per wine: 1 ambient. 7 normal freezer 7 -80oC freezer

25 replicates per wine.

### Sample Prep and Handling

1.  Purchase 3 bottles of wine.
2.  In lab prepare glassware:
    1.  3 beakers, water wash, methanol wash, dry.
3.  Label sample vials.
4.  Pour wines, filter wines into sample vials.
5.  Divide samples up into location by bags.

Sample vial labelling: 'a' : 'ambient'. 'n' : 'normal freezer' 'e' : 'extreme freezer'

Note: please find better names haha.

Please refer to this google sheet for the sample table.

[wine_deg_sample_tracker](https://docs.google.com/spreadsheets/d/15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY/edit#gid=545759601)

Table was generated with: [2023-04-11_wine_deg_sample_tracker_creation](../../wine_analysis_hplc_uv/wine_deg_study/2023-04-11_wine_deg_sample_tracker_creation.ipynb)

1.  Day 1:
    1.  Setup system. 09:00
    2.  wine 1:
        1.  Sample Prep.
        2.  Place samples in their locations.
        3.  Run ambient first. 10:00
    3.  wine 2:
        1.  Sample Prep.
        2.  Place samples in their locations.
        3.  Run ambient first. 11:00
    4.  wine 3:
        1.  Sample Prep.
        2.  Place samples in their locations.
        3.  Run ambient first. 12:00
    5.  Continual runs
        1.  1, 2, 3. 3h per set starting from 1pm. 20 h until 9 = 6 runs per wine. Day 2:
    6.  n_freezer:
        1.  wine 1 09:00
        2.  wine 2 10:00
        3.  wine 3 11:00
    7.  e_freezer:
        1.  wine 1 12:00
        2.  wine 2 13:00
        3.  wine 3 14:00
    8.  Ambient runs

For the first 24hrs run each {wine block} every 3 hrs.

was more complicated than I expected to plan this out, however I now have a good idea of how to do it.

1.  After the first day of continuous runs, I will set up the Scheduler to run the ambient wines at 6 - 9am every day.
2.  Every second day I will come in and run the freezer samples. - Mayhap jake can help with that.

## 2023-04-12

### Experiment Start up Procedure

1.  run a0101
    1.  prep n01 and e01 samples.
    2.  store n01 and e01 samples.
2.  run a0202
    1.  prep n02 and e01 samples.
    2.  store n02 and e01 samples.
3.  run a0303
    1.  prep n02 and e01 samples.
    2.  store n02 and e01 samples.
4.  Program 'a' Sequence to run until 4am tomorrow.
5.  Program 'a' scheduler to run every day at 4am to 9am.
6.  program freezer schedule.

### Programming Ambient Runs to Automatically Run Daily

There was an expectation that there would be a simple method of scheduling a run, or sequence. There was not. To accomplish this we were required to write a macro script.

The script is called 'DailyAmbientSequence.mac' and it is stored in "\~\chem32\CORE". It is loaded into Chemstation by"users.mac" in that directory. The macro itself prepares the instrument to run, loads the sequence "2023-04-12_WINE-DEG_AMBIENT.S" and then runs it.

The Command Scheduler will be used to schedule this macro to run daily at 4am.

To test this procedure we need to:

1.  See what happens when running from a freshly booted chemstation.

    1.  set the scheduler to run 5 mins from now.
    2.  close down chemstation.
    3.  open chemstation.
    4.  see what happens.

    Result: Having a "user.mac" file in the CORE directory causes error "Autostart Macro Failed, error# 41064 occured" which results in a GUI without menus or a system diagram. Thus atm we need to load DailyAmbientSequence manually every time Chemstation online is opened.

2.  see what happens when schedule to run, run a diff run, schedule run again.

    1.  schedule 2 runs, one 2 mins from now, one 10 mins from now.
    2.  After first run completes, manually run a diff run.
    3.  See if the second scheduled run starts.

    Result: Good to go. Appears to work.

While testing the automation of the sequence i ran into an error "G1329B:DEAAC15668 - motor overtemp" which according to [link](https://community.agilent.com/technical/lc/f/forum/337/autosampler-arm-motor-over-temp) was related to my use of vial locations 98 - 100. Changing to 88-90b fixed the issue.

Now, lets set the 4am daily schedule up then call it a day.

## 2023-04-13

### Results from overnight runs

All ran successfully, except that the sequence programmed to run at 21:54 did not run because the previous sequence was still going. Error read: "Not allowed" presumably because the previous sequence was still going.

The instrument actually ran out of water at 10:15am, so my mobile phase consumption calculations were off.

TODO: - \[x\] find a macro to moderate flow rate, set a 0.1ml/min flow rate when not running a sequence, have every experiment bracket prefixed with a flow rate ramp up in order to consume less mobile phase. ✅ 2023-04-21 - \[x\] Find bigger containers for H2O mobile phase. ✅ 2023-04-26

To verify total sequence time elapsed I will look at when sequence started and finished in the logbook and compare against method run time. For the sequences last night, the run time was 52 min per injection, 156mins is the expected sequence time.

seq1 start = 19:34 seq1 end = 22:15

[calculating variance between expected sequence time elapse and actual](2023-04-13_calculating-sequence-time-elapsed-average.ipynb)

From the above, we can say that each sequence ran 5 minutes longer than expected, or an average of \~2mins per injection in sequence. Thus a good rule of thumb would be to add 2 minutes to the end of the sequence for each injection in the sequence.

### Today

Get the ambient runs going.

136 mins per sequence + 5 mins = 141 mins.

12-14 - started sequence on samples 96-99 on a 44min run at 2.5% gradient "\~\\2023-04-13_WINES_2023-04-13_11-59-01".

### Programming a start up macro

Need to write a macro to get the instrument up to run conditions as per laboratory standard:

1.  Turn on at 0.1 ml/min flow rate.
2.  ramp up to 1.0 ml/min flow rate over x minutes, say 5 mins.
3.  hold at 1.0ml/min for 30 mins.
4.  Start run.

Now, with the requirement of 30 mins to equilbriate the column temperature, how long will the compartment take to warm up to running temp?

The [manual on the column compartment](https://www.agilent.com/cs/library/usermanuals/Public/G1316-90011_TCC-A-B-C_ebook.pdf) says that there is no measure to guarentee that the column has reached thermal equilibrium, however pressure fluctuations can be an indication that it has NOT. It also says that it takes about 5 mins to warm up. Therefore it should be at 30oC by the time the flow rate is up to 1mL/min. Thus instead of holding for 10mins as usual, hold for 30 before running anything.

So I need:

1.  A start up method/macro that ramps the flow rate up.
2.  A wait method/macro.

To view the list of available macros: Type "SHOW" in the command line.

To communicate with the pump:

`SendModule$ (ModuleID$, Command$)`

Pump = `PMP`

`DownloadLPmp [Module number]`

Current hypothesis is that it is `LeoPumpMethod<n>` where `<n>` is the module number, presumably 1 for this setup. Within this is FLOW and FRAMP/

### Chromatogram Quality Analysis Cont.

Time to get back into building that [module](../../wine_analysis_hplc_uv/prototype_code/chrom_quality_analyser.py). Today I want to get the same result as it currently is up to but for the qualified runs.

## 2023-04-14

Today i need to: - \[x\] Drop the sample kit off to Davy. ✅ 2023-04-21 - \[x\] Rewrite library.py and join_metadata_spectrum_tables.py such that the modifications and subsetting made affect libray.metadata_table and library.spectrum_table directly. ✅ 2023-04-21

An overall todo for the week is: - \[ \] data quality analysis - \[ \] [chrom_quality_analyser](../../wine_analysis_hplc_uv/prototype_code/chrom_quality_analyser.py) - \[ \] make statements about data quality. - \[ \] Integrate functions from chrom_quality_analyser into Agilette. - \[x\] drop sample kit off to davy. ✅ 2023-04-21

Then further on: - \[ \] replace time axis with retention factor on all chromatograms. - \[ \] write binning algorithm as a pseudo peak alignment. - \[ \] read peak alignment resources assembled in bookmarks. - \[ \] Perform PCA on data. - \[ \] Make statements on PCA clustering. - \[ \] Swap over to PCD. Assemble library.

### Rewriting Library Tables

In [metadata_sampletracker_cellartracker_join](../../wine_analysis_hplc_uv/agilette/modules/metadata_sampletracker_cellartracker_join.py) I edit, subset and mask the `Library.metadata_table`, resulting in `super_table`. This has the unfortunate side effect of detaching those changes from Agilette, meaning that I cannot use the `Library.load_spectrum()` method. So instead of 'detaching', edits need to be made on `Library.metadata_table` and `Library.spectrum_table` directly. Presumably, Transformation functions need to be done on both independently, but subsetting can be applied first to metadata_table then mask spectrum table with metadata table. This is an important distinction.

#### Recoding Log

First off, I need to move the to and from csv actions to Library. While doing this the question arising of whether a different file format would be better. After a quick bit of reading, ".feather" was said to be the fastest for read/write. To swap to .csv, I first wanted to test the performance difference, because this change would require some rewriting.

For the first run of Library, i.e. getting all the data into memory, then writing to csv, takes 3.47 seconds for the full dataset. On the second run, where it reads from the csv, it takes 2.087 seconds. Thus the difference is 1.4 seconds to read all the data. What about with feather? Initially I need to reset the index because feather cannot serialise non-standard indexes (odd because I didnt think I set an index in Library)

So swapping to feather INCREASED the write time to 7.438 seconds and the read time to 1.79 seconds. GG. Back to csv.

Anyway, lets move the csv functionality to Library.

13:28 - Why am I moving the csv functionality? Its a stop-gap measure to speed this along, but if introduced in general use will generate n .csv's for n possiblities of Library initialization.

What's the workflow.

1.  Initialize Library.
2.  Library initializes Run_Dir objects in a list.
3.  Run_Dir initializes with spectrum and metadata objects.
4.  Library forms seperate dfs containing Run_Dir metadata and Run_Dir spectrum objects seperately.
5.  Operations are performed on library metadata table to clean it up, subset for desired runs and join with externally sourced metadata.

The problem here is that the resulting super_table is seperate from the library metadata and spectrum tables.

Within runtime, there exists the list of Run_Dir objects. The tables I am operating on are running on copies of their values, rather than the Run_Dir objects themselves, and they fall out of sync.

There are several options to solve this problem. 1. make the changes and subsetting directly on the metadata_table class object then do a left join with the spectrum object, on some primary key - when cleaning the ID's, keep an 'old id' column and join on that. 2. Integrate the cleaning functions into library initialization - drawback is that on initalization the Run_Dirs are class objects not cells in a df so the transformation approach would have to be changed.

.. others.

The first approach appears to require the least work and has the added benefit of creating a mapping between the old and new ids.

So, when is old id replaced with a new one?

1.  `df['id'] = df['id'].replace({'z3':'00'})`
2.  `df['id'] = df['id'].apply(lambda x : x[1:3] if len(x)==4 else x)`
3.  `df['id'] = df['id'].replace(replace_dict, regex = True)`

The cleanest way to do this will be to create an `exp_id` and `new_id` column then do all of the above operations on `new_id`.

The reason why I wanted to integrate a csv loader into Library was because if I want to operate directly on Library.metadata_table, I will have to load the library every single run, and cannot initialize from csv - won't have the class objects in memory. However, it is 3.31 seconds to load all the metadata and perform the operations required to reach super_table. That's an acceptable length of time atm.

I think we'll just have to integrate a spectrum -\> csv step on first run. but cross that bridge when we get to it.

First i'll remove the csv element.

Potentially the sample_tracker and cellar_tracker tables can be left as csvs because they wont be altered as much.

15:15 - The code currently creates a number of seperate dataframe objects. This isnt great code, but it was done for debugging purposes. I should change those such that they all assign to the same variable name.

## 2023-04-21

It took a lot of work to fix what turned out to be a minor problem. Seems to be a common trend with pandas. The problem was two-fold. The first one was that strings suck. For reasons I cannot recall in this moment, It was necessary to apply `lower()` to the metadata_table wholly while doing the cleaning and joining operations. This caused the `libray.join_write_spectrum_table_to_db()` join to fail because the primary key was a combination of the acquisition date and filepath, but the filepath was lowerd from all CAPS to all lower. Wouldn't be a problem in modern filenaming, but back in the early noughts, CAPITAL filenaming was still in fashion.

The second problem was much easier to suss out and solve. The particular filters applied to produce runs I identified as being suitable for further analysis (the main requirement being that they included a .UV file, not because that particularly meant something, but because the inclusion of that datatype was a milestone in my experimental procedure, yet somehow did not include it) didn't incldude the necessary UV spectrum files (some of them), causing an Exception. This was fixed by including an if statement in the list comprehension checking whether the 'uv_filename' column contained 'empty' or a filename. This behaviour is more interesting than was initally obvious, because the overall action of loading the spectrum data took a cumulative 23 seconds to load. It was a lucky fault, because it drew my attention to it.

Things to do to reduce the time to load UV spectra: 2. \[x\] write a multiprocess.map function to load the data in parallel.

10-26: - I'm keen to try multiprocess first, maybe get used to running it, shouldnt be difficult..

12-51: Done. Only took 2.5hrs. Implementing a basic multiprocessing solution took the spectrum extraction time down from 30 seconds to 12 seconds. However, this is not good enough. It is now time to persue a database solution.

## 2023-04-18

To do today: - \[x\] branch the git repo while developing the database approach. - \[x\] add sample_tracker table. - \[x\] add cellar_tracker table. - \[x\] adapt super_table_pipe for use in database. - \[x\] apply chemstation metadata cleaning pipe to metadata table.

Let's rewrite them in polars in order to get a good feel for how to use the library.

### DuckDB

https://duckdb.org/docs/api/python/overview

#### Relations

`db.sql()` returns Relation objects.

Relation objects are representations of the query, they don't execute until display or fetch methods are called on it.

The Relation objects can be storesd in variables then called in later queries.

#### Data Input

Either use specific db methods or create a relation object that reads a file.

```         
db.read_csv('file.csv')
db.sql('SELECT * FROM 'file.csv')
db.sql('SELECT * FROM df)
```

Note: The advantage of `read_csv` method is that it samples the file before reading it, for possibly better data input and formatting.

#### Data Output

```         
duckdb.sql('SELECT *').write_csv('out_file.csv')
duckdb.sql("COPY (SELECT *) TO 'out_file.csv'")
```

#### Persistant Storage Database

`.connect` method used to connect to (+ create) a persistant database.

```         
con = db.connect('file.db')
con.sql("CREATE TABLE test")
con.sql("INSERT INTO test VALUES (42)")
con.table('test').show()
```

Note: `connection` and `db` have the same methods, they can be used interchangably. The important part is that the `db` methods are for in-memory, `connection` is for persistant.

#### Dataframes

duckdb can directly query dataframes stored as Python variables.

```         
db.sql('SELECT * from df').show()
```

Can also register a Dataframe as a virtual table, like a SQL view, good if the DF is stored as a class variable, or dict etc.

```         
db.register('virt_df', df)
```

Can create a persistant table from a df:

```         
con.execute('CREATE TABLE table_name AS SELECT * FROM virt_df') # create a table
con.execute('INSERT INTO table_name SELECT * FROM virt_df') # insert into existing table
```

##### Interpreting Object Datatypes

Because Objects can contain any data, duckdb needs to analyse it by sampling before its inputted into the db. If the wrong datatype for the schema is chosen, it can throw an error. The sample size can be increased to ensure that the correct datatype is chosen.

```         
duckdb.default_connection.execute("SET GLOBAL pandas_analyze_sample=100000")
```

#### Relation to Pandas Dataframe

`.df()`

Both `fetchdf()` and `fetch_df()` are available aliases of `df()`.

For large tables, `fetch_df_chunk(vector_multiple)` is available, where chunk is 2048 \* vector_multiple.

### DB API

Complaint with DB-API 2.0 specification described by PEP 249.

#### Multiple Connections

`cursor()` can be used to open multiple connections to a database at the same time.

The connection is closed when the cursor() passes out of scope, or `close()` is called.

#### SQL Queries

```         
con.execute("query 1")
con.execute("query 2")
con.fetchall()
```

#### Describe

`con.description`

### Relational API

Once a relational object is created, i.e. `db.sql()` then further relational objects can be formed through queries refering that variable name directly.

rel = db.sql("query_1") rel_2 = db.sql("SELECT \* from rel")

#### Built-in operations

-   `.aggregate()`
-   `.except_()`
-   `.filter()`
-   `.intersect()`
-   `.join()`
-   `.limit()`
-   `.order()`
-   `.project()`
-   `.union()`

### SQL

#### Comments

"--" used to prefix comments in SQL queries.

#### 1. Create new table

""" CREATE TABLE table_name ( column_name_1 DATATYPE, column_name_1 DATATYPE, ... column_name_n DATATYPE, ); """

##### Column Datatypes

-   INTEGER
-   SMALLINT
-   REAL
-   DOUBLE
-   DECIMAL
-   CHAR(n)
-   VARCHAR(n)
-   DATE
-   TIME
-   TIMESTAMP

#### 2. Remove Table

""" DROP TABLE table_name; """

#### 3. Add Values to a Table

""" INSERT INTO table_name (col_1, col_2, col_3) VALUES (col_val_1, col_val_2, col_val_3) """

-   non-numeric values must be inputted as strings, i.e. surrounded by single quotes.
-   dates must be inputted as YYYY-MM-DD.

#### 4. Table Queries

##### SELECT

To get data.

""" SELECT \* FROM table_name; """

To get specific columns:

""" SELECT col_1, col_2 FROM table_name; """

The SELECT statement actually produces a new table, so the output table can be highly customized within the statement.

The AS statement can rename the columns.

""" SELECT col_1 AS new_name_1, col_2 AS new_name_2 FROM table_name; """

#### WHERE

Particular rows of a column can be selected with WHERE, using AND, OR, NOT to define boolean expressions.

""" SELECT \* FROM table_name WHERE col_1 = 'value_1' AND col_2 \> 5; """

#### SORTING

SELECT tables can be returned with a specific ordering via ORDER BY

""" SELECT \* FROM table_name WHERE col_1 = 'value_1' AND col_2 \> 5 ORDER BY col_1; """

#### Remove Duplicate Rows

""" SELECT DISTINCT col_1 FROM table_name; """

#### Join Query

Queries can be made upon a number of tables at once in a composite query, or 'join query'.

""" SELECT table_1.col_1, table_2.col_2 FROM table_1, table_2 where table_1_col_1 = table_2_col_2 """

To specify the table the column comes from, use the dot accessor `table_1.col_1`.

#### OUTER Join

`LEFT OUTER JOIN` `RIGHT OUTER JOIN`

With syntax:

""" SELECT \* FROM table_1 LEFT OUTER JOIN table_2 ON (table_1.id == table_2.id); """

LEFT means that all rows of table_1 will be preserved, but only rows from table_2 that match table_1 id will be merged. RIGHT does the opposite.

### Editing super_table_pipe

To adapt all of the actions in super_table_pipe we'll have to apply some edits to it to enable plugging the database tables in, then import it into 'adapt_super_pipe_to_db.py' to perform the same actions. Then I can write each of the cleaned tables to a 'cleaned_X_table' within the db.

20:11 -

To adapt my cleaning pipes to the database table, which is based on rainbow directly, I am going to need to fork and edit rainbow directly to ensure that sequence names are included in the metadata.

## 2023-04-20

### Attempt no.2 at starting wine deg experiment

Outcome: Oddities in how Chemstation handles Methods and runs meant that I was unable to start the experiment today.

Cause: Once a method and sequence ends, the system resets to the starting flow rate of the last method.

Solution: - Rather than having the ramp up as part of a gradient within one method, you are required to have one method per flow rate step. - Best method is to program one with all the parameters necessary, then copy that file, then go back in and edit the flow rate on at a time.

My start up methods are in "C:\Chem32\\1\METHODS\\0_jono_methods\start\_up_methods" and the seqeunce is at "C:\Chem32\\1\SEQUENCE\\0_jono_sequences\FLOW\_RATE_RAMP_COLUMN_EQUILIB.S"

Unfortunately because I have to work tomorrow the experiment will be set back yet again. We'll just have to cop the time lapse, and hope that the ambient degredation is fast, and freezer degredation is very slow.

### Programming the automatic run

To program it, the order needs to be:

1.  Start up.
2.  Run Sequence.
3.  Shutdown.

The shutdown step should be programmed into the sequence itself.

So the macro is at: "C:\Chem32\CORE\daily\_ambient_sequence.mac"

And currently contains:

```         
name DailyAmbientSequence
    PrepRun
    LoadSequence ,"FLOW_RATE_RAMP_COLUMN_EQUILIB.S"
    RunSequence
    LoadSequence ,"2023-04-12_WINE-DEG_AMBIENT.S"
    RunSequence
EndMacro
```

Once begun, all data files will be located at:

"C:\Chem32\\1\DATA\\0_jono_data\\0_2023-04-12_wine-deg-study\ambient"

### Freezer Runs

#### Freezer Run Checklist

00:00 Replace H2O (Phase A) and top up MeOH (Phase B) 00:10 Start up instrument with macro 10_jono_flow_ramp_column_equilib.mac\* 00:10 Set up Sequence+. 00:30 Get sample e010y 00:30 Defrost sample e0x0y 00:35 Place e010y in vial loc 1. 00:40 FLOW_RATE_RAMP_COLUMN_EQUILIB.S ends. 00:45 Start todays WINE-DEG-FREEZER.S. 00:45 e010y injected. 01:25 Get sample e020y 01:25 Defrost e020y 01:30 Place e020y in vial loc 1. 01:35 e020y injected. 01:15 Get sample e030y 01:15 Defrost e030y 01:20 Place e030y in vial loc 1. 02:25 e030y injected. 03:05 Get sample n010y 03:05 Defrost n010y 03:10 Place n010y in vial loc 1. 03:15 n010y injected. 03:55 Get sample n010y 03:55 Defrost n010y 04:00 Place n010y in vial loc 1. 04:05 n010y injected. 04:45 Get sample n010y 04:45 Defrost n010y 04:50 Place n010y in vial loc 1. 04:55 n010y injected. 05:45 shutdown.

-   

    1.  type SHOW, double click on the macro in the window that pops up. 2. run "macro 0_jono_flow_ramp_column_equilib.mac, go". Should be able to just press the up key in the command line until you find it.

-   Sequence Template is titled 'WINE-DEG-FREEZER.S' in "C:\Chem32\\1\SEQUENCE\\0_jono_sequences\WINE-DEG-FREEZER.S". just modify the repeat number suffix on each sample name and data file name then save as "WINE-DEG-FREEZER-1.S", incrementing the suffix for every run.

    So for example, the first sequence is e0101, second is e0102, .., e0107.

## 2023-04-21

Time to begin the data analysis.

### Data Analysis Outline

1.  [ ] baseline correction.
2.  [ ] correlation coefficient analysis to select most representative spectrum.
3.  [ ] Peak alignment using Dynamic Time Warping using the most representative spectrum.
4.  [ ] Create a data cube tensor from all the included samples with dimensions samples, time steps, wavelengths.
5.  [ ] Mean centering.
6.  [ ] PARAFAC analysis to obtain factor matrices.
7.  [ ] K-means clustering to observe initial results.
8.  [ ] Combine factor matrices with metadata to train xgboost model.
9.  [ ] Evaluate results.

### To Do Today:

-   [x] begin wine deg experiment.
-   [ ] update logbook.

### Ambient Wine Start

09:00 - \[x\] Replace MeOH and H2O flasks with bigger ones. 09:15 - \[x\] Start up instrument (30mins) 09:20 - \[x\] set up autonomous sequences. - \[x\] Set up sequence overnight sequence.\* 09:25 - \[x\] set up single run method 09:30 - \[ \] test overnight sequence 09:50 - \[x\] test single run methods 10:00 - \[ \] wine 1. 13:00 - \[x\] get more methanol for lab 10:10 - \[ \] wine 2. 14:00 - \[ \] wine 3. 15:00 - \[ \] top up mobile phases. 13:00 - \[ \] start overnight sequence 13:15 - \[ \]

#### Overnight sequence:

Will start at 13:15. 50 mins per run. Say jake gets in at 10 tomorrow to do the freezer runs..

3 samples, run one after the other. for.. 8 repeats.

According to HPLC calculator, 1 44min run will consume:

28mL H2O 22mL MeOH

8 \* 3 runs = 24 runs.

28*24 = 672mL h2O 22* 24 = 528mL MeOH.

per block: h20 = 84mL. meoh = 66mL.

Should be right.

How to program the sequence?

#### Autonomous Sequence

5 runs, a0101, a0201, a0301, then 2 a0301 repeats.

issue: on test, instrument did not load specified sqeuence, instead tried to run the currently loaded one, in this case, column equilib.\$\$

Note: ! marks a comment.

load and run a macro with macro "MACRO/PATH" , go

## 2023-04-26

### Super Table Pipe.

Haven't completed that pipeline yet.

Starting with the three raw tables, need to end up with the super table, as one callable module.

1.  raw tables.
2.  cleaned tables.
3.  super table.

15-15: That's done.

Now, cleaned_sample_tracker seems to be running twice. fix this. Or at least the results are displaying twice.

## 2023-04-27

### todo:

-   [ ] depict events over weekend regarding wine deg experiment
-   [ ] troubleshoot solutions.
-   [ ] determine what needs to be done to fill holes in data.
-   [ ] get peak alignment code done.

### Depicting Wine Deg Experiment Timeline

[Timeline Table](https://docs.google.com/spreadsheets/d/15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY/edit#gid=2084667564)

#### Data Collected So Far

[wine_deg_data_table](https://docs.google.com/spreadsheets/d/15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY/edit#gid=1397176347)

#### Statements on Results Thus Far

refer to : [calculating_time_differences](/Users/jonathan/001_obsidian_vault/mres_logbook/calculating_time_differences.py) for calculations. \##### A0101

\[\[2023-04-27_a0101.png\]\]

Time points:

|     | datetime            | time_delta      |
|----:|:--------------------|:----------------|
|   0 | 2023-04-21 13:05:39 | 0 days 00:00:00 |
|   1 | 2023-04-24 10:57:14 | 2 days 21:51:35 |
|   2 | 2023-04-26 11:15:06 | 4 days 22:09:27 |

Regions:

i.e. areas of significant shape, derived from 2023-04-21 13:05:39

1.  0 - 3.5 mins.
2.  3.5 - 19.57 mins.
3.  19.58 - 35 mins.
4.  35 - 44 mins.

Significant peaks per region.

region 1 0 - 3.5 mins:

1.  1 triplet spanning 0.5 - 1.3 mins
2.  1 doublet spanning 1.2 to 1.6 mins
3.  1 doublet spanning 1.836 to 2.349 mins.
4.  1 singlet at 2.968 mins.

\[\[2023-04-27_a0101_region_1.png\]\]

region 2 3.5 - 19.57 mins:

1.  1 singlet at 3.585 mins.
2.  1 singlet at 4.731 mins.
3.  1 doublet at 5.569, 5.831 mins.
4.  1 doublet at 6.468, 6.738 mins.
5.  1 singlet at 7.331 mins.
6.  1 singlet at 8.060 mins.
7.  1 singlet at 9.065 mins.
8.  1 singlet at 10.127 mins.
9.  1 singlet at 10.672 mins.
10. 1 singlet at 11.025 mins.
11. 1 singlet at 11.391 mins.
12. 1 singlet at 11.672 mins.
13. 1 singlet at 12.252 mins.
14. 1 doublet at 12.891, 13.118 mins.
15. 1 singlet at 14.637 mins.
16. 1 singlet at 15.185 mins.
17. 1 singlet at 16.179 mins
18. 1 singlet at 16.926 mins.
19. 1 singlet at 18.160 mins.
20. 1 doublet at 18.519, 18.732 mins.

\[\[2023-04-27_a0101_region_2.png\]\]

Region 3 19.58 - 35 mins:

1.  1 singlet at 19.850 mins.
2.  1 singlet at 20.221 mins.
3.  1 singlet at 20.448 mins.
4.  1 singlet at 20.920 mins.
5.  1 doublet at ?, 21.391 mins.
6.  1 singlet at 30.560 mins.
7.  1 singlet at 32.242 mins.
8.  1 singlet at 33.519 mins.

\[\[2023-04-27_a0101_region_3.png\]\]

Region 4 35 - 44 mins:

Empty.

So. Changes by region:

region 1 0 - 3.5 mins:

1.  triplet 1 - peak 1 decomposes into two, drop by 30 mAU, peak two decreases by 10 mAU, peak 3 no change. Consistent increase in tR over time. Increase of 0.05 mins.
2.  doublet 1 - similar shape, shift of 0.15 mins to the right, peak 1 decrease in mAU of 30, peak two 10maU.
3.  Doublet 2 - shift of 0.3 mins to the right, decrease of 10mAU on peak 2.
4.  singlet 1 at 2.968 mins -

region 2 3.5 - 19.57 mins:

Region 3 19.58 - 35 mins:

Region 4 35 - 44 mins:

#### New approach

new method, using integration results:

1.  extract integration results for each run as csv
2.  align peak \# and area for each, calculate deltas.

Result: the native peak ID algo's misalign the peaks, or don't try and retain consistency, resulting in reported huge changes in peak area. ignoring the three mis id'd peaks results in a change ranging from 25.69 to 0.1 absolute.

#### Blank Runs

I am forming a small library of blank runs from which to select a representative for subtraction from library runs for the wine deg study.

Problem is, a set of peaks have been appearing from 15 to 23 mins, 7 peaks with a mAU from 5 to 35 consistently.

filepaths:

"\~/0_jono_data/2023-04-27_BLANK.D" and BLANK_2.D.

Running at 100% methanol to see what happens.

3 minutes in an amorphous peak came out with a mAU of 20.

Build back up to flow conditions, then see what happens.

14-17: done, peaks still present. At present time will ignore, try to remove later.

### Wine Deg Homepage

\[\[wine_deg-home\]\]

### Building Master File

The project has reached a level of complexity that requires the creation of a master file. Similar in spirt to [wine_auth_db_initializer](../../wine_analysis_hplc_uv/prototype_code/wine_auth_db_initializer.py), it will contain the top level pipeline from raw table creation to processing, step by step, creating all of the necessary tables etc. It will also act as an index leading to all other components integrated into the project at that time.

Components: 1. raw table acquisition 1. init_raw_chemstation_table 2. init_raw_sample_tracker_table 3. init_raw_cellartracker_table 2. table cleaning 1. init_cleaned_chemstation_metadata_table 2. init_cleaned_sampletracker_table 3. init_cleaned_cellartracker_table 3. Data processing

So really, we can alter wine_auth_db_initializer to serve this purpose. Modify 'initialize_db()' to 'build_db_library()', move the db deleter block to a seperate function and just keep adding.

### Debugging Duplicate print of cleaned sampletracker info

Noting now that the results of raw_sample_tracker table is printing twice as well.

Potentially duplicate tables? don think its possible.

Its definitely in init_cleaned_sample_tracker_table, at least the clean_sample_tracker_table reprint..

At this point, certain that runtime only calls display_table_info once.

Fixed, there was a second call to display_table_info after the write_df_to_table in sample_tracker_df_cleaner.

### Peak Alignment

\[\[mres-logbook_peak-alignment-module\]\]

## 2023-04-29

-   [ ] Finish freezer run 3.
-   [ ] start daily ambient sequence.
-   [ ] finish interfacing with peak alignment module.

Freezer run 3 requires me to complete the n freezer samples. 45 mins a run, 135 minutes all up. Once thats done start an ambient sequence and go.

### Finish Interfacing with Peak Alignment Module

Now that the data structure is established:

df\[sample_name, signal\[df\[mins, signal\]\]\] we need to create an interface between this structure and the dtw module.

But before that, need to build an interface between it and .corr.

.corr requires that we pass a 1D array, i.e. the y column, and the sample names as column names. So:

\[y_values, sample_name\].

## 2023-05-01

Chemstation has died. cannot load any agilent-oriented software.

Tried to use SFC computer to run 1260 Infinity HPLC, however the setup is missing the bin pump, otherwise good to go. Need to install bin pump, or make it detectable, etc. Not sure how it works.

Got chemstation installation disk, can try to repair the software. Will back everything up first tho.

18-36:

Ok, so the peak alignment pipe is at a point where we're ready to do 2 things:

1.  compare across wavelengths to find a most 'informative' wavelength, i.e. maximum variation.
2.  test the peak alignment on all samples in library.

the first one should occur AFTER baseline subtraction and time interpolation.

To do this:

1.  integrate baseline correction into the database. Both time interpolation and peak alignment should ONLY occur in-memory.. or be labelled for a particular dataset etc. The reason being that both of these actions are inter-sample, whereas baseline correction is intra-sample.
    1.  Do it for whole spectra.
2.  Write the time interpolation and peak alignment as tables in the database ANYWAY because analyses might be valid on sub-groups.
3.  Write a function to calculate the euclidean distance between input chromatograms.

Compare the euclidean distances across the pipeline.

## 2023-05-02

### Fixing 1260 infinity HPLC chemstation

Continuing from yesturday.

Left off last night waiting for \Chem32 to compress before moving to the hard drive. This didnt work because teh compressed directory is larger than the available space on the disk. Will dry compressing directly onto the hard drive.

12:14 -

Ended up running cp on powershell and copying over manually as scripting compression methods failed.

### Outlining A Data Processing Paper

[Outlining the paper](dp_paper_logbook/dp_paper_logbook.md#outlining-the-paper-and-setting-up-the-work-environment)

### Setting up for PCD

Need to: - \[ \] find quantiy of reagents. - \[x\] find pump. - \[ \] test pump.

### Fixing Chemstation

Current issue: bin pump not present in instrument configuration on SFC computer.

Going to try to fix this issue by copy pasting the drivers into the chem32 folder simply by copying the WHOLE driver dir from one computer to another.

-   [x] back up SFC driver folder.
-   [x] copy HPLC driver folder to usb
-   [x] remove SFC driver folder.
-   [x] copy HPLC driver folder into SFC chem32
-   [x] test.

Result: no change.

New approach: Try plugging in the iso pump and see what happens.

Solution Found! the issue was in the configuration of the instrument type - needed to be set to Agilent LC. On doing this, the pumps became visible.

14:59 - Error encountered.

"There is no valid entry for device GCI in the DRIVERS.INI file.

Sounds like the drivers folder might come in handy after all..

### Agilent HPLC 1260 Infinity Stack MAC address

\[\[agilent_1260_infinity_hplc_mac_address\]\]

### Daily Summary

1.  Began developing a usb-based workplace to be able to plug-and-play across windows and mac computers.
2.  Set up and validated an isocratic pump for use with PCD on the 1260 hplc.
3.  Set up and tested the 1260 HPLC setup on the SFC computer.
4.  started another ambient run.
5.  Outlined the data processing paper.

## 2023-05-04

### Migrating to VSCode

Have migrated the majority of my workflow to vscode, away from obsidian. Notebook and logbook templates are in place. Use "0_logbook_note" for logbooks, and a variation of "0_processing_template" for general notes.

### Bluetooth Enabled Workstation

Have installed UP500 bluetooth adapter into instrument computer to enable me to use both wireless mouse and keyboard between laptop and instrument computer. This will allow me to multi-task seamlessly while running experiments.

### Today

Now the instrument is back on track, its time to finish off the wine deg runs.

Today I will do a full freezer run, and set up yet another ambient run.

### Freezer Runs

THe run data results are to be stored as single run files in "\~\DATA\\0_jono_data\\0_2023-04-12_wine-deg-study\freezer".

12:58 -

e0103 started.

### Expand Pipeline to all Samples

Doing this will create too much data to analyse by eye. Thus we should come up with metrics to guage distance, i.e. euclidean distance.

The pipeline currently:

1.  gets the spectra.
2.  subsets the spectra.
3.  subtracts the baseline.
4.  interpolates time axis.
5.  finds the representative sample.
6.  aligns the sample chromatograms.

To describe the similarity between n-dimensional objects the euclidean distance can be used. I will write it as a seperate function and test it. Generalizing the peak alignment pipe to chromatogram-spectrums has proved harder than I initially thought, so I will develop a new pipe for it, then attempt to find a middle-ground.

### Time Formats in SQL

There is Date and Timestamp.

Date is format YYYY-MM-DD following ISO 8601 format. Timestamp is format YYYY-MM-DD HH:MM:SS+MsMs, also ISO 8601 format[@_TimestampType_2023].

### SQL Query to Select from super_table Unique new_id with the latest acq_date for that id

```         
"""
SELECT new_id, MAX(acq_date) AS latest_acq_date, ANY_VALUE(vintage_ct) AS vintage_ct, name_ct, varietal, hash_key
FROM super_table
GROUP BY new_id;
"""
```

## 2023-05-05

### Workflow

I have completely detached my workflow from Obsidian in favor of VSCode as a platform, while retaining markdown as the text format of choice. Along with this, I have created a top-level personal log file which will serve as a general-purpose dump of information AND track current system setup. It can be found at "\~/mylog/mylog.md". It is a git repo which will need to be committed after every update. Refer to it for more details, but basically, all research and logbook notes will be templated from the global snippets file, and symlinks will be added to workspace directories as necessary to enable a clean workflow.

Can create files from markdown wikilinks just like obsidian.

There will be a MRes Project homepage directory which will contain symlinks to each project homepage.

The logbook directory will contain symlinks to sub-project logbooks. directories i.e. papers stemming from the MRes thesis. There will only be one logbook homepage per, seperated into subprojects by headers. Too much atomization otherwise. Each project will have its own logbook file, and only one.

Starting from tomorrow for the main logbook (of which this file belongs to). The current chemometrics paper's logbook is [here](dp_paper_logbook).

Speaking of that project, its current structure is as follows:

. ├── dp_paper_logbook │ └── dp_paper_logbook.md ├── homepage_paper-data-processing-for-chemometrics.md ├── index │ ├── project_dir_structure_description.md │ └── outline.md ├── intro │ ├── hplc_uv-vis_fingerprinting.md │ └── intro.md └── research_notes ├── cross-distance_matrix.md ├── dtw-python_notes.md ├── dynamic_time_warping.md ├── giorgino_ComputingVisualizingDynamic_2009.md ├── images │ └── analytical_fingerprints_flowchart.jpeg ├── metabolites.md ├── metabolomics.md ├── metabolomics_targeted-vs-untargeted-studies.md ├── pharmaceutical_analysis.md └── spectrum-chromatograms.md

Which I am quite happy with. See how it goes over the next little while, and tweak where necessary.

### Data Processing for Chemometrics Paper Progress

[Narrowing the scope to ensure that I publish onrent time](./dp_paper_logbook/dp_paper_logbook.md#paper-progress---narrowing-the-scope)

> > > > > > > > > > DELETE THIS LINE AFTER TRANSFERRING OLD FILES TO ABOVE THIS LOCATION

## 2023-05-06

### Spectra Preparation Prior to Calculation of Euclidean Distance

[observing_spectra_shape_variation](../../../wine_analysis_hplc_uv/prototype_code/observing_spectra_shape_variation.py)

2023-05-06 00:17:44

To apply Euclidean Distance to measure the similarity of two 1D-arrays, the matrices need to be the same size. Thus I need to build a validation and broadcast function prior to every calculation of the distance.

Is it better to broadcast all spectra matrices to the same size before calculation, or pair-wise? For a meaningful result, should avoid broadcasting. Frankly, I need to understand how different the sizes are. Problem is, if I broadcast one, that will significantly increase the magnitude of the distance. Probably better to subset.

To observe the distribution of size differences:

1.  Get all sample matrices. ~~2. Form a sequence of pairs of chromatograms without reversable repeats.~~ ~~3. iterate through that sequence, for any whose shapes dont match, return the sample names and the shapes.~~

Wrong. Best approach is to get a Series of matrix shapes.

2023-05-06 01:32:40 done. Now to build safeguards. Apart from debertoli it appears that one or two matrices have one more observation than the majority. presumably just a glitch in the machine. To select those which are not like the norm, I will find the mode of the shapes and use that value as the reference. If a matrix is larger than the mode, I will trim it from the end to match the mode shape. If it is 10% less than the mode, I will broadcast with the last recorded value. If it is \>10% less than the mode, I will alert the user who can then choose to include or exclude those samples from the selection.

TODO: - \[x\] calculate mode of matrix shapes. - \[x\] implement trim to mode shape function. - \[x\] implement pad. - \[x\] implement user warning and pad for samples \>-10% of mode.

<!-- 2023-05-06 18:32:04 ended up implementing a mode and deviation table, see mres_logbook 2023-05-06-->

#### Further Thoughts on Implementation

2023-05-06 10:15:46

The problem with this implementation so far is that I have been focusing only on the row size, while talking about shape. I need to acknowledge that I am only concerned about row counts at this point, not column counts. Thus I should refocus my code to make this explicit.

I also dont actually know how to implement a subsettnig or padding. Should look into this before writing more code.

I can implement subsetting, or trimming or whatever you want to call it, through slice notation. i.e. \[:max_length\] where max_length is an integer.

But what about padding? It appears that pd.DataFrame.reindex can be used by defining extended index and columns, then reindexing the dataframe on those with argument `fill_value` = 0.

2023-05-06 11:43:10

Padding is done.

Current workflow is:

1.  Get all sample matrices.
2.  Get shapes of matrices as a df with index of sample names.
3.  rehape the matrices to match the library mode.

Now need to implement the variation test.

1.  find all samples whose shape doesnt equal the mode.
2.  print their shape.

### Removing Anomolous Files from Dataset

2023-05-06 14:46:56

backlink: [mres logbook](./mres_logbook.md#removing-anomolous-files-from-dataset)

While doing this, I have found at least one run that had \>11% of the mode of the selected runs. i.e. an aborted run. This raises the question of how many more there are in the dataset.

The function `dim_size_deviation_report`, currently in `observing_spectra_shape_variation`, which is being developed to support the calculation of euclidean distances between SC matrices, will be very helpful in verifying the shape of these matrices. The shape is important because most all matrix operations require that matrices be the same shape.

Anyway, I'll track that aborted run down and expunge it from the library. To do this I'll make sure the filepath is included in the super_table query so it carries through to the report.

The offending file is: "\~/0_jono_data/2023-03-14_2021-debortoli-cabernet-merlot_avantor_3.d". I'll just manually delete it from both instrument and local storage.

2023-05-06 15:41:09

Done. No other anomalous runs appeared, so it looks like we're good to proceed. The next stage is to verify the reshape function.

2023-05-06 15:48:21

Verified.

2023-05-06 14:46:56 first day of the new logbook format.At some point i'll go back and add the old logbook pages in.

Today I am in the lab running the 4th freezer runs. I am also working on calculating Euclidean distances of spectrum-chromatogram matrices [here](../prototype_code/observing_spectra_shape_variation.py).

2023-05-06 15:48:21 What now? Now I need to go back to `peak_alignment_spectrum_chromatograms` and see what we're up to.

### Calculating Euclidean Distances

backlink: [mres logbook](./mres_logbook.md#calculating-sc-euclidean-distances)

2023-05-06 15:59:07

Have introduced `observing_spectra_shape_variation.observe_sample_size_mismatch` into `peak_alignment_spectrum_chromatogram.peak_alignment_spectrum_chromatogram`.

Correlation matrix has been 'successfully constructed, however all pair distances are equal to zero. Problem.

2023-05-06 16:42:50

All sc matrices are filled with zeroes. presumably the resizing function is not putting the original values back in.

2023-05-06 17:05:38

The offending line of code was in `observing_spectra_shape_variation`.reshape_dataframe:

``` python
else:
        new_columns = range(desired_columns)
  ->   df = df.reindex(columns=new_columns, fill_value=0)
```

THe full block does the following (pseudocode):

if no 'desired_rows' provided: desired_rows = num_rows of current dataframe

if no 'desired_cols' provided: desired_cols = num_cols of current dataframe

if desired_rows \< num_rows current df: slice df to length equal to desired_rows else: 1. make a new index equal to a range of length desired_rows 2. reindex the df with new_index, fill with zeroes.

Looks lke it should work, however it is causing problems. Since there is no instance where there will be less columns (for now) ill leave it commented out and move on.

2023-05-06 17:39:46

Ok, `wine_analysis_hplc_uv` repo is up to date..

So the calculated distance values are huge, like 5 digit numbers. It migh be nicer to normalize the absorbance axis to reduce the absolute magnitudes. But how do I do that? Also what is the units of the Euclidean distance? probably the units of the vector.

I will add a normalization of library function here[label](../prototype_code/peak_alignment_spectrum_chromatograms.py) "normalize_library_absorbance"

2023-05-06 18:02:54

Done. Program is running a bit slow considering there are only 5 samples though, should investigate chokepoints with @timeit.

Looks like its the normalization block. Wonder why.

Fixed. Shouldnt be surprised that applymap methods are slow. Replaced with MUCH faster select_dtypes method.

#### 2023-05-06 End of Day Summary

What now?

THe fourth freezer run is about to finish, and next I will be initialzing yet another ambient run.

I've run out of time today, and I cannot come in tomorrow as I have other things to do.

On monday I will run the remaining UV/vis samples then swap to PCD on Tuesday.

In the meantime, regarding data processing, we need to verify what the l2 norm matrix is looking at, i.e. where in the pipeline we are. Once verified, need to write a method to pickle the l2 matrix.

Really need to create a DA pipeline / project outline in the homepage that I can refer to. starting to get lost/distracted, need to keep my eye on the ball. Be good to spend half an hour collating project notes from the logbook and etc.

## 2023-05-08

### Measuring Effect of Peak Alignment

Have started a project to measure the effects of peak alignment on the original signal:

[Measuring Effect of Peak Alignment](notes/project.peak_alignment.md#measuring-effect-of-peak-alignment)

2023-05-08 10:47:40

Last day of UV/vis.

TODO:

-   [x] methanol flush.
-   [x] freezer runs.
-   [x] ambient runs.
-   [x] library sample runs.

#### Realising that the 44min runs arnt at 2.5%

Have realised that the '44min' method was actually at a 3.16(...)% gradient, NOT 2.5%. Great.

Lucky it appears that only the wine degredation study and samples 96 - 100 are affected by this. Now considering that the differnce in gradient isnt HUGE, there is still a potential to align the signals, saving this data. The CUPRAC portion of the wine deg study will need to be run 3.16(...)%/min gradient.

#### 44 min 3.16(...)%/min gradient timetable

| min   | A    | B    |
|-------|------|------|
| 00:00 | 095% | 005% |
| 30:00 | 000% | 100% |
| 32:00 | 000% | 100% |
| 34:00 | 095% | 005% |
| 44:00 | 095% | 005% |

#### How to Calculate Gradient

To make it clear, a gradient elution method % gradient should be calculated at the time point prior to the time point where methanol is set to 100%. I.E in the table above, the gradient is calculated at 30 mins, where the difference is from 5% to 100%, or a 95% increase. 95% over 30 mins = 95/30 = +3.16(...) % / min.

Now, in the wine deg, for these samples, we want to cut it at 24 mins rather than 30, or 6 minutes earlier.

y = mx+5 where y is the % methanol, and t is time.

#### 24 min 3.16(...)% / min Timetable

at 24 mins, 3.16(...) = 81% methanol at that time point.

|  min \| A \| B
| 00:00 \| 095% \| 005%
| 24:00 \| 019% \| 081%
| 25:00 \| 000% \| 100%
| 27:00 \| 000% \| 100%
| 29:00 \| 095% \| 005%
| 39:00 \| 095% \| 005%

#### Method Update

2023-05-08 12:20:40

New method setup as per timetable above '\~\\0_jono_methods\\0_H2O-MEOH-2_5_37-MINS.M'. Will be using it for the remainder of the deg study. For CUPRAC, will have to set up one with isocratic pump timetable as well.

Speaking of CUPRAC, will have to produce the new samples tomorrow as well. big day.

#### Getting back to It

Last week we [left off](#2023-05-06-end-of-day-summary) finishing up the L2 norm similarity matrix. We need to:

-   [ ] make a clear data processing pipeline.
-   [ ] Run the full L2 norm matrix module to see how long it takes to run. \[\[\]\] Once that's done, focus again on the dp paper. After lunch review where we're at with the outline and make a TODO list for the week.

First though, I need to revig a project structure. The heirarchy will be:

homepage -\> mres -\> project -\> sub-projects.

\[\[homepage\]\] -\> \[\[homepage_wsu_research.md\]\] -\> \[\[homepage_wine-analysis.md\]\] -\> \[\[homepage_paper-data-processing-for-chemometrics.md\]\]

So the homepage_wine-analysis is in \~/wine_analysis_hplc_uv/notes/ but is soft linked in \~/001_obsidian_vault/homepages/ so its visible there too. I am going to move mres_logbook to wine-auth_logbook.md in wine_analysis_hplc_uv/notes/ and add another symlink to its original location.

#### Data Analysis Pipeline

2023-05-08 09:00:00

We do have the [build_db_library](../prototype_code/build_db_library.py) file, but that is intended for database initialization prior to data processing, i.e. phase 1. There are multiple phases to this project and the overall project code will reflect that:

Phase 1: Data Collection and Preprocessing Phase 2: Data Processing Phase 3: Data Analysis Phase 4: Model Building Phase 5: Results Aggregation and Reporting

Phase 1: is covered by build_db_library. Need a superstructure file. call it "core.py". It can be the main driver. It is [here](../core.py)

The reason this stuff is needed is because without a heirarchy the project is flat and its uncertain where it begins and finishes, so to speak.

### Renaming Peak Alignment Module

2023-05-08 00:00:00

peak_alignment.py has been renamed to peak_alignment_pipe.py, just to better reflect its nature, and aspects of it have been refactored to make it more modular.

##### Acq Date Problem

2023-05-08 15:22:25

Apparently for all the samples I selected to test code on, they all have the same acq_date timestamp, being 2023-04-04 00:00:00.000. Looking at the database super_table acq_date column shows that the lack of HH:MM:SS is consistant across all samples, but the dates do vary. I guess all of these samples were run on the same day.

I will need to investigate why the HH:MM:SS is empty, but its a sub-priority for now. Would definitely be useful for the wine deg analysis..

#### Adding normalization to Peak Alignment Pipe

2023-05-08 17:00:00

For the samples:

| \#  | wine                                     |
|-----|------------------------------------------|
| 0   | 2021 stoney rise pinot noir              |
| 1   | 2020 yangarra estate shiraz mclaren vale |
| 2   | 2022 alkina grenache kin                 |
| 3   | 2021 babo chianti                        |

The peak alignment is causing a change in the absorbance axis as well as the time axis. Which was not expected. To observe this better I will integrate peak finding into the graphical display.

I desire to have each peak maxima marked on the display, with a hover of its x and y value and peak idx in order from 0 to n.

###CUPRAC Reagent Formula

2023-05-08 [CUPRAC Reagent Formula](notes/project.hplc.cuprac.md#cuprac-reagent-formula)

#### Final UV run

2023-05-08 18:59:54

backlink: [mres logbook](./mres_logbook.md#final-uv-run)

13 more samples to set up.

-   [x] add to sample tracker
-   [ ] add to cellartracker
-   [x] set up sequence
-   [x] prepare samples.
-   [x] get more water for phase A.
-   [x] inject.
-   [ ] update 'sampled_date' on sampletracker

#### Plotting in Plotly

2023-05-08 20:00:00

[mres logbook](./mres_logbook.md#plotting-in-plotly)

Have run into a major problem with Plotly - it doesnt keep marker sizes when zooming in, i.e. zooming in causes the lines and markers to appear the same size relative to the plot window rather than the axis value. This means that when I for example zoom in on a peak interval, there is the disconcerting effect of the marker remaining the same size. Apparently this is a reported feature issue that Plotly does not seem willing to address. Thus we turn to other plotting libraries.

I think we should return to seaborn/matplotlib.

Going to be a bit of a pain to redo the code, but really, there isnt that much to modify. More a problem of relearning matplotlib.

2023-05-08 21:39:46

matplotlib/sns dont play well with interactivity, 3d plots or streamlit. Best to stick with plotly.

[RenaudLN](https://community.plotly.com/t/scattermapbox-fix-radius-of-marker-when-zooming/11630/4) says that using client side callbacks.

[Emmanuelle](https://community.plotly.com/t/keeping-dot-size-fixed-on-scatterplot/39539/3) says that shapes + an invisible trace will provide the expected behavior, the trace enabling hovering.

## 2023-05-09

### Order of Data Preprocessing Operations

2023-05-09 09:22:16

Normalize after baseline subtraction so that the peak maxima is still 1. if normalize before, no difference but the peak maxima is 1 - baseline, bit messy.

### Pickling Peak Alignment Pipeline to Speed Up Processing

2023-05-09 10:51:45

pipeline is done, but realising that pickle doesnt play nicely with either streamlit or pipelines. Or, the three dont play nicely with each other.

Scenario 1:

pickle whole pipeline, display plots at the end.

drawback - have to store intermediate results, breaking cleanless of pipeline.

Scenario 2:

pickle individual components of pipeline, have option to use or not

drawback - still messy, but better. hard to figure out how to program. Have to pass use_pickle to each pipe. Can use presence of filepath as the bool. If no filepath, make new.

filepath pattern: \~/peak_alignment_pipe_pickles/{pipe}.pk1

### Setting up Infinity Stack for CUPRAC

2023-05-09 - [Setting up Infinity Stack for CUPRAC](notes/project.hplc.cuprac.md#setting-up-infinity-stack-for-cuprac)

### Preparation for First CUPRAC Wine Deg Run

2023-05-09 - [Preparation for First CUPRAC Wine Deg Run](notes/project.wine_deg.cuprac.md#preparation-for-first-run)

### CUPRAC Purge

[CUPRAC Purge](notes/project.hplc.cuprac.md#cuprac-purge)

### Cleaning Wine Study Files for Database Entry

2023-05-09

Today I will begin version 2 of the build_library pipe, whose progress will be documented in the file linked below:

[Cleaning Wine Study Files for Database Entry](notes/project.wine_data_aggregation.md#cleaning-wine-study-files-for-database-entry)

#### Leak Issues

2023-05-09 17:32:14

Dumb day really, managed to overwrite/delete the first coffee run. On swapping back to my column, had leak issues for an hour stemming from a crushed capillary, which i have replaced. Its too late in the day now to commence runs again, as I will need an hour to do Jakes column. Leave it till Thursday.

#### Refactoring Build Library

2023-05-09 [Refactoring Build Library](notes/project.wine_data_aggregation.md#refactoring-build-library)

## 2023-05-11

### Verifying Performance of CUPRAC on Agilent Infinity Stack

Documents my efforts to move from standard HPLC-DAD to CUPRAC derivatized detection via verification on a coffee standard.

2023-05-11 [Verifying Performance of CUPRAC on Agilent Infinity Stack](notes/project.hplc.cuprac.md#verifying-performance-of-cuprac-on-agilent-infinity-stack)

### Starting Cuprac Wine Deg Runs

Documents the first day of runs, preparation of reagents, programs, and troubleshooting, and authorship of the CUPRAC SOP.

2023-05-11 - [Starting Cuprac Wine Deg Runs](notes/project.wine_deg.cuprac.md#starting-cuprac-wine-deg-runs)

Note: I have for the first time applied black to the whole 'wine_analysis_hplc_uv' project dir to autolint. Too much to check all, but a brief skim looked hopeful.

## 2023-05-12

### Fixing Datafile Aggregation Package Imports

My constant struggle to understand the logic of Python imports is played out in the section linked below:

2023-05-12 - [Fixing Datafile Aggregation Package Imports](notes/project.wine_data_aggregation.md#fixing-datafile-aggregation-package-imports)

### Cuprac Wine Deg Sequence Failure

A description of a catastrophic sequence failure after the first day of running CUPRAC can be found by following the link below. In summary, precipitate was forming and blocking flow to the detector. The solution is determined to be halving the injectin volume.

2023-05-12 [CUPRAC Overnight Sequence Failure](notes/project.wine_deg.cuprac.md#cuprac-overnight-sequence-failure)

### Establishing CUPRAC Startup Methods

A brief note stating that automated start up sequences in Chemstation have been prepared can be found by following the link below:

2023-05-12 [CUPRAC Startup Methods](notes//project.wine_deg.cuprac.md#cuprac-startup-methods)

### Batching CUPRAC Reagent

The first bulk batch of CUPRAC reagent was prepared today and is described in a note found by following the link below:

2023-05-12 [CUPRAC Reagent Batch](notes//project.wine_deg.cuprac.md#cuprac-reagent-batch)

## 2023-05-15

### Structuring Thesis Reading Notes

2023-05-15 09:15:21

Thesis structure has been started.

For each topic section, need to have the following paragraph structure:

1.  What is the topic.
2.  When is the topic.
3.  Why is the topic.
4.  How is the topic.
5.  Where is the topic.

Or something like that.

### Locating Notes by Topic using Grep

[mres logbook](mres_logbook.md#locating-notes-by-topic-using-grep)

Logbook is still fragmented into individual files, so to search over I can use noisy GUI search options or I can use grep.

1.  use grep to get list of files containing references to HPLC

2.  go through the list, extract useful information into their own notes.

3.  link to these notes in chromatography_homepage.md under #HPLC.

4.  \[\[mres_logbook_hplc_grep.md\]\]

result:

Didnt work so well, grep got weird results. Doing manually.

### Organizing Notes on Hand

2023-05-15 15:00:00

I've been performing research for over a year now, but my notes are scattered AF. Need a methodology of organising them.

Current approach:

1.  Homepage -\> sub-homepages -\> headings -\> individual notes.

The atom will be the reading note of a literature source.

Since homepages are TOPIC homepages, the same literature note could fall into a number of different TOPICS. Duplication of notes within topic homepages is not ideal. ergo homepages should not contain bodies of text, only links and descriptions of links. Infomration should be stored in standalone notes.

Literature note files will be headed with the citekey of the source as per the the current zotero setting - name_TitleKeyWords_year.

Throw them all in reading_notes_2023.

Time to get shit done.

as of 2023-05-15 12:44:21, zotero URI doesnt work from VSCode.

2023-05-15 13:20:19

Gone through 998_calendar/ and extracted useful lit notes into homepage_literature_notes.

2023-05-15 14:48:38

Have compiled a list of 'verified' literature notes in '\~\homepages\homepage-literature-notes.md' have deleted empty lit notes in `~\z_literature_notes\`.

Now, I will spend another hour fixing my library pipeline, then another hour reviewing my collected notes, then another hour fixing the code, then another hour reviewing the notes.

### Finalizing Library Pipe

2023-05-15 16:16:15

Swapping back to writing. Currently testing the cleaning functions.

2023-05-15 17:27:54

Back to coding. Left off still trying to find a good review of modern HPLC.

### Making Build_Library Modular for Different Project Demands

A note on how the chemstation, sampletracker and cellartracker ETL pipelines need to be seperated prior to combining their data can be found by following the link below:

2023-05-15 [Making Build_Library Modular for Different Project Demands](notes/project.wine_data_aggregation.md#making-build_library-modular-for-different-project-demands)

## 2023-05-16

2023-05-16 10:10:06

TODO:

-   Start CUPRAC deg analyses.
-   [x] start up instrument.
-   [x] Make CUPRAC reagents.
-   [x] create cuprac solution, degas for 20 mins.
-   [x] modify methods to inject 5mL wine.
-   [x] Start ambient runs.

Labwork Landing

-   [x] Labwork Landing Page
    -   [x] SOP's
    -   [x] Make CUPRAC SOP sheets as markdown files, add to landing page.

Thesis

-   [x] Create an instrumentatin spec sheet to be used as basis of experimental section.
-   [ ] finish HPLC section.
-   [ ] finish PCD section.

Have filled out instrument specifications, located [here](mres_thesis_instrument-specs-setup.md).

### CUPRAC Experiment Problem - Sawtooth Pressure Curve

A description of a anomolous sawtooth curve occasionally encountered during CUPRAC runs, especially after mobile phase renewals, can be found by following the link below:

2023-05-16 [Problem: Sawtooth Pressure Curve](notes/project.hplc.cuprac.md#problem-sawtooth-pressure-curve)

### CUPRAC Wine Deg Experiment Update

2023-05-16 [Experiment Progress Update](notes//project.wine_deg.cuprac.md#experiment-progress-update)

### 2023-05-17

2023-05-17 09:07:00

Tasks:

1.  [x] Build a pipeline to prepare data to present to Andrew.
2.  [ ] HPLC Note.
    1.  [x] take reading notes from @sun_ModernTechniquesFood_2018
    2.  [x] proecss reading notes
    3.  [ ] write paragraph.
3.  [ ] CUPRAC Note.
4.  [ ] PCD Note

| Time Interval | Task                          |
|---------------|-------------------------------|
| 9 - 10        | HPLC note                     |
| 10 - 11       | Pipeline                      |
| 11 - 12       | HPLC note                     |
| 12 - 1        | Lunch                         |
| 1 - 2         | CUPRAC note                   |
| 2 - 3         | get ready for work and leave. |

### AWRI Presentation

Andrew has requested that I prepare a presentation for him to share with AWRI. The project can be found by following the link below:

[cuprac wine data presentation](../../../../../raw_uv_cuprac_comparison/wine.cuprac.data-presentation.md)

### 2023-05-18

Lab day.

TODO:

-   [x] get instrument and running
    -   [x] cuprac startup [CUPRAC SOP](chemistry_sop_cuprac.md)
    -   [x] run normal freezer wines.
    -   [x] run extreme freezer wines
    -   [x] back up "\~/chem32/3/"
    -   [x] fix last sequence data, organise properly.
        -   [x] organise data better
        -   [x] enter samples into sampletracker with the ids that they have on chemstation
    -   [x] start next overnight sequence

### New Chemstation File Organization

2023-05-18 18:51:00

I have organised the data in 0_jono_data to correspond to projects. All relevant sequences have been harvested of their constituant .D dirs, who have been placed in their again, corresponding (newly created) project dirs. There as follows (as of this date). On my drive, the data dir before reorg is under "0_jono_data_2023-05-18", and after under "0_jono_data_2023-05-18_neworg/0_jono_data/":

-   2023-04-27_raw_uv-blanks/
-   cuprac/
-   old_data/
-   raw_uv/
-   sequences/
-   tests/

It goes without saying that all samples bar old_data/ and tests/ are wine.

To keep the data dir organised, all 'test' runs should go under the corresponding dir.

Now to quickly digest the HPLC reading notes..

## 2023-05-22

### Update on CUPRAC Experiments

Follow the link below to find a brief update on the success of the CUPRAC runs thus far:

2023-05-22 [Update on CUPRAC Experiments](notes/project.hplc.cuprac.md#update-on-cuprac-experiments)

### Reusing Krud Catchers

2023-05-22

It was decided that we can reuse krud catchers by backflushing them after a number of runs.

TODO:

-   [x] send data to andrew
    -   [x] get the wine deg data, first injections of both , and look at their veracity. If good, send to andrew as csv's.

### Fixing Highlighting Problem in Terminal When Viewing Chemstation Files on Mac

2023-05-22 13:52:36

The highlighting issue in terminal view of data files has happened again. This is occuring because zsh authors expect files to be 644 permission, that is -rw-r--r--, but they are all drwxrwxrwx, or 777. This is deemed as bad. To fix (i.e., remove the highlighting) run the following:

``` bash
chmod -R 0_jono_data .
```

### wine_analysis_hplc_uv as an Importable Package

A milestone moment occured today when I found that I needed to be able to make wine_analysis_hplc_uv (or at least `build_library`) into an importable package, and how to import them into other projects. More can be found by following the link below:

2022-05-22 [wine_analysis_hplc_uv as an Importable Package](notes/project.wine_data_aggregation.md#wine_analysis_hplc_uv-as-an-importable-package)

## 2023-05-23

2023-05-23 09:58:27

TODO:

-   [x] fix git repo so I can push to origin.
-   [x] modify chemstation so I can process the files without creating a database, just output as a csv.
-   [x] Complete the todos [here](wine.cuprac.data-presentation.md)
-   [ ] review HPLC related articles in mobile browser.
-   [ ] write HPLC note

### Refactoring wine_analysis_hplc_uv

At this point I had to acknowledge that the code was too interwined to allow for easy development and that a number of changes needed to be made before I could continue. This is described in greater detail in the link below:

2023-05-23 [Refactoring wine_analysis_hplc_uv](notes/project.wine_data_aggregation.md#refactoring-wine_analysis_hplc_uv)

## 2023-06-02

### Data Aggregation Project Update

The link below describes the results of my refactoring:

[Data Aggregation Project Update](notes/project.wine_data_aggregation.md#refactoring-update)

I have to get ready for work now, but when I get back to the laptop..

TODO: - \[x\] finalise commiting changes to wine_analysis_hplc_uv - \[x\] clean up project - \[x\] identify what else needs to be added to ChemstationProcessor - \[x\] merge refactoring-chemstation-for-use-outside-of-pipeline back into mainG - \[x\] to do this: - \[x\] test performance of package in the branch - \[x\] review the diffs

## 2023-06-04

### Sample Tracker Data Entry

The link below describes how I cleaned the sampletracker data for pipeline consumption:

[Sample Tracker Data Entry](notes/project.wine_data_aggregation.md#sample-tracker-data-entry)

### Update on SampleTracker Class Development

Following the link below will lead you to a description of my troubles writing a SampleTracker google sheet class based API:

2023-06-04 [Update on SampleTracker Class Development](notes/project.wine_data_aggregation.md#update-on-sampletracker-class-development)

## 2023-06-06

2023-06-06 09:13:08

TODO:

-   [x] write email to GRS about extension
-   [x] fix data acquisition pipeline.
-   [x] finish cleaning sampletracker.
-   [ ] perform qualitative descriptive data analysis.
-   [ ] perform quantitative descriptive data analysis.
-   [ ] data treatments
-   [ ] model building

TODO:

-   [x] fix data acquisition pipeline
-   [x] write and send grs email
-   [x] complete sampletracker
-   [ ] qualitative analysis of samples.

## 2023-06-07

### Update on OOP Refactor of `wine_analysis_hplc_uv`

An update on the refactoring of the project can be found by following the link below. At this point, `SampleTracker` and `MyCellarTracker` are done.

2023-06-07 [Update on OOP Refactor of `wine_analysis_hplc_uv`](notes//project.wine_data_aggregation.md#update-on-oop-refactor-of-wine_analysis_hplc_uv)

TODO

-   [ ] EDA on sample_tracker + cellartracker:
-   [x] Total counts by detection type
    -   [ ] for each detection type:
        -   [x] total unique sample count.
        -   [x] describe duplicates
        -   [ ] count unique wines by
            -   [x] varietal
            -   [x] type
            -   [x] country of origin
            -   [x] vintage

## 2023-06-08

### Chemstation Unit Tests

Unit tests for `ChemStation` have been written, as described in the link below:

2023-06-08 [Chemstation Unit test_sampletracker](notes/project.wine_data_aggregation.md#chemstation-unit-tests)

### Logging in Python

A cheatsheet for Python logging (and minimum standards that I have set for my future projects) can be found by following the link below:

2023-06-08 [Logging in Python](notes/python_logging_cheatsheet.md#python-logging)

## 2023-06-09

### Rebuilding Build Library

Efforts to reconstruct `build_library` ETL pipeline with OOP for related behavior can be found by following the link below. It was at this point that I began using project defined constants for filepaths:

2023-06-09 [Rebuilding Build Library](notes/project.wine_data_aggregation.md#rebuilding-build_library)

## 2023-06-11

### Debugging ChemstationProcessor

The link below leads to a description of a XML parsing error encountered when trying to read what would turn out to be corrupted .acaml files:

2023-06-11 [Debugging ChemstationProcessor](notes/project.wine_data_aggregation.md#debugging-chemstationprocessor)

### Storing Multi-way Data in Databases in Long Form

A description of my reasoning for converting three-way spectral data into long format for database storage can be found below:

2023-06-11 [Storing Multi-way Data in Databases in Long Form](notes/project.wine_data_aggregation.md#storing-multi-way-data-in-databases-in-long-form)

## 2023-06-13

### Chemstation Package Rebuild Update

A penultimate `build_library` development update can be found below:

2023-06-13 [Chemstation Package Rebuild Update](notes/project.wine_data_aggregation.md#chemstation-package-rebuild-update)

## 2023-06-14

### Establishing DB Insert Proceudures

Speculation on how to update a persistant database file rather than regenerating from scratch for an update can be found by following th link below:

2023-06-14 [mres logbook](notes/project.wine_data_aggregation.md#establishing-db-insert-procedures)

### Reintroducing Cleaning Functions

Now that the OOP interface for the ETL pipeline are done, cleaning functions needed to be added back in. That is described in the note that this link leads to:

2023-06-14 [Reintroducing Cleaning Functions](notes/project.wine_data_aggregation.md#reintroducing-cleaning-functions)

### Testing Suite Rules

2023-06-14 11:00:00

1.  It needs to be vertical. That is, specific modules should rest within directories dedicated to a topic, i.e. \~\tests\test<topic>\test<topic_2>\test\_<specific>.py
2.  It needs to be specific: 1 short function per test. Extract functions that establish a state from the test itself, so the state functions can be used in later tests that build on the aspect being tested at that time.
3.  Make it modular. Have as many of the state-setting variables be arguments of the test function so that the state can be controlled from the top-level caller. In this way, we can change the focus of the test suite without modifying the test code.

### Cleaner Tests {#cleaner-tests}

A note on developing tests for my cleaners.

2023-06-14 [cleaner tests](notes/project.wine_data_aggregation.md#cleaner-tests)

### Identifying Non-matching ids

A description of how to detect ids that have failed to be cleaned correctly can be found by following the link below:

2023-06-15 [identifying non-matching ids](notes/project.wine_data_aggregation.md#identifying-non-matching-sample-ids)

### Moving My Test Modules to mydevtools

All test modules have been moved to "mydevtools.testing". All previous tests will now need to be updated to call "test_report" from that module. To reduce duplications, import "mytestmethods" into the test subpackage root.

## 2023-06-16

### Fixing Project Environment

2023-06-16 - \[\[fixing_project_environment\|Here\]\] is a note on my struggles to make my Python project environment function.

### Python Kwargs

2023-06-19 - \[\[pyton-kwargs\|This\]\] is a note on how to use args, kwargs in Python with the \*\* unpacking operator.

## 2023-06-21

### Rectifying Wine Identify and Sample id

While writing unit tests for the chemstation cleaner functions, I [discovered](#cleaner-tests) that the sample tracker id index was out, and had repeated 116 twice. This is a problem because that field was the only key linking the chemstation files to a samples identity. As it's out by one, we just need to verify which wine 116 actually is.

116 wines:

wine 1: 2022 sigurd chenin blanc wine 2: 2021 marco de bartoli cattarato

116 Observations:

-   It's a CUPRAC wine, run 23-05-18
-   spectrum is wrecked, could be what CUPRAC will look like.
-   the 450nm wavelength looks ok as long as you discard after 25 mins or so.
-   The sequence was 'CUPRAC_WINES 2023-05-18 16-48-18' which matches up with wine 2.
-   116 was the first designated 'curpac' wine. 115, 114 have been sorted into 'raw_uv'.

I can confirm that 114 is a raw UV wine, judging from its sc.

Note: any wine that is not cuprac only has a wavelength range up until 400nm ( at that time point) because i had to increase it to 450nm to catch the cuprac wavelength.

Ok so at this point I am fairly certain that 116 is wine 1. Another thing I could check is whether the next red wine lines up. That would be 120, 2021 Stefano Lubiana Pinot Noir.

It's definitely a red, but then the next three wines are also pinot noirs.

There are several routes I can take to get to the bottom of this mystery:

TODO:

-   [ ] check the backups (I dont think I preserved anything tbh)
-   [ ] get 'description' into the metadata tables, hope that some of them are different, and use them for alignment.
-   [ ] find a red and a white one after the other and compare the pair. Ideally a big red and a light white. For example, 153 and 154. 153 is a petit chablis and 154 is a margaret river shiraz.

Features that need to be added to the dash app:

TODO:

-   [ ] wavelength and sample string presence in database - currently entering a missing wavelength or sample id results in obscure errors.
-   [ ] sample overlays:
    -   [ ] samples on 2d plot
    -   [ ] 3d plots side by side (more than two, have a drop down or something..)
-   [ ] plot updates via database callbacks for modifying the display
-   [ ] ability to produce several apps to duplicate displays

Features that need to be added to data pipeline:

TODO:

-   [ ] read description field from `sequence.acam_` in `uv_extractor`.

2023-06-21 11:27:20

the "\~/0_jono_data/curpac/" directory is missing wines 140 to 179. Now, I'm certain I copied them before leaving the lab. The question is, where? My notes are a total mess, I need to clean them up. Problem is that they are currently under date subheadings, which is useless for locating specific topics, or sub-projects. Extracting them into sub-project notes with the logbook as a central index will be the most useful approach. However I would like to get some version control going, but the notes folder is not being tracked by git. So I should initiate a git repo, commit everything, then start sorting the notes.

TODO:

-   [ ] initiate repo in \~/mres_thesis/notes
-   [ ] break logbook down by topic

The method to do this is:

1.  Where a section is, place a title. this title will form the backlink target.
2.  extract the section into its own note, or append to a topic note, i.e. a sub-project.
3.  add a backlink in the new note / note section to that title.
4.  add a link to the new note / section header in the logbook file.

See how that goes.

2023-06-21 13:14:38 have started. Looking good, but concerned as to how fragile links are.. as usual.

### Mres Logbook File Branching

A look at how to go from a monolithic logbook file to a branched file that interweaves back so that the core logbook file (this one) maintains a chronological order, but the branched files are topic/progress/chronological order. The idea is to enable a consistant flow by topic/project in a file, but also track the chronology.

*Update*: As of 2023-09-26 12:40:17 this failed because A. I quickly got distracted and forgot about it altogether, but B. because text files are not able to manage this kind of refactoring, as everything has to be done manually. The lesson here is to write monolithic log files then reference them as desired, or dont keep logs at all. Or write daily/weekly logs, not concurrent logs. The reason for this is that fundamentally, every thought is not important, nor are most progress updates. It simply generates a lot of noise. The past two years of log files could have easily not been written at all, or the content instead be placed in project README files, or similar.

#### Naming Syntax

note name syntax: toplevelheiarchy.sublevel.name.md. If a space is necessary, use underscores. i.e. "project.hplc.raw_uv.md".

### Section Structure

in branch note:

``` markdown

... # content

## Section Heading

timestamp

backlink: [mres logbook](mres_logbook.md#section-heading)

... # content
```

in mres logbook:

``` markdown

... # content

## Section Heading

timestamp [Section Heading](notes/project_note.md#section-heading)

... # content
```

So make sure "Section Heading" is the same in all three locations to maintain a link.

## 2023-06-22

2023-06-22 10:00:47

Most of the way through the logbook fragmenting task that I begun yesturday. It's slow going because its super boring.

To make some progress on [Andrew Tasks](notes//project.andrew_tasks.md) I need to:

TODO:

-   [x] find the remaining cuprac files.
-   [x] unpack/rename the files as necessary - `/Users/jonathan/0_jono_data/0_2023-04-12_wine-deg-study/cuprac/ambient/2023-05-30/CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51`

2023-06-22 10:21:09

Found two suspect sequences at `/Users/jonathan/0_jono_data/0_2023-04-12_wine-deg-study/cuprac/ambient/2023-05-30/`, `/CUPRAC_WINE-DEG_AMBIENT_AUTO 2023-05-30 15-33-28` and `2023-05-30/CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51`.

`~/CUPRAC_WINE-DEG_AMBIENT_AUTO 2023-05-30 15-33-28` is a wine deg sequence.

`~/CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51` is generic samples running from 166 to 179.

2023-06-22 11:01:16

The missing sample files: 140 to 166, belong to the sequence that was rushed to complete, where the ids were possibly mistaken. I think that with everything else going on, I never copied those files over. Simple answer. I will go now to get the files.

### A Sequence Process Procedure

2023-06-22 14:02:40

For an 'unprocessed sequence' (rename the .D dirs to match the sample tracker id's):

1.  move sequence into `"/Users/jonathan/0_jono_data/sequences/unprocessed_sequences"`.
2.  When ready to being processing sequence, copy it to `"wip_sequence"`.
3.  in `"wip_sequence"` copy of sequence, for each .D di, open `"~.D/SAMPLE.XML"` and find the sample name/
4.  Rename the .D directory as per the sample name.
5.  Once all .D dirs renamed, mv to `"processed_sequences"`.

From there, feel free to copy the .D wherever else you like.

The point is that the three states of the sequence dirs are physically tracked, from unprocessed to processed, and at the end of any processing run, `"wip_sequences"` should be empty.

### Identifying The Missing Sequence

The easiest way to understand what is going on with the missing sequence would be to read the sequence of each sample from its data files. Luckily, 'sequence.acam\_' contains a sequence entry:

"ACAML/Doc/Content/SampleContextParams/Identparam/Name"

If I can integrate parsing of that entry into the rainbow process, we'll be able to add that extra metadata without adding overhead.

### Documenting Rainbow Code

1.  Call read() which checks the extension of the target directory. if .D, calls `agilent.read()`. `agilent.read()` calls: `chemstation.parse_allfiles()`, returning a list of DataFile objects:
    1.  for each file in path, calls `parse_file()`:
        1.  checks extension of file, calling `parse_ch()`, `parse_uv()` or `parse_ms()`.
            1.  For `parse_uv()`:
                1.  reads the binary stored data.
                2.  reads the 'notebook', 'date', 'method', 'unit', 'signal' and 'vialpos' into a "metadata" dict.
                3.  returns a DataFile object initialized with multiple arguments, including the metadata dict.
2.  read() then initialzes a 'metadata' var from `parse_metadata()`, initialized with the list of DataFiles.
    1.  gets the vial pos from one of a set of files, including `sequence.acaml`.

So, adding a line that reads the sequence name into the metadata dict at that location would solve my problem. Specifically, `"/Users/jonathan/Library/Caches/pypoetry/virtualenvs/wine-analysis-hplc-uv-F-SbhWjO-py3.11/lib/python3.11/site-packages/rainbow/agilent/chemstation.py"` line 736. Copy the behavior of the `"get_xml_vialnum"`. Hell, considering that the function parses the entire XML file, just rewrite the function to parse whatever metadata you want.

### Prototyping Rainbow Addition

I intend to fork Rainbow and modify `get_xml_vialnum()` to obtain a number of metadata fields from `"sequence.acam_"`.

TODO:

1.  [x] fork Rainbow into environment
2.  [x] Setup test environment.
3.  [x] make modifications
4.  [x] test modifications.
5.  [x] rebuild db.

### Fork Rainbow into Environment

-   [x] fork Rainbow
-   [x] identify path to clone into: \`"/Users/jonathan/mycode/python_code/rainbow++"\`\`\
-   [x] clone fork
-   [x] install devtools.

### Setup Test Environment

-   [x] setup unit test main module.
-   [x] get directory sampler module.
-   [x] import directory sampler module into test main module.
-   [x] test sampler.

### Make Modifications

The modification we are looking to make is to generalize `get_xml_vialnum()` to `parse_acam()` and allow it to accept a variable number of kwargs. It will read the XML file and for each kwarg it will read the contents into a dictionary that it returns. This means that I can add keywords down the track without further core modifications.

something like:

``` python
extract_fields(**kwargs):

return_dict = {kwarg, None for kwarg in kwargs}

root = tree.getroot()

for kwarg in kwargs:
    for result in root.xpath(f"..{kwarg}..") # add specific string
        if result.text
            return_dict[kwarg] = result.text

return return_dict
```

### Installing Dependencies with PIP and a pyproject.toml

Use `pip install .` to install declared dependencies from a pyproject.toml file.

### XPath

XML Path Language.

A "non-XML syntax" for navigating XML or XML-like files.

XPath uses URL-like path notation [@_XPathMDN_2023].

lxml is a Python package for processing XML [@_LxmlProcessingXML_].

2023-06-26 09:43:38

The path to the field I want to extract is [here](file://vscode.dev/github/jonathanstathakis/rainbow/blob/main/tests/inputs/red.D/sequence.acam_#L165).

According to [XML Tools](https://github.com/DotJoshJohnson/vscode-xml), the XPath to the sequence name field is:

/ACAML/Doc/Content/SampleContextParams/IdentParam/Name

This path however returns None.

## 2023-06-26

### Identifying Why Xpaths Did Not Work - Namespaces

2023-06-26 11:22:21

The reason for this was namespaces, of which XML Tools is agnostic. I have written a 'sequence.acam\_' extractor with a XPath factory function that depends on a dict of {Element Text: "/relative/xpath"}. This way I can modularly select how much data to extract, while being specific about the path to the element read.

As of 2023-06-26 12:12:05 this code has been integrated into rainbow.agilent.chemstation, replacing 'get_xml_vialnum'. Now to test.

To test, I need to replicate the behavior of uv_extractor then see what is in uv_file.metadata.

Interestingly, there is a hash_key in the sequence metadata xml files, perhaps I should use that instead of generating my own.

### Useful LXML Element Properties

2023-06-26 13:00:11

For these XML files I am handling, the only use useful lxml.etree.\_Element properties are `.tag`, `.text`, '.base', and '.sourceline\` which return the element descriptor, content, sourcefile and source file line, respectively \[sourcefile and source file line, respectively [@_LxmlEtreeElement_].

### Adding DataDir Metadata to ChemstationProcessor Metadata

2023-06-26 13:27:45

My modifications are completed and passing their tests. Now to test uv_extractor.

While adding these modifications I noticed that uv_extractor was ignoring the metadata stored at the directory object level, i.e. whre my modifications would take effect, so I've made some modifications there to combine the two metadata dictionaries generated from the ".UV" file and the data directory object.

I need to uninstall the current Rainbow package and install my modified fork.

### Identifying a Better Primary Key

2023-06-26 14:33:46

Changes have been passed testing with `chemstation.read_single_file`.

Before I merge the branch back into main, I want to see if we can use a hash key from the xml file as this will greatly simplify the addition or modification of database tables down the track. To do this I will iterate through a sample of datadirs and collect a chosen hash key into a set. If an encountered hash_key is already in the set, we will know that it is a duplicate.

2023-06-26 15:56:02

The following code has been run several times on small samples of datadirs, with no duplicates found:

``` python

from multiprocessing import pool
import trace
from mydevtools import testing, project_settings, function_timer as ft
from mydevtools.testing import make_test_sample_dir
import os
import shutil
from lxml import etree
import traceback


def testing_for_unique_hash_keys(pool_path: str):
    xpath_dict = {"id": "/SampleContexts/Setup"}
    ids = set()
    for datadir in os.listdir(pool_path):
        filepath = os.path.join(pool_path, datadir, "sequence.acam_")
        try:
            id = extract_sequence_metadata(filepath, xpath_dict)

            if id in ids:
                print("duplicate id found:", id)
                break
            ids.add(id)
        except Exception as e:
            print(filepath, e)
            print(traceback.print_exc())

    print(ids)
    return None


def xpath_factory(rel_path: str):
    namespace_inj = "/acaml:"
    path_root = "./"
    common_path = "/Doc/Content"
    path = common_path + rel_path
    path = path.replace("/", namespace_inj)
    xpath_exp = path_root + path

    return xpath_exp


def extract_sequence_metadata(filepath: str, xpath_dict: dict):
    """
    For a given .xml file "filepath" and dictionary of relative xpaths starting from
    "/Doc/Content", return a dictonary of extracted metadata.
    """
    assert os.path.exists(filepath)
    seq_metadata = {
        key: None for key in xpath_dict.keys()
    }  # setup the results container
    assert filepath.endswith(".acam_"), f"{filepath}"
    tree = etree.parse(filepath)  # create the etree object
    root = tree.getroot()

    namespace = root.tag.split("}")[0].strip(
        "{"
    )  # get the namespace for the file from the root tag
    ns = {"acaml": namespace}

    for description, rel_path in xpath_dict.items():
        try:
            # construct xpaths using the defined namespace
            xpath_exp = xpath_factory(rel_path)
            xpath_exp = xpath_exp
            # get the result of the xpath exp as a list
            result = root.xpath(xpath_exp, namespaces=ns)
            # extract the item text from results list
            # each item is <class 'lxml.etree._Element'>
            # ideally metadata is a flat dict, so if a result contains multiple items, assign them with incrementing keys

            id = result[0].attrib["id"]
            return id
        except Exception as e:
            print(e)
            print(traceback.print_exc())

    return seq_metadata


def main():
    dest_dir, src_dir, pattern = (
        os.path.join(os.getcwd(), "test_pool"),
        "/Users/jonathan/0_jono_data/mres_data_library/",
        "**/*.D",
    )
    make_test_sample_dir.create_test_pool(
        dst_parent_dir=dest_dir, src_dir=src_dir, filename_pattern=pattern
    )
    try:
        testing_for_unique_hash_keys(pool_path=dest_dir)
    except Exception as e:
        print(e)
        print(traceback.print_exc())
    finally:
        None
        shutil.rmtree(dest_dir)
    return None


if __name__ == "__main__":
    main()
```

now testing on total dataset..

### Identifying Corrupt sample.acaml Files

2023-06-26 16:02:38

Any samples that are not run as a sequence do not have .acam\_, but .acaml instead. The structure appears to be the same though. So probably need to add that clause in so that any single run samples have the Name SingleSample.

There are a number of datadirs for which I am unable to process in the current configuration. They are:

{'/Users/jonathan/0_jono_data/mres_data_library/raw_uv/2023-02-23_2021-DEBORTOLI-CABERNET-MERLOT_AVANTOR.D', '/Users/jonathan/0_jono_data/mres_data_library/raw_uv/029.D'}

029.D contains 'sample.acaml' 2023-02-23_2021-DEBORTOLI-CABERNET-MERLOT_AVANTOR.D contains 'sample.acaml'.

Now, I do have 'sample.acaml' in the file dict but its failing to parse - could be a quirk of 'sample.acaml'.

The parsing error is:

Traceback (most recent call last): File "/Users/jonathan/mres_thesis/code_tools/testing_for_unique_hash_keys/testing_for_unique_hash_keys/testing_for_unique_hash_keys.py", line 37, in testing_for_unique_hash_keys id = extract_sequence_metadata(filepath, xpath_dict) \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ File "/Users/jonathan/mres_thesis/code_tools/testing_for_unique_hash_keys/testing_for_unique_hash_keys/testing_for_unique_hash_keys.py", line 84, in extract_sequence_metadata tree = etree.parse(filepath) \# create the etree object \^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^\^ File "src/lxml/etree.pyx", line 3541, in lxml.etree.parse File "src/lxml/parser.pxi", line 1879, in lxml.etree.\_parseDocument File "src/lxml/parser.pxi", line 1905, in lxml.etree.\_parseDocumentFromURL File "src/lxml/parser.pxi", line 1808, in lxml.etree.\_parseDocFromFile File "src/lxml/parser.pxi", line 1180, in lxml.etree.\_BaseParser.\_parseDocFromFile File "src/lxml/parser.pxi", line 618, in lxml.etree.\_ParserContext.\_handleParseResultDoc File "src/lxml/parser.pxi", line 728, in lxml.etree.\_handleParseResult File "src/lxml/parser.pxi", line 657, in lxml.etree.\_raiseParseError File "/Users/jonathan/0_jono_data/mres_data_library/raw_uv/029.D/sample.acaml", line 19018 lxml.etree.XMLSyntaxError: xmlParseEntityRef: no name, line 19018, column 33 None

In these files there is a data analysis component, specifically an integration. It appears that at 19018:33 there is an ampersand in the string that is causing an error. The XPath for this location is: "/ACAML/Doc/Content/Injections/Result/SignalResult\[9\]/Peak\[41\]/Symmetry/@val".

Removing that ampersand results in a new error:

lxml.etree.XMLSyntaxError: PCDATA invalid Char value 1, line 19028, column 3

At this location is some sort of whitespace error. There are 11 of these speread throughout the file.

This fustratingly means that I cannot use 'sample.acaml' without editing the file.

One solution could be to parse only a part of the file. The invalid characters are all 10000+ lines down, but the target element is at line 150.

Note: in sample.acaml, the description element is at line 195, XPath: "/ACAML/Doc/Content/SampleParams/IdentParam/Description"

According to ChatGPT, the character is a control character that is invalid in XML 1.0, which presumably is what lxml uses.

So we're gna have to write a specific function to clean the file and then parse it. The XPath is probably different as well -

sample.acaml: /ACAML/Doc/Content/SampleContextParams/IdentParam/Name sequence.acam\_: /ACAML/Doc/Content/SampleContextParams/IdentParam/Name

2023-06-26 23:02:37

I've first rewritten extract_sequence_metadata (now called extract_id) to apply the extraction process directly upon root object rather than filepath, allowing me to clean files if necessary, or read directly from the file.

Unfortunately the preprocessing did not help, because the file appears to be irredeemable. I have instead opted for manually editing the file to remove the offending section, that is the section starting with the element:

<Peak xsi:type="PeakType" id="dbdc8d7c-c674-41a9-8dff-2e74532877e3">

in /029.D/sample.acaml, that element is on line 19008 to 19114. in /2023-02-23_2021-DEBORTOLI-CABERNET-MERLOT_AVANTOR.D/sample.acaml the error started on a different line.

Manually removing the SOH characters and repairing the broken xml code enabled me to successfully parse the file.

This could be a potential problem for parsing further single samples, but in the meantime we'll move on.

## 2023-06-27

### Integrating The ID Extractor into `rainbow.agilent.chemstation`

2023-06-27 09:37:10

To achieve the integration, I need to more clearly understand the logic of the two programs and then see where I can merge them. They are both a bit messy atm. It would be useful to isolate the core logic into their own modules.

2023-06-27 09:54:25

Have reverted rainbow.agilent.chemstation to its state prior to my previous updates, have extracted it into its own module, added more comments and successfully tested its reintegratoin into rainbow.

From the id work there are three changes to be made:

-   [x] add id to xpath_dict

It might not be as complicated as it seems. chemstation already has specific logic based on which files it encounters. considering I couldnt automate the cleaning of sample.acaml anyway, all i need to actually add to the core module is id handling. Right. That's done, but because the id value is not stored in the same way as the other target values (its an attribute not text), it needs a specific logic.

2023-06-27 10:06:00

That's done. Looks good for sample 138.D. Now to test it on the full dataset. At this point I might as well do it through my ChemstationProcessor.

### Parsing Error Due to Bad Filepath

2023-06-27 10:09:28

Running test_chemstation.py in its current state results in a number of errors of the form:

> File "/Users/jonathan/mycode/python_code/rainbow/rainbow/agilent/ext_seq_metadata.py", line 22, in extract_sequence_metadata tree = etree.parse(filepath) \# create the etree object

> Error reading file '/Users/jonathan/0_jono_data/mres_data_library/raw_uv/2023-02-22_CRAWFORD-CAB_02-21.D/sequence.acaml': failed to load external entity "/Users/jonathan/0_jono_data/mres_data_library/raw_uv/2023-02-22_CRAWFORD-CAB_02-21.D/sequence.acaml"

Solutions on stackoverflow to this problem suggest that the program is not passing a legitimate file to `etree.parse()`, causing the error.

I've added `assert os.path.exists()` which has shown a number of assertion errors.

2023-06-27 10:22:32

Got it, simple typo in the 'if sample.acaml' block in `chemstation.parse_metadata`. This is what happens when you have duplicate code blocks..

ChemstationProcessor initialization test has passed.

Initial tests on metadata df look good, but itd be nice to have a visual glance at the data. Lets output to a spreadshee

2023-06-27 10:38:33

Ok, current changes pass the metadata_df test.

Now to remove my generated id and use that one instead. Simplest integration will be to remove my generator and rename 'id' 'hash_key

2023-06-27 10:45:20

ALTER TABLE your_table ADD COLUMN standard_date TIMESTAMP;

That's done too. primary key gen has been moved to \~/mycode/python_code/, 'id' has replaced 'hash' key in all core files.

Now, what was I doing..? I was verifying the samples that I had. As of 2023-06-27 11:06:08 I have rebuilt the database.

What I was doing was testing the cleaning functions, specifically the chemstation cleaner, when I encountered the duplicate ids.

### Identifying A need to Standardise Database Input Functions

2023-06-27 11:18:17

While rewriting the dbs, I found that the sampletracker db process differs from the chemstation one, and that they all have slightly different methods. Pain in the ass, but I can't spend an hour rectifying this. Maybe another time..

TODO:

-   [x] standardize db interfaces in ch, st, count.

2023-06-27 11:20:23

Ok, its 11:20, the place is a mess and you haven't showered in days. Take a quick break, clean up, get a fresh perspective. By the end of the day we need to make a clear decision on sample identities, and finish all preprocessing. Extracting the description field would be useful though, Im sure I was writing wine names there as well..

TODO:

-   [x] finish sanding the floor.
-   [x] Add description element to `rainbow.agilent.chemstation`.
-   [x] process unprocessed sequences - dont rename the files just move them into 'data library'
-   [x] identify which wine 116 is
-   [x] enter all wines into cellartracker
-   [x] finish testing preprocessing functions.

### Adding Sequence and Sample Description Fields to Extracted Metadata

2023-06-27 11:58:12

There are two relevent description elements - sequence description and sample description. Ideally I can find a file that contains both.

"./mres_data_library/raw_uv/094.D/sequence.acam\_" Description element 4 at 196 has both a id Name element and a wine name description and the wine is 21 le macchiole bolgheri rosso.

I cannot find a file matching the criteria of having both a sequence and sample description, so we'll use two files for the test. "\~/0_jono_data/mres_data_library/raw_uv/094.D/sequence.acam\_" with the sample description, and "\~/0_jono_data/mres_data_library/cuprac/139.D/sequence.acam\_" with the sequence description.

2023-06-27 12:12:40

Test environment is setup in "/Users/jonathan/mycode/python_code/rainbow/tests/jonathan_dev_tests/jonathan_dev_tests_main.py".

2023-06-27 12:15:20

| idx | path                                                          |
|-----|---------------------------------------------------------------|
| 1   | /ACAML/Doc/DocInfo/Description                                |
| 2   | /ACAML/Doc/Content/SampleContextParams/IdentParam/Description |
| 3   | /ACAML/Doc/Content/SampleParams/IdentParam/Description        |

... so we want one and two.

2023-06-27 12:31:58

Extraction of both description elements confirmed. Need to test on a sample.acaml though, see what happens.

2023-06-27 12:37:32

Confirmed to work on single samples (if the sample.acaml file isnt corrupted), specifically "/Users/jonathan/0_jono_data/raw_uv/2023-02-22_KOERNER-NELLUCIO-02-21.D".

This is a great success. The established xpath workflow makes it really easy to extract as much metadata as I desire.

### Identifying 116

2023-06-27 13:39:53

Investigating 116 with the new metadata leds me to the conclusion that the Sigurd Chenin Blanc was either not run, or the file has been lost somewhere. The 116 in chemstation metadata corresponds to the second 116 in sampletracker, marco de bartoli cattarato, both in its run date - 2023-05-18 20:58:22, and sample block, CUPRAC_WINES 2023-05-18 16-48-18. I have marked it as such in the sample_tracker sheet and removed the 116 identification.

There is no need for me to manually rename the datadirs for the files now that I have full XML metadata extraction.

### Finding "Missing" Sequences

I currently dont have any samples \>139 in my library. They either havent been copied over or they've been misplaced. To locate them I've been using the following command:

``` bash
find . -type f -path "*.D/sequence.acam_" -exec grep -l "<Name>140 " {} \;
```

Which has shown to work for ID 139.

Running this in 0_jono_data has uncovered no files. Nor in KEYCHAIN. 179 has turned up in "./0_jono_data_2023-05-31/0_jono_data/0_2023-04-12_wine-deg-study/cuprac/ambient/2023-05-30/CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51/024-1401.D/sequence.acam\_"c

166 - 179 have been found as part of the "CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51" sequence. So missing 139 - 165.

2023-06-27 14:17:07

"CUPRAC_WINES_2023-05-30 2023-05-30 18-01-51" added to db.

2023-06-27 15:36:2023-06-27 15:36:05

Remaining Sequence "CUPRAC_WINES_2023-05-22 2023-05-22 10-24-28" as well as "CUPRAC_WINE-DEG_FREEZER 2023-05-30 11-02-39" and "CUPRAC_WINE-DEG_NFREEZER 2023-05-30 13-21-05"

have been added to the local 0_jono_data, and the cuprac sequence has been added to the db. There are now 170 samples in total in the chemstation metadata table.

I now need to clean the primary key columns to be able to achieve joins, and add the remaining wines to cellartracker.

TODO:

-   [x] finish writing chemstation cleaning tests
-   [x] finish adding wines to cellartracker

## 2023-06-28

2023-06-28 08:41:38

Most wines are added to cellartracker, there are one or two that I need to double check vintages of, or cant identify:

| id  | vintage | name                                    |
|-----|---------|-----------------------------------------|
| 77  | to fill |                                         |
| 78  | to fill |                                         |
| 109 | 2019    | rr (?)                                  |
| 112 |         | clembush                                |
| 113 |         | leflaive macon-verze blanc le monte     |
| 127 | 2021    | cantina orsogana                        |
| 143 | nv      | Totti's Vino Rosso                      |
| 146 | nv      | Merivale White Semillon Sauvignon Blanc |
| 157 | 2021    | piagre gampania bianco                  |
| 179 |         | mystery "barret rady"                   |

The totti's wines I can probably enter but I cant be bothered, their metadata is on the totti's menu.

TODO:

-   [ ] check vintage of:
    -   [ ] clemens busch troken riesling
    -   [ ] leflaive macon-verze blanc le monte
-   [ ] check with column about the identity of the 'cantina orsogana' and 'piagre gampania bianco'
-   [ ] add tottis red and white into cellartracker.

But in the meantime we can expect 10 wines to not have any cellartracker rows post-join, but we can expect them to have rows post-join of chemstation + sampletracker.

on to finishing the cleaner.

2023-06-28 09:17:01

As the new hash_key extracted from `.acaml` is called 'id', we will now refer to it as such, and replace what was 'id' with 'samplecode'. so 'notebook' to 'samplecode'

``` sql
ALTER TABLE chemstation_metadata ADD COLUMN standard_date TIMESTAMP; UPDATE chemstation_metadata SET standard_date = STRPTIME(date, '%d-%b-%y, %H:%M:%S');
```

2023-06-28 11:47:40

As I am testing my ch_m_cleaner, I am finding a number of samples for which a number of reasons I want to preserve the data in sample_tracker, but dont exist in ch_m, for example sample 'babo-chianti', which aborted. Because of the abortion, no .UV file was created, and thus was not detected by ChemstationProcessor.

Actually, no point keeping it. has been removed from sample_tracker.

2023-06-28 12:22:18

Found sigurd chenin blanc and 2 other 'missing' samples in a wine deg raw_uv folder named '2023-05-08'. Have introduced them into the data_library, but it raises the sticky question of how to handle the doubled up 116 samplecode again. I've long since given up on having a clean serial samplecode, so let's leave it as it is in the raw table, and modify it to "sigurdcb" in the cleaner and sample_tracker.

## 2023-06-29

### Missing samples

As of 2023-06-29 00:42:04, the following samples are missing from my dataset:

{'134', '104', '70', '103', '71', '77', '00', '31', '03', '02', '01', '105', '69', '14', '28', '78'}

Now, anything below 10 - 15 I think were on the wrong column/the sequence failed, but id have to double check, but a few of the missing ones are odd, and I suspect they are littered around the instrument hard drive in locations they should not be.

Anyway, I'm calling it there. Cleaning is done, ready to move on to the joins. If those files turn up, so be it.

### Finalizing Table Joins

2023-06-29 09:02:53

CH_M cleaning is done, now on to join validation. Simplest method of validating the join is that (for a left join) the number of rows of the resulting table should equal the number of rows of the left table, and the number of columns should equal the sum of the number of columns of the input tables. testing this should be straight forward.

Still need to get the cleaned tables into the db though.

2023-06-29 10:43:03

ch_m_cleaner established as a class with almost fully internal function except for export using Exporter class.

In the interest of speed I am forgoing writing formal tests for the cleaner to db processes, but I will definitely have to spend some time revisitng my test suite once some results are found.

Cleaners: - \[x\] ch_m - \[x\] st - \[x\] ct

2023-06-29 13:49:39

cleaners are done, they have been integrated into build_library, and pytest-style build_library test has been written. It will need to be automated in the future, but I have visually observed that it is working.

Now lets get the joins done. Build them as modules.

I have considered constructing one or more lookup tables however on reflection I'm not sure whether they are suitable, as we've essentially got a number of sequential left joins. First things first, let's revisit the sampletracker-cellartracker join.

Now that the 'preprocessing' stage of the project is done, we should consider refactoring the project to obscure those sub-packages from the greater whole.

2023-06-29 14:06:23

I'd forgotten that the st-ct join depends on a fuzzy match. One which will absolutely need internal testing. Let's have a looking at "cellartracker_fuzzy_join.py".

How to set up a test?

-   Internal test criteria should be that all lines are matched with x accuracy, i.e. 80% or something.
-   external test criteria should compare the starting and ending geometry of the tables

2023-06-29 14:15:03

There is no reason why this code is 'cellartracker' specific. Should spend a mo generalizing it, turn it into a class. Itd be great if some level of abstraction could be achieved for a generalized 'interact with db' super class..

## 2023-06-30

2023-06-30 00:02:28

There are a number of steps required to proceed with the fzjoin.. The first of which is to check whether we actually need to bother with it.

### SQL Inner Joins

``` sql
SELECT a1, a2, b1, b2
FROM A
INNER JOIN B on B.f = A.f;
```

## 2023-07-01

### DuckDB and SQLAlchemy

2023-07-01 16:15:45

[SQLAlchemy](https://docs.sqlalchemy.org/en/20/) is a Python toolkit for general purpose interaction with SQL databases. Rather than building my own tools I should utilize industry standards like this.

DuckDB can be integrated with SQLAlchemy through the [`duckdb-engine`](https://pypi.org/project/duckdb-engine/#the-name) package.

1.  install `duckdb`
2.  install `duckdb_engine`
3.  install `sqlalchemy`

I'll do that now.

And now I'll test SQLAlchemy

## 2023-07-02

2023-07-02 23:45:58

DuckDB doesnt play nice with SQLAlchemy, so i've retreated back to the duckdb API, which is proving richer than I originally understood.

## 2023-07-03

2023-07-03 12:36:15

all joins have been achieved. Moving on to EDA. \$\$ todo:

-   [ ] go back and formalize join modules, write tests, then build into pipeline
-   [ ] drop unnecessary columns from cleaned tables prior to joins (reduce noise).

## 2023-07-07

2023-07-07 08:34:03

I am trying to produce some descriptions of the dataset to send to andrew, but it appears that python is not the best at this kind of thing.

Keeping it simple - for each detection type, produce the following:

-   [x] total counts
-   [x] unique rwines in dataset
-   [x] duplicated wines in dataset
-   [x] non-duplicated wines in dataset

## 2023-07-10

2023-07-10 14:55:38

Nature has a standard figure width of 89 mm for single column and 183 mm for double column.

## 2023-07-11

2023-07-11 09:39:32

Time to finish off the report.

-   [x] finish reorganizing data by wine category
-   [x] write descriptive paragraphs for each seperated by detection type.
-   [x] adjust formatting of pages to try and contain as much in a page as possible.

Data reorganization: - \[ \] Overall descriptions of each dataset - \[ \] variety - \[ \] type - \[ \] country

Varietal section is made up of:

1.  Grouped bar plot comparing the most represented varieties by each dataset
2.  tables of that bar plot, probably seperate.
3.  tables of varieties with less representation, again probably divided by detection type.

To make the bar plot need to concatenate the variety dfs before filtering

### MATPLOTLIB/SEABORN

[mwaskom on stack overflow](https://stackoverflow.com/a/23973562/21058408) provides a clear explanation of the logic behind seaborn OOP-able and figure level functions.

summary - most functions take an ax object, so best practice is to create one first then pass it to the plotting function.

``` python
import matplotlib.pyplot as plt
import seaborn as sns

# the straight forward method of creating a figure and axes class object:
fig = plt.figure()
ax = fig.add_axes
sns.barplot(x, y, ax=ax1)

# However there is a subplots wrapper that simplifies the creation of both single and 
# subplotted figure objects and associated axes

f, ax1 = plt.subplots(1)

# can also provide nrows, ncols arguments, creating a comparable list of axes objects 
# a[n], or a[n][m]

f, (ax1, ax2) = plt.subplots()
sns.barplot(x, y, ax1)
sns.lineplot(x, y, ax2)
```

An oddiy of matplotlib in jupyter notebooks is that because jupyter tries to plot/output whatever is in the cell, and most of the axes and figure methods return plotable objects,

## 2023-07-16

### HPLC Spectrometry Data Processing For PCA

2023-07-16 20:32:57

[Beltrame et al.](zotero://select/items/2540beltrame_QUANTITATIVEVALIDATEDMETHOD_2016) indicate that they are able to feed the continuous signals into the PCA model without quantizing the data.

Conclusion - Seemingly difficult to find any papers using PCA in a similar manner. Maybe just go for it.

1.  assemble the data.
2.  scale the data
3.  fit the model.
4.  visualise.

### 1. Assemble the data

The data should be organised in wide form where each column is a wines intensity array. The time axis is not necessary.

### July Update

Its been a while since an update was made. After finalizing my pipeline, I dove headfirst into producing an EDA report of the dataset using Quarto as the document producing engine. This was a task for which i was woefully ill-equiped, and had to learn the following to produce a viable analysis and document:

-   method chaining in pandas
-   advanced data structure manipulation techniques in pandas including groupbys, multiindexing, assign, transform, and when to use each.
-   lambda functions in python
-   pivot tables
-   advanced matplotlib formatting (still havent fully gotten that)
-   stylesheets in matplotlib
-   seaborn
-   Quarto
-   LaTeX
-   pandoc
-   Pandas Style class.

The biggest pain was the lack of clear documentation and examples in Quarto for this use-case. Also ChatGPT is either getting worse or the technology is too new for it. Because Quarto depends on Pandoc which dependso n xlatex downstream, documentation is spread across three different technologies, and limited by using Jupyter vs. qmd.

But we got there in the end, and the skills I have learnt during this exercise will benefit me throughout the rest of the project.

One thing to come out of this is that I will be producing the thesis draft entirely in qmd as a project with chapters in 1+ qmd files, allowing Quarto to bind them together into the greater document. This will save writing time down the track, and my progress reports will also be my chapters.

The missing pre-commit bug is really messing with my workflow, it would be beneficial to fix asap. 2023-07-23 12:08:02 update - fixed it, was an artifact of when I didnt realise that a local .venv had been accidentally created and was screwing up all the paths. Deleting the `.git/hooks` dir, uninstalling and reinstalling pre-commit fixed it.

I really need to start joining communities working in the same space (or similar) as me. The time invested would help ease a lot of uncertainty and decision paralysis.

Anyway, the next step is to develop a PCA pipeline. it will be [here](../src/wine_analysis_hplc_uv/modeling/pca.py) with its test module [here](tests/test_modeling/test_pca/test_pca.py)

## 2023-07-23

2023-07-23 13:18:14

To automate testing I need to modify all existing tests to pytest. This is a one day job that can wait until after i get some preliminary results. In the mean time i will move them to 'old_tests' and make all future tests in the pytest manner.

## 2023-07-24

2023-07-24 09:29:21

Have been developing a pipe to get the datasets back into memory as dfs for further operations, but have run into a bottleneck. Converting from the db back into DF is very slow. 167 samples is 164 million rows and takes 16 minutes to complete (weird symmetry there). There needs to be a better method.

Primarily I trying to avoid iterative processes, loops etc. Hence why I chose a db solution in the first place.

To test: 1. conversion to different data formats, pl, etc. see what options there are. 2. extract wines one by one through iterative process - both long and pivot in db before conversion 3. swap to a different db system (this is more work, will req. generalizing a lot of code)

Lets try polars first..

2023-07-24 10:54:56

Polars got it down to 4 mins, polars to pandas takes 0.14s. Lets go with that. In fact, lets learn some polars.

but we need to finish the report and send it to andrew. Done.

2023-07-24 11:21:58

On to PCA.

To get the data in a suitable format for PCA we need to select 1 wavelength then pivot the table so wines are columns, absorbances are values, then slice it so all columns are same length. Once this done, move on to looking into how to apply the model.

2023-07-24 12:13:49

Getting into polars is going fairly smoothly, but ive been having issues with the pivot. Some wines have random null sequences, start and finish with null, etc. on visualisation, it is apparent that one of two things are happening:

1.  during the pivot, the wines are not being aligned, are being broadcast weirdly, or some other anomolous behavior.
2.  the duplicate wine names in the super_tbl are throwing it off somehow.

Its probably 1 though. I should swap back to pandas and see if the behavior is different.

12:18 - testing with pandas results in the same. its an abberation of the sql query. Presumably using the wine name is not pulling the first encountered, but ALL of them. This raises another point - i really should be selecting a specific dataset, ideally cuprac, and we need to subset the tables based on unique ids not wine name.

Current process is as follows:

1.  get a random sample of wine names from super table wine column
2.  samples are selected in this clause:

``` python
    wine_clause = f"WHERE super_rel.wine IN {tuple(wines)}"
```

So we need to change this to id. The way to this to still be human readable will be to get id in the first query and pass that through. sample ids rather than wine names, as the ids will be unique in that table.

2023-07-24 16:03:44

So it had nothing to do with how i was joining the tables and everything to do with the fact that to pivot you need a common index. That has been achieved in pandas. Down the track i would like to explore how i can achieve this in both duckdb and polars.

Now that the raw data is good to go, its time to proceed to modeling.

How to do this?

2023-07-24 19:56:17

Rudimentary PCA model pipeline is written, but i have no idea how to interpret it.

I have refactored lib_eda into a qmd project titled 'thesis' found at ~`~/Users/jonathan/mres_thesis/thesis`~ (*UPDATE*: 2023-09-26 15:32:04 this filepath is defunct, thesis is located at /Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notebooks/thesis.qmd) thi which is also a git repo and python project. this will the the central hub for the thesis (and possibly any papers) and general notes. This format excites me because it reduces the overhead and double-handling of information. Notes can be written directly into the qmd files which are then automatically rendered. I just need to configure a .bib and we're good to go.

Its tempting then to move all my project notes to this location as there is something much more satisfying about being able to see the final product as you are writing it. I'll start with PCA related notes, I can develop that note while I research how to interpret my PCA biplot.

Speaking of the biplot, I had an epiphany over dinner, an obvious one really. the sample vectors are grouped by similarity, ergo whites are distant from reds etc. Thus for a simple classification application, the proximity of a sample in question to labelled samples would result in a classification.

2023-07-24 21:23:59

Pulling random samples of the dataset has unveiled the concerning fact that a number of the cuprac samples are failed runs. I need to start verifying them. The best way to do this will be be to add a 'verify' column to super_tbl. But this gets me further and further away from my pipeline. So I first need to look at ratifying the pipeline with current manipulations. For example, super_tbl should not exist, I should be writing sub-queries, or a view or something. Time to dust off the pipe.

Anyway, I've instantiated a module [hereTitle](../wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/modeling/validating_data.py) to contain the data validation methods. Ill need to be able to query specific wines by name. I know i developed the sc_plot app, but building web apps has too much overhead when I can just run it in an interactive window.

2023-07-24 22:39:26

Because I made the modifications to super_tbl rather than ct_tbl, its going to be too difficult to bother to try and form a state comparison. I'll just use the code itself as the barometer. We know we need to strip punctuation (read single quotes, apostrophes) from wine names,

## 2023-07-25

2023-07-25 06:42:12

Observing the difficulty of tracking state changes in my database has motivated me to modify how I handle the 'super table'. It should not be a table recorded in memory, and any 'super table' attributes should instead be made available as the result of joins or instantiated as a view. The idea is to avoid repeated data and to provide a clear path from start to finish that can be fully automated.

2023-07-25 09:00:31

Rewriting the queries has been complicated by the fact that sampletracker-cellartracker join is formed by the fuzzy join. One possibility is to write the join_key column back into sampletracker once a join is achieved, then future joins are on that column.

So it goes:

fuzzy join, join_key back to sampletracker.

2023-07-25 10:15:29

im at a loss as to how to develop this code without tests i can rely on. goddamn. So i definitely need build library tests. but i also need a clear end goal. What i need is to redo build library so its primarily in sql. I also need to develop tests for it. How do you write pytest tests for sql operations? Lets create a copy db file then play there.

Note: dont use views, use `CREATE TEMP TABLE`

Unit testing DB with pytest: - use `con.rollback()` to reverse any changes - define a class that contains the setup and teardown methods/objects and validation data - setup the class to mimic the structure of the data working with - setup the db within the class in memory - define fail cases as well i.e. NULL

project logging:

-   each module logger should be instantiated with: `logger=logging.getLogger(__name__)` [^2]

[^2]: https://engineeringfordatascience.com/posts/python_logging/

Basic set up for cross module log use in tests:

``` python
import logging
logging.basicConfig() # 1
logger = logging.getLogger(__name__) # 2
logger.setLevel("INFO") # 3
from wine_analysis_hplc_uv.chemstation import logger as chemlogger # 4
chemlogger.setLevel("INFO") # 5
```

1.  To use logging effectively at all you need logging.basicConfig(). sets up handlers and stuff. doesnt work without it
2.  to instantate a logger for the current module. best practice is to set it to **name** to be the same as python import statments, see 4.
3.  set the level globally, "INFO" for just "INFO", "DEBUG" for all.
4.  to get messages from other modules during run time need to import the logger from that module
5.  to control the message level of module level loggers need to set their level as well.

pytest cli and log output:

Pytest does not make it easy to output stdout print or logs. This is because they dont scale well and the assumption is that you are building automated tests (i guess?), but they are very useful for test development/debugging.

To enable output of logs during runtime, place the following in your pyproject.toml:

``` toml
[tool.pytest.ini_options]
log_cli = true
log_cli_level = "INFO"
```

For reference pytest starts at WARNING for standard capture. by default, logs are only printed if a test fails.

To output print statements, use the `-s` flag on CLI. You are supposed to be able to pass CLI flags to the automated testing system in vscode in settings.json but i havent gotten it working yet.

2023-07-25 21:23:35

Now that I have a working test framework I need to move back to the task at hand. I think the first approach will be to..

-   [x] add removing single quotes and other illegal characters from cellartracker.name
-   [x] add a column 'wine' to cellartracker

## 2023-07-26

2023-07-26 10:33:37

Chemstation and CT pipe tests have been successfully rewritten to work with pytest. Now i can apply the additions mentioned above and test the whole library at the same time.

2023-07-26 11:16:06

Now that the mentioned features have been added to the cellar tracker pipe, i need to move back to modifying the sc_tbl pipe to include a super_tbl-esque subquery.

But first I need to do a fresh build_library run, and write some tests for it..

Could write a scaffold that initially works from physical memory (to speed development) but eventually will operate in virtual for the duration of the test.

2023-07-26 14:32:31

WHen i get back to it, I was writing a test for chemstation cleaner class definition as the full implementation is currently untested, only the individual units.

## 2023-07-31

2023-07-31 11:47:09

Build library pipe has been verified to work, tests have been written. TBH though the test does not have any assertions as its somewhat complicated to define it, currently im relying on pl.DataFrame.describe() to visually identify abberations in the descriptive statistics of the tables.

The next step is to integrate a foreign key into sample_tracker as the result of the fuzzy join between sample_tracker and cellar_tracker.

1.  fuzzy join sample_tracker and cellar_tracker.
2.  join the fuzzy join table 'join_key' to c_sample_tracker'
3.  c_sample_tracker to db.

define a test as: 1. shape of the new c_sample_tracker = (m,n+1) of original sample_tracker 2. join on the join key with c_cellar_tracker results in a valid tbl of expected shape.

## 2023-08-01

2023-08-01 11:49:18

While developing the 'super table' pipe as a assemblage of sql queries, i've discovered that the final join has missing wine, color, and varietal values. 12001/289225=0.04, 4 percent of 66 wines.. something like 2 - 3 wines? Presumably the 'added_to_cellartracker' column is not totally accurate.

First, if we look at changing the st-ct join from left to inner.. that results in two wines that are not in cellar_tracker but are marked as such. Now the first insight here is that we can probably do away with the 'added to cellartracker' subquery. Secondly, I would like to identify the missing wines.

On inspection with the following query:

``` --sql
SELECT
st.samplecode, st.vintage, st.name, st.ct_wine_name
FROM
c_sample_tracker st
ANTI JOIN
c_cellar_tracker ct
ON
st.ct_wine_name=ct.wine
WHERE
st.added_to_cellartracker='y'
```

It appears that all of the nv wines do not have an entry in `ct_wine_name`, and thus cannot join and thus end up with missing values for those cellar_tracker columns. I suspect this is yet again because of how pandas handles null values, and when I am forming `ct_wine_name` there are nulls in the vintage column, resulting in a failed string concatenation. The simplest solution is to remove the 'wine' generation lines from the ct and st pipes. But at the moment the nv wines are but a curio, and I'm gna leave them out of further studies.

In fact, I should filter for red wines only during further method development.

TODO:

-   [ ] fix up non-vintage name formation and vintage handling.

2023-08-01 13:26:03

Pivoting `wine_data` has resulted in a jagged join because 5 wines have 8999 rows prior to the pivot. Considering I selected 0 - 30 mins and 450 wavelength I dont understand how this can be.. duplicates.

This is because of duplicates. I fixed this by grouping by a concat of `samplecode`, `wine` to form the index col rather than just `wine`.

2023-08-01 14:30:50

So the wine data (née super-tbl) pipe has been rewritten as a 3 part query, currently in 3 different relation objects, but potentially can be combined as sub-queries. Keeping them seperate allows me to observe the states between joins.

Now it will be useful to be able to observe the chromatograms and PCA biplots at the same time, perhaps to develop an intuition for the relationship between the shape of the two.

2023-08-01 20:11:40

See `/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/modeling/investigating_dataset.ipynb`, have found that 7 CUPRAC samples have failed runs. Havent' checked, but i think we'll find that they were all run at 10uL injection volume, which was causing the krud catcher to get blocked mid run.

The wines are:

| \#  | id     | wine                                                    |
|-----|--------|---------------------------------------------------------|
| 0   | 128    | 2019 mount pleasant wines mount henry shiraz pinot noir |
| 1   | 161    | 2021 le juice fleurie fleurie gamay                     |
| 2   | 163    | 2015 yangarra estate shiraz mclaren vale                |
| 3   | 164    | 2015 yangarra estate old vine grenache                  |
| 4   | 165    | 2020 izway shiraz bruce                                 |
| 5   | ca0101 | 2021 yering station pinot noir                          |
| 6   | ca0301 | 2021 chris ringland shiraz                              |

I've filtered them out at `pivot_wine_data` at the moment but should probably add them to the st pipeline at some point.

Also, samplecode list has been added as a constant `BAD_CUPRAC_SAMPLES` in `definitions`.

TODO:

-   [x] remove the wines above from the dataset during ST pipe
-   [ ] maybe add a column in sample_tracker with 'include in dataset'.. also add notes describing the samples in sample_tracker.

## 2023-08-04

2023-08-04 16:09:28

EDA on the dataset has spured me to try and a) define an abstraction of the signals in order to find descriptive statistics techniques, b) find fields that work with similar signal to similar ends. Through this effort I realised that chromatograms are simply time series and can be treated as such, and that fields as diverse as financial forcasting and biosignal monitoring, and that there should be copious volumes of literature describing how to handle time series in any way I desire.

Secondly, a quick search using these terms has uncovered [*Standardization of chromatographic signals* by Cuadros-Rodríguez et al.](zotero://select/items/%2540cuadros-rodr%25C3%25ADguez_StandardizationChromatographicSignals_2021a).

TO DO:

-   Review the series:
    -   [x] part 1
    -   [x] part 2
-   Review:
    -   [x] R.H Jellema, A. Folch-Fortuny, M. MWB Hendriks, Variable shift and alignment, in: S.D. Brown, R. Tauler, B. Walczak (Eds), Comprehensive Chemometrics: Chemical and Biochemical Data Analysis, second edition, Elsevier B.V., Amsterdam, 2020, vol. 3, ch. 3.05, pp. 115--136. doi:10.1016/B978-0-12- 409547-2. 14886- 3. Topic: peak alignment.
    -   [ ] R. Smith, D. Ventura, J.T. Prince, LC-MS alignment in theory and practice: a comprehensive algorithmic review, Brief. Bioinform. 16 (2013) 104--107, doi:10. 1093/bib/bbt080. \[21\] P. Filzmoser, B. Wal. topic: peak alignment.

## 2023-08-07

2023-08-07 11:09:32

From @cuadros-rodríguez_StandardizationChromatographicSignals_2021 we have a generalized signal pre-processing pipeline:

1.  translation onto one time axis using a spline function.
2.  resampling to 'select and truncate'.
3.  smoothing and denoising with a savitzky-golay filter.
4.  baseline correction.
5.  intensity normalization via scaling to the internal standard maxima.
6.  alignment.
7.  second resampling.

Now we should justify all of these steps with further research, but it looks more or less correct. Of question will be why smoothing, denoising and baseline-correction are useful considering that you do lost information.

2023-08-07 22:44:59

Reading @brown_2020c has inspired me to write the first chapter of my thesis / a paper - comparing peak alignment methods, primarily COW, PTW, PAFFT, and RAFFT. We could also compare raw uv/vis and CUPRAC with these techniques. To work up the data we need to do the following:

1.  time axis interpolation (need to clarify why this is necessary)
2.  resampling (clarify what this is)
3.  savitzky-golay filtering to smooth and denoise.
4.  baseline correction through a PLS derivative.
5.  intensity normalization on maxima in dataset.
6.  alignment.

We'll need to clarify which similarity metric is best, but probably go with Euclidean Distance.

So overall, the task is as follows:

1.  identify a CUPRAC and raw sample representive set, both red wines, 5 each.
2.  determine how best data structure to handle the signals assuming that we're gna have to work with 100+ wines.
3.  Apply the preprocessing steps specified above.
    1.  Calculate similarities at every stage.
4.  Identify reference signal - play with different solutions i.e. the sample with highest mean correlation, an aggregate sample made up from all included signals.
5.  align on reference signal
6.  calculate similarities again.

Thats it. Thats a chapter done. Once its done..

### Establishing a Preprocessing Pipeline

2023-08-07 23:53:04

A cursory search for tensor data structures suitable for custom preprocessing alogrithms has turned up nothing.

The following are potential options:

-   [keras](https://www.tensorflow.org/guide/keras/preprocessing_layers)
-   [mne](https://mne.tools/dev/auto_tutorials/preprocessing/index.html)

2023-08-08 00:27:29

A discussion with Serious Stel on The Python data-science-and-ai discord channel led me to the underestanding that at least initially, a multiindexed pandas dataframe would be the first-base solution for producing a processing pipeline. This is a good chance for me to practice with multiindexes as well. Lets work on that, but then compare the computational speed of both a multiindexed structure, and a series of dataframes, and see who has the greater computational cost.

If the cost is too high, we can check out polars or tensorflow.

## 2023-08-08

2023-08-08 00:33:59

The previous preprocessing approaches can be found at `/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/signal_processing`. At some point I should collect my notes from the development of this module, and document the code therein. But first Im going to establish the data structures of the new sample sets. The module can be found [here](../wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/signal_processing/form_frames.py)..

2023-08-08 00:50:48

First off ill need to modify `get_wine_data` to be more generalized as it is currently specific to CUPRAC reds. I guess ill have to write test functions for the different options as well.

To generalise `get_wine_data` in a usable manner, we'll probably need to concatenate the sql queries into a multi join query. As I do not know how to do this, we'll need to learn.

2023-08-08 08:38:34

To write a test for a generalized `get_wine_data`, i will need to create a class with all of the attributes to be fed to the test.

Condensing the query will make it easier to and modify. Do that first.

2023-08-08 09:11:07

The module is a bit of an odd one because its structured somewhat like 5 nested functions within one. Probably best to first move all ofthe comments to the top of the function..

2023-08-08 10:48:13

The mulitjoin is formed and has been shown to work. Now i'll piece by piece form optional parameters to generalise the python wrapper function. The parameters are as follows:

-   detection
-   samplecode
-   wine
-   color
-   varietal
-   mins
-   wavelength

We'll have to write tests for every combination.

2023-08-08 14:55:40

This has proven harder than expected. I think ultimately we're gna have to revisit how we're storing the data. It appears that both my original temporary table and refactored multijoin query encounter a memory error when trying to convert the joined data table to a df. Both a pandas or polars df.

2023-08-08 15:14:16

The temp table approach takes 5.4542 seconds to complete a cuprac red 450nm query.

I should swap back to a multijoin and see the performance. I assume itll be v similar. Actually, some sort of stress test would be a better benchmark.

2023-08-08 15:42:00

Ok so it turns out its the introduction to Python memory that causes the bottleneck/crash. the formation of the wine_data table is still instantaneous.

Converting to the multijoin single statement halves the query time for cuprac reds at 450nm from 0.89 seconds to 0.46 seconds. so it definitely wasnt that. It becomes apparent that the best way to test the query will be to write the tests in sql, or extract geometric descriptions through metadata functions, rather than converting to a dataframe.

## 2023-08-09

### Optimizing get_data

2023-08-09 16:05:29

THe UNNEST logic i built to enable optional parameters is resulting in 1 - 2 minute queries even with no values provided. To set a baseline, I will run a base query with no where clause and then build up the number of ANDS. 3 runs per increase to get a bare bones distribution.

| i   | numWHEREclause | run1  | run2 | run3    | mean  |
|-----|----------------|-------|------|---------|-------|
| 1   | 0              | 18.5  | 18.6 | 17.4    | 18.6  |
| 2   | 1              | 20.6  | 19.1 | 20.1    | 19.91 |
| 3   | 2              | 18.00 | 16.8 | 17.2    |       |
| 4   | 3              | 19.4  | 17.9 | 18.1    |       |
| 5   | 4              | 19.3  | 17.7 | 19.0    |       |
| 6   | 5              | 22.7  | 20.3 | 16.6171 |       |
| 7   | 6              | 19.44 | 19.5 | 18.3    |       |

And the minutes COALESCE clauses:

A single observation amounted to two minutes run time. so it turns out that it must be something to do with the greater than or equal two expression with NULL.

Reducing the observation to 1 wine under cuprac detection (2018 crawford river cabernets) sees 92 seconds of run time.

2023-08-09 22:14:38

Okok, changing the mins logic has resulted in a 400% increase in speed for a full query.

Now I want to develop a in DB pivot method.

But first I want to experiment with full dataset to different data formats, starting with polars.

full table to polars takes 329 seconds. full table to arrow takes 165 seconds. full table to parquet file takes 70 seconds, parquet -\> pandas takes 100 seconds

SO evidently to parquet is much faster.

Surprisingly, parquet -\> polars results in a memory crash. So going parquet -\> pandas is the best option.

Lets leave that for now and focus on pivoting db tables

## 2023-08-10

2023-08-10 07:34:07: have moved this project logs to this file as opposed to "mres_logbook.md". A Jonathan with more time than me can get busy moving relevant sections here.

*UPDATE*: The contents of that logbook file were standardized with this one then moved back here, was a failed experiment.

### Pivoting DB tables

I will developing a module/query to pivot db tables [here](src/wine_analysis_hplc_uv/db_methods/pivot_wine_data.py).

the following call produces an acceptable sample set:

``` python
get_data.get_wine_data(
        con,
        samplecode=('124', '130', '125', '133', '174'),
        wavelength=(450,),
        color=('red',) 
    )
```

i am working with a long 'wine_data' table with columns 'wine', 'mins', 'value'. each of the rows is an observation of a 'wine' sample with 'wine' as the primary key. 'mins' and 'value' are numerical columns. 'mins' always ascends from 0 to roughly 54.0, 'value' is variable and contains the intensity values of the instrument recording the signal.

I would like to add an 'obs_num' column to each sample in the long table based on a groupby of the 'wine' column where the 'obs_num' starts at min('mins') and ascends by 1 per row as 'mins' ascends, with the last 'obs_num' value at max('mins')

2023-08-10 09:28:37

To add row numbers to the long table to act as an index for the pivot:

``` sql
SELECT
  wine,
  mins,
  value,
  ROW_NUMBER() OVER (PARTITION BY wine ORDER BY mins) AS obs_num
  FROM wine_data;
```

Ok, so a pivot was achieved with `pivot_wine_data()` [here](src/wine_analysis_hplc_uv/db_methods/pivot_wine_data.py).

Its a single wavelength, and no minutes. Its too conceptually difficult to continue to persue this in SQL. In the interest of time, I will instead look at batch extraction, or other means of reducing the memory cost.

That being said, this is blazingly fast to return results and produce plots.

2023-08-10 10:51:23

Ok, now that I've established the basis of some tools for working with these large datasets I will go back to working on the preprocessing pipeline.

The wine set i will work on is the same one as above.

Fist step is to produce the dataframes in an appropriate format to work on. This has been achieved by modifying the `pivot_wine_data` function in pca module to produce a dataframe with a heirarchical index of ('wine', \['mins', 'val'\]).

2023-08-10 16:57:19

Getting there. With the help of user **Alex** ive got a method of pivoting the tables out in duckdb prior to moving to Python, making everything lightning fast. However, working with multiindexes is a goddamn pain, and i need to learn how to do that better. Also extracting 'obs_num' with the pivot would be v useful, but currently dont seem to be able to.

[pivot_wine_data](src/wine_analysis_hplc_uv/db_methods/pivot_wine_data.py) will need a big clean up before we can proceed really, but we're still testing it.

It currently looks like ive still got duplicates in the set somehow, specifically:

98 2020 barone ricasoli chianti classico rocca di ... mins 0 value 0

Which is forcing the dataset to be twice as long as it should be. 72 2021 de bortoli sacred hill cabernet merlot mins 5712.

## 2023-08-13

### Sequence Alignment

2023-08-13 12:19:38

The first stage of the preprocessing pipeline is sequence alignment, specifically Multiple Sequence Alignment (MSA). This is necessary because the chromatograph can have a variation in observation points. What is the variation in observation points?

db long table pivot wine data test: To debug the too long pivot table, try swapping `sample_code` out for `id`, a possibly more reliable primary key.

### Justification of need for MSA

For a time series $y = f(x)$, MSA aligns $x$ of all sequences determined to be related so that they all experience the same variation (none) from the input, and thus all output variation is contained within $f(x)$ unique to that signal.

From what I've seen so far (very little) it is a concern if observation frequencies differ, or start and stop at different times. In my dataset the observation time points differ minutely, and thus the question of whether it is necessary to undertake MSA can be raised. A first touch check of whether I need to could be to observe the point-by-point variance across the included time series. Frankly I'll need to do further research on both time series statistics and MSA.

TODO:

-   [x] identify issue with too long sample.
-   [x] write pivot table test
-   [ ] summarise @brown_2020c 3.05
-   [ ] annotate @brown_2020 3.06 (and make book section item to cite)
-   [ ] research Savitzky-Golay smoothing
-   [ ] research MSA
-   [ ] research time series statistics
-   [ ] restore use of peak alignment module with current data format

## 2023-08-15

### Too long pivot table exploration

Changing samplecode to id has not fixed it. Again, lets identify the culprit.

[This](solve_too_long_pwine_data.ipynb) is a jupyter notebook in which I will perform investigations to determine the cause of these errors.

2023-08-15 07:43:17 Solved - 98 was run on an older method resulting in different run parameters, 72 is an algamation of two samples, only one of which is still in sampletracker. This is layover from when I was validating the avantor column. 98 can be straight up dropped, not worth it to validate the method. 72 is slightly more difficult, but I think ill just drop it too. They have been removed by excluding them from the `get_data` query `AND ((SELECT UNNEST($samplecode)) IS NULL OR st.samplecode IN (SELECT * FROM UNNEST($samplecode))) AND st.samplecode NOT IN ('72','98')`. The [notebook](solve_too_long_pwine_data.ipynb) is now broken because the target samples are no longer retrieved by `get_data`, but if I found a way of making that exclusion optional then it could work again. Not worth the time to worry about atm though.

### Introducing Injection Volume Into the dataset

One factor I overlooked is that the injection of my CUPRAC samples differ, initially they were set to 10uL but after issues with precipitation it was dropped to 5uL. This will affect the heights of the samples as per beer-lambert law. So to adjust for injection volume I need to get the injection volume measurements from the .acaml file, add it to rainbow, and rerun the pipe. Its good to rerun it anyway, keeps it from going stale, so to speak.

2023-08-15 08:47:14: The injection volume can be found in .D/acq.macaml at the XPath `/ACAML/Doc/Content/MethodConfiguration/MethodDescription/Section/Section[3]/Section[4]/Parameter[5]/Value`.

2023-08-15 09:06:02": Problem - Injection volume is stored in `acq.macaml`, which is currently not parsed by rainbow. The secondary problem is that `parse_metadata` is setup to only read from one type of file, with a case-like setup of serial IF clauses each with their own return statement. My solution will be to add a `acq.macaml` parse as the first block within `parse_metadata`. This way we will avoid changing the logic.

2023-08-15 12:20:37: Got it. The generated xpath was incorrect, it should have read: `/MethodConfiguration/MethodDescription/Section/Section[1]/Section[2]/Parameter[2]/Value` Had to manually locate it by starting with a root xpath and iterating through the returned children, adding more to the path and iterating again until i reached my goal. I have added some documentation, a test based on previously generated 094.D metadata, have commited and pushed. Now I will generate a library in a new db and check the injection volume distribution. 2023-08-15 15:36:47: Testing the modifications on my dataset through chemstation tests (which have been updated as pytest format and added to tests dir) have revealed that the xpath developed from 094 is not universal. Fun. I will now try to develop one for 116.D

2023-08-15 17:28:17: Ok, for a set of 4 DD (data dirs .D) I have shown that half of them work with the 116.D xpath, and half with the 094.D. Since both 116.D and 094.D are in the sampleset.. thats not really saying much. Time to expand it to the full set.

## 2023-08-16

2023-08-16 13:10:20: Discovered that there were at least 5 different absolute xpaths to Injection Volume depending on the sample in question. Scrapped absolute paths for a relative approach based on `.findall()` and filtering by parameter. This approach has been verified on the whole dataset, tested and commited. The results of this study are [here](tests/testing_inj_vol.ipynb), awhere we found that no CUPRAC samples in the current dataset were collected with a 10uL injection volume. It was good to verify though. 2023-08-16 14:38:30: `build_library` has been validated, rerun with injection volume update. verified, changes have been commited. Back to.. whatever i was doing?

### MSA

#### MSA reading notes

| @listgarten_2004 \| A MSA study proposing a novel HMM model with an example of application on TIC LC-MS 2D signal dataset.\|

### Revitalizing the peak alignment pipe

[The peak alignment pipe](src/wine_analysis_hplc_uv/signal_processing/peak_alignment/peak_alignment_pipe.py) has been revitalized using a mock dataset shaped into the same structure as the pipe originally expected. tHis has been done to then rewrite the pipe to match the new multiindexed approach. While working on this though I started wondering about dataframe schema validation for pipes, a solution to the ever-present problem of how to ensure the input of a pipe is appropriate for it. Turns out many others have encountered the same problem, and that there many dataframe validation packages out there, including [pandera](https://pandera.readthedocs.io/en/stable/index.html). It looks promising, but perhaps overkill. Specifically, in order to handle a variable number and name of columns, they would need to be regex-matchable, as as [described here](https://pandera.readthedocs.io/en/stable/dataframe_schemas.html#column-regex-pattern-matching).

So I think we'll write a prefunctory validation function based on the properties of the multiindex. Specifically, we're talking about the dataframe resulting from the `pivot_wine_data` function.

## 2023-08-22

2023-08-22 11:42:11: lets peel it back even more than that. Just make sure the multiindex names are correct.

## 2023-08-23

2023-08-23 09:52:28: Multiindex dataframe validation has been established as `check_dataframe_props`, currently stored [here](tests/test_preprocessing/test_baseline_subtraction.py). It checks that the column index level names match expectation, and that for the vars level, the columns \['mins','values'\] are in the right order for X number of sample columns. There is also a list of TODO features to add to make it more specific.

2023-08-23 13:47:09:

The time has come to adapt the rest of the peak alignment pipeline, however, the first thing to do is to produce a sample dataset. Operations like the baseline calculation are very time-consuming, so decimation is key to a quick development cycle. Ideally it will be a process that will decimate from the baseline as much as possible while preserving all peaks. I should develop this in a jupyter notebook where I can also place my notes about decimation processes.

### EDA decisions

I have reached a point where I need to start keeping a formal track of decisions made during EDA. These will be kept the [README](README.md#eda-decisions), with links to the associated notebook justifying the decision.

### Time axis offset

I have discovered that each samples time axis reliably follows a frequeny of 2.5Hz (assuming that is the setting), but is offset by a specific value given by the first value. Subtracting the first value from every value in the time axis column corrects the offset so that every observation is now at the same time value. Refer to [this notebook](./notebooks/determining_time_axis_offset.ipynb) for the specifics and proof.

### Higher dimensional dataframe plotting

To plot 2D data in a high-dimensional (multiindex columns) dataframe, need to shed the higher level labels to be able to refer to the specific columns, i.e. 'mins', and 'value'. In a pipe:

``` python
...
.groupby(['level1','level2',...,'leveln'],axis=1)
.apply(lambda grp: display(grp.droplevel(axis=1, level=['level1','level2',...,'leveln']).plot(x='colx',y='coly')))
...
```

## 2023-08-29

### Time Series Characterization and Compression

2023-08-29 10:50:22 My experiments to characterize the time axis of my dataset and develop some unification methods has resulted in a OOP API for time axis unification [here](src/wine_analysis_hplc_uv/signal_processing/mindex_signal_processing.py). The report can be found [here](notebooks/time_series_characterization_and_compression/time_axis_characterisation_and_compression.ipynb). The long and short of it is that all the time series require a small amount of adjustment prior to higher level processing, and that it is feasible that a 80% compression size across the datasets can be achieved.

2023-08-29 10:58:22 In the interest of speeding up development, I should consider processing the entire dataset and storing it in a seperate database file, as the compression will drastically increase extraction time. In the meantime I will continue with my adaption of the peak alignment module to mindex format.

2023-08-29 10:58:54 I was developing the baseline subtraction method, however the limited size of the sample data meant that my baselines were generally negative, resulting in unexpected changes once the baseline was subtracted. With the development of a downsampling [protocol](./notebooks/time_series_characterization_and_compression/downsampling_signals.ipynb) I should produce a csv file with the downsampled signals (or parquet) for use in testing. This will match the overall behavior of the data better than a subset.

## 2023-08-30

### Stack datatype conversion

2023-08-30 10:57:20

When working within a multiindex environment, it gets very tiresome to be constantly pivoting between tidy and long formats. The 'stack' methods are a bit odd in how they stack inside out, so to speak, so its required me to reorder and sort the column indexes every time i go from long to tidy:

``` python
df = (
      df
      .unstack(["wine", "samplecode"])
      .reorder_levels(["samplecode", "wine", "vars"], axis=1)
      .sort_index(axis=1)
      .reindex(["mins", "value"], level=2, axis=1)
        )
```

I thought I'd found a way of shortcutting it by a series of stacks and unstacks on different levels of the column index:

``` python
.stack(['vars']).unstack(['samplecode','wine','vars']) # from long to tidy
```

Except that as my datatypes became more sophisticated, columns belonging to the same level took on different types. Specifically, the 'vars' level contained 'mins', and 'value'. Initially, 'mins' and 'value' were floats, so there was no problem with datatype compatibility, however mins is now `timedelta`, and 'value' has remained a float. The issue is, that `.stack[('vars')]` puts the 'mins' and 'value' elements in the same column with alternating indexes, and pandas is forced to typecast them both to `object` dtype. As this is the final operation possible, the only alternative would be to iterate through each multiindex and typecast them back to their preferred format. It seems that the first option is the necessary one.

### Seaborn plotting

Seaborn is built around long format data. For all kinds of data, ensure that 1 there is 1 column for x, 1 column for y, and 1+ column for categories.

## 2023-09-04

### Project Organization

2023-09-04 12:21:45

Yet again I'm having problems with project organization. Long story short I want a method of adding verticality to my EDA processes so that state changes of the data can be kept track of and ordered. To give an example, I've applied a number of processes to the data including correcting offsets, smoothing, scaling etc, but without a top-level method of tracking (and justifying) these steps, then as the number of processes (and code) increases, it will become proportionally more difficult to both track and formalize these processes/decisions. In the past I have tried to establish a thesis homepage, however the problem with these approaches is that unless you are constantly present and editing the homepage, current efforts rapidly become detached from there, making it effortsome to reestablish synchronicity. The solution I will take today will be to establish a chapter level notebook that will display the results of efforts created in other pythoon/notebook files. For example, for the preprocesing phase I will establish a chapter notebook that summarises the investigation including establishing the test data set, the ordered processing steps etc. In the meantime, it will merely link to the relevent low-level notebook, but we can experiment later with embedding ala qmd. Also, To ensure that there is clean and organized code, all but the basest operations should be stored in python files rather than written in notebooks themselves, then imported into the notebook - notebook is to display tables and figures, not the code (in this context).

## 2023-09-26

### Combining All Logbook Files

I have decided to combine all existant logbook files into one monolith, namely this file. I have achieved this with the second monolithic file created in August, but am now left to deal with the obsidian single file per day files. Luckily they follow a template of sorts, so it should not be overly difficult. I should break down the directory into blocks based on common features then see how to handle each. I should also start manually to get a feel for the process to automate, and whether I will need to.

1.  Get the date from the file name/creation date field.
2.  Get the content lying between "Activities" and "Daily Summary"

Landmarks:

"\# Logbook"

## 2023-10-11

![Wine Database Schema](../../../../../001_obsidian_vault/Excalidraw/wine_database_schema.excalidraw.png)

<!-- end_file -->