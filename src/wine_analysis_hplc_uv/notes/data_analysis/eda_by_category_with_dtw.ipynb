{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Signal EDA by Wine Category\"\n",
    "format: html\n",
    "cdt: 2023-10-18T00:00:00\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back: [Thesis](src/wine_analysis_hplc_uv/notebooks/thesis.qmd) Subheading Profile Descriptions by Category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will contain a description of the signal profiles of each wine category by detection method. The intent is to provide an intuition into how models will behave when exposed to these categories, and see how similar they may or may not be. The hypothesis is that within each selected category there will be significant correlation, but also sufficient variance to uniquely identify each sample. The working hypothesis is also that DTW with a Sakoe-Chiba band of window size 10 will enable alignment without extraneous mutation.\n",
    "\n",
    "We will start with the varietal category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this method, we need to connect all the separate pipes that resulted in the dataset used to test DTW. As per @bos_2020 [1679], the stages are:\n",
    "\n",
    "1. denoise and smooth\n",
    "2. baseline correction\n",
    "3. retention time alignment\n",
    "4. peak deonvolution and resolution enhancement\n",
    "5. data compression\n",
    "\n",
    "Now, our signals are sufficiently quiet and smooth to skip the first step, and we're not interested yet in stage 4, and stage 5. is achieved through resampling. So, we need:\n",
    "\n",
    "1. get data\n",
    "2. resampling\n",
    "3. baseline correction\n",
    "\n",
    "All relevant methods are in mindex_signal_processing SignalProcessor but no full pipeline method has been established yet. Lets build one with a single sample as the test subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from wine_analysis_hplc_uv.old_signal_processing.signal_processor import (\n",
    "    SignalProcessor,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from wine_analysis_hplc_uv.notebooks import eda_by_category_methods\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\": 100})\n",
    "\n",
    "plotter = eda_by_category_methods.Plotting()\n",
    "\n",
    "scipro = SignalProcessor()\n",
    "\n",
    "data = pd.read_parquet(definitions.RAW_PARQ_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pipe(scipro.propipe)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.pipe(scipro.most_correlated, \"blinesub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pipe(scipro.dynamic_time_warping, \"blinesub\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relplot_df = (\n",
    "    data.loc[:, pd.IndexSlice[:, :, :, [\"blinesub\", \"aligned\"]]]\n",
    "    .melt(ignore_index=False)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        winelabel=lambda df: df.role + \"_\" + df.samplecode + \"_\" + df.wine,\n",
    "        mins=lambda df: df.mins.dt.total_seconds() / 60,\n",
    "    )\n",
    "    .set_index(\"mins\")\n",
    "    .sort_index()\n",
    "    .loc[0.0:20.0, :]\n",
    ")\n",
    "\n",
    "rp = sns.relplot(\n",
    "    data=relplot_df,\n",
    "    x=\"mins\",\n",
    "    y=\"value\",\n",
    "    hue=\"subsignal\",\n",
    "    col=\"winelabel\",\n",
    "    col_wrap=2,\n",
    "    kind=\"line\",\n",
    "    errorbar=None,\n",
    "    height=3,\n",
    "    aspect=2,\n",
    ")\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "hm = plotter.alignment_heatmap(data, ax=ax, signal_label=\"blinesub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not actually clear if modifying the window size is having an effect on the alignment.. need a mathematical description of the alignment. Lets start with peak detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the peak indices for each sample, join the resulting series with the data and add\n",
    "# a sparse column containing values where peaks are detected by boolean masking\n",
    "peaks = (\n",
    "    data\n",
    "    # go to long form df for groupby operations\n",
    "    .melt(ignore_index=False)\n",
    "    # groupby 'samplecode' and select 'value' series\n",
    "    .groupby(\"samplecode\")[\"value\"]\n",
    "    # find peaks based on the given parameters, the peak index array is the zeroth\n",
    "    # element of the returned tuple\n",
    "    .apply(\n",
    "        lambda value: pd.Series(\n",
    "            signal.find_peaks(\n",
    "                value,\n",
    "            )[0]\n",
    "        )\n",
    "    )\n",
    "    # return to a frame\n",
    "    .to_frame(name=\"idx\")\n",
    "    # move 'samplecode' from index to column\n",
    "    .reset_index(\"samplecode\")\n",
    "    # add a column 'ispeak' that contains True values to be used to identify the peak\n",
    "    # elements after the join\n",
    "    .assign(ispeak=True)\n",
    "    # set the index as 'samplecode', 'idx' to prepare for join with the df\n",
    "    .set_index([\"samplecode\", \"idx\"])\n",
    ")\n",
    "display(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I index the original frame with the found peaks to get the peak values, and preferably assign a boolean peak column alongside the signal column?\n",
    "\n",
    "One method could be to massage the df into the same shape as the peak df, i.e. long, indices by group. once thats achieved a `where` call should enable me to mark the values corresponding to the indice and the samplecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    # convert the timedelta index to float for easier plotting\n",
    "    .pipe(lambda df: df.set_axis(axis=0, labels=df.index.total_seconds() / 60))\n",
    "    # slice to the aligned signal from 0 to 16 minutes\n",
    "    .loc[0:16, pd.IndexSlice[:, :, :, \"aligned\"]]\n",
    "    # go long form for groupby operations\n",
    "    .melt(ignore_index=False)\n",
    "    # add a incremental index to match the find_peaks results\n",
    "    .assign(idx=lambda df: df.groupby(\"samplecode\").cumcount())\n",
    "    # set the index as samplecode and the newly formed idx\n",
    "    .reset_index()\n",
    "    .set_index([\"samplecode\", \"idx\"])\n",
    "    # left join based on samplecode and idx\n",
    "    .join(peaks, on=[\"samplecode\", \"idx\"])\n",
    "    # any rows who did not have a corresponding peak element are NaN, now filled with False\n",
    "    .assign(ispeak=lambda df: df.ispeak.fillna(False))\n",
    "    # add a 'peak' column that is equal to the peak value using 'ispeak' as a mask on 'value'\n",
    "    .assign(peak=lambda df: df[\"value\"].loc[df.ispeak])\n",
    "    # go to default index\n",
    "    .reset_index()\n",
    "    # add an overlay plot of the signals and their peaks\n",
    "    .pipe(\n",
    "        lambda df: so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "        .layout()\n",
    "        .add(so.Line(), y=\"value\")\n",
    "        .add(so.Dot(), y=\"peak\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from @scipy_findpeaks_2023: \"a peak or local maximum is defined as any sample whose two direct neighbours have a smaller amplitude.\" It notes that noisy signals can result in errors due to loss of information about local maxima. In these cases they recommend using `find_peaks_cwt`, or exploring smoothing options.\n",
    "\n",
    "Considering the results of above, perhaps it would be a good idea to experiment with smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above prototyped code has been wrapped in a class and placed in 'eda_by_category_methods'\n",
    "\n",
    "processor = eda_by_category_methods.Processing()\n",
    "\n",
    "\n",
    "def find_peaks_1(df, kwargs: None) -> pd.Series:\n",
    "    a = signal.find_peaks(df.signal, **kwargs)[0]\n",
    "    peaks = df.iloc[a].signal\n",
    "    return peaks\n",
    "\n",
    "\n",
    "data = data.loc[lambda df: df.signal_label == \"blinecorr\"].assign(\n",
    "    peaks=lambda df: df.groupby(\"samplecode\", group_keys=False).apply(find_peaks_1)\n",
    ")\n",
    "\n",
    "\n",
    "data.pipe(processor.find_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation of [chromatograpR](https://ethanbass.github.io/chromatographR/articles/chromatographR.html#pre-processing-data) we could use some smoothing, especially on 176, which is coincidentally our selected reference. But what is smoothing? Also, They use parametric time awrping or variable penalty dynamic time warping for alignment. They then use 'complete-linkage hierarchical clustering' to link peaks across samples.\n",
    "\n",
    "Unfortunately there does not seem to be a Python library that implements penalty dynamic time warping, so lets focus on smoothing for now.\n",
    "\n",
    "As per previous studies, first base is a Savitzky-Golay filter, which is implemented by SciPy. @cuadros-rodr√≠guez_2021 used a 5 point window and second order polynomial. The SciPy implementation is a 1D filter that requires the data, a window length and polyorder. There are also a number of other parameters. It returns an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "\n",
    "data = pd.read_parquet(definitions.RAW_PARQ_PATH)\n",
    "f = processor.process(data)\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, no effect. The smoothing necessary to remove those detected peaks will result in unsatisfactory loss of signal information. Ergo better to use constraints in the peak detection algo. Also, remaining in simple long form with no multiindex massively reduces reshaping overhead and makes UDF functions much simpler to define..\n",
    "\n",
    "Now lets add kwargs for peak finder.. added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)\n",
    "pro_data = processor.process(data, find_peak_kwargs=dict(height=6))\n",
    "\n",
    "pro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so.Plot(pro_data.loc[lambda df: df.mins < 21], x=\"mins\", color=\"samplecode\").add(\n",
    "    so.Line(), y=\"signal\"\n",
    ").add(so.Dot(), y=\"peaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we're only interested in the top 10 peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 peaks per sample\n",
    "pro_data = pro_data.assign(\n",
    "    select_peaks=lambda df: df.groupby(\"samplecode\", group_keys=False)[\n",
    "        \"peaks\"\n",
    "    ].nlargest(10)\n",
    ")\n",
    "\n",
    "# plot the peaks on top of the curves\n",
    "(\n",
    "    pro_data.loc[lambda df: df.mins < 21]\n",
    "    .pipe(\n",
    "        lambda df: (\n",
    "            df\n",
    "            if so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "            .add(so.Line(), y=\"signal\")\n",
    "            .add(so.Dot(), y=\"select_peaks\")\n",
    "            else df\n",
    "        )\n",
    "    )\n",
    "    # peak table\n",
    "    .pipe(\n",
    "        lambda df: (\n",
    "            df\n",
    "            if display(\n",
    "                df.loc[:, [\"samplecode\", \"wine\", \"mins\", \"select_peaks\"]]\n",
    "                .dropna()\n",
    "                .assign(n_peak=lambda df: df.groupby(\"samplecode\").cumcount())\n",
    "                .pivot(\n",
    "                    columns=[\"samplecode\", \"wine\"],\n",
    "                    index=[\"n_peak\"],\n",
    "                    values=[\"mins\", \"select_peaks\"],\n",
    "                )\n",
    "                .reorder_levels([1, 2, 0], axis=1)\n",
    "                .sort_index(axis=1)\n",
    "            )\n",
    "            else df\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now align, get the top 10 peaks again, and compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dtw\n",
    "\n",
    "# find reference\n",
    "def reference(df):\n",
    "    reference = (\n",
    "        df.corr().mean().loc[lambda df: df == df.max()]\n",
    "        # .pipe(scipro.dynamic_time_warpingi)\n",
    "    )\n",
    "    return reference.index\n",
    "\n",
    "\n",
    "reference = reference(\n",
    "    pro_data.pivot_table(\n",
    "        columns=[\"samplecode\", \"wine\"], index=[\"mins\"], values=\"signal\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align\n",
    "\n",
    "ref_signal = pro_data.loc[\n",
    "    pro_data.samplecode == reference.get_level_values(\"samplecode\")[0]\n",
    "].signal.reset_index(drop=True)\n",
    "\n",
    "(\n",
    "    pro_data.set_index([\"mins\"])\n",
    "    .groupby(\"samplecode\", group_keys=False)[\"signal\"]\n",
    "    .apply(scipro.align_query_to_ref, ref_signal)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023-10-18 Due to the fact that I am prototyping a lot of code and changing approaches (multiindex column default to long defaul primarily), literate progrmaming style is failing, re. version one of this notebook is somewhat unsalvagable - a testament to why rushing always fails in the end. At some point I'll have to go through and fix it. in the meantime, copy code over and push ON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set up environment\n",
    "import pandas as pd\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "import seaborn.objects as so\n",
    "from wine_analysis_hplc_uv.old_signal_processing.signal_processor import (\n",
    "    SignalProcessor,\n",
    ")\n",
    "from wine_analysis_hplc_uv.notes.data_analysis import eda_by_category_methods\n",
    "\n",
    "plotter = eda_by_category_methods.Plotting()\n",
    "\n",
    "scipro = SignalProcessor()\n",
    "data = pd.read_parquet(definitions.RAW_PARQ_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, no effect. The smoothing necessary to remove those detected peaks will result in unsatisfactory loss of signal information. Ergo better to use constraints in the peak detection algo. Also, remaining in simple long form with no multiindex massively reduces reshaping overhead and makes UDF functions much simpler to define..\n",
    "\n",
    "Now lets add kwargs for peak finder.. added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time std and drop vars level\n",
    "dtwprocess = eda_by_category_methods.DTWProcessing()\n",
    "sdata = dtwprocess.std_time(data)\n",
    "sdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth\n",
    "\n",
    "smdata = dtwprocess.smooth(sdata, \"samplecode\", \"signal\", \"smooth_signal\")\n",
    "smdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bline\n",
    "\n",
    "bcdata = dtwprocess.blinecorr(\n",
    "    smdata, \"samplecode\", \"smooth_signal\", bcorr_label=\"bcorr\"\n",
    ")\n",
    "\n",
    "bcdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot baseline correction\n",
    "\n",
    "(\n",
    "    bcdata.melt(\n",
    "        id_vars=[\"samplecode\", \"wine\", \"mins\"], var_name=\"siglabel\", value_name=\"sig\"\n",
    "    ).pipe(\n",
    "        lambda df: so.Plot(data=df, x=\"mins\", y=\"sig\", color=\"siglabel\")\n",
    "        .facet(col=\"samplecode\", wrap=2)\n",
    "        .add(mark=so.Line())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect peaks prior to alignment\n",
    "\n",
    "peaks = eda_by_category_methods.Peaks()\n",
    "\n",
    "pddata = peaks.peakdetect(\n",
    "    bcdata, grouper=\"samplecode\", signal_label=\"bcorr\", col_label=\"unaligned_peaks\"\n",
    ")\n",
    "\n",
    "\n",
    "pddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to display peaks\n",
    "(\n",
    "    pddata.melt(\n",
    "        id_vars=[\"samplecode\", \"wine\", \"mins\"], var_name=\"siglabel\", value_name=\"sig\"\n",
    "    )\n",
    "    .loc[lambda df: (df.mins < 5) & (df.mins > 3)]\n",
    "    .pivot_table(index=[\"samplecode\", \"wine\", \"mins\"], columns=\"siglabel\", values=\"sig\")\n",
    "    .reset_index()\n",
    "    .pipe(\n",
    "        lambda df: so.Plot(data=df, x=\"mins\", color=\"samplecode\")\n",
    "        .add(so.Line(), y=\"bcorr\")\n",
    "        .add(so.Dot(), y=\"unaligned_peaks\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we're only interested in the top 20 peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 20 peaks of the unaligned set\n",
    "\n",
    "tpdata = peaks.top_peaks(\n",
    "    pddata, \"samplecode\", \"unaligned_peaks\", 20, \"top_20_unaligned\"\n",
    ")\n",
    "tpdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the peaks on top of the curves\n",
    "\n",
    "peak_y = \"top_20_unaligned\"\n",
    "display(\n",
    "    tpdata.loc[lambda df: df.mins < 21].pipe(\n",
    "        lambda df: (\n",
    "            so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "            .add(so.Line(), y=\"bcorr\")\n",
    "            .add(so.Dot(), y=peak_y)\n",
    "            .plot()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# peak table\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks.peak_table(tpdata, [\"samplecode\", \"wine\"], \"top_20_unaligned\", \"peak_n\", \"mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now align, get the top 10 peaks after alignment and compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: Use transform with a custom function if you are expecting a result with the same shape as the input i.e. a column of the dataframe. Apply rarely gets this right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref = ApplyDTW().find_ref(tdata, 'samplecode','bcorr')\n",
    "\n",
    "dtwalign = eda_by_category_methods.ApplyDTW()\n",
    "\n",
    "# aligned data\n",
    "adata = dtwalign.align(\n",
    "    tpdata, primary_key=\"samplecode\", siglabel=\"bcorr\", aligned_label=\"aligned\"\n",
    ")\n",
    "adata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot aligned dataset\n",
    "\n",
    "(\n",
    "    adata.loc[lambda df: df.mins < 20].pipe(\n",
    "        lambda df: so.Plot(data=df, x=\"mins\", color=\"samplecode\")\n",
    "        .layout(size=(5, 3))\n",
    "        .add(so.Line(), y=\"aligned\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot aligned against unaligned set\n",
    "(\n",
    "    adata.loc[lambda df: df.mins < 20]\n",
    "    .melt(\n",
    "        id_vars=[\"mins\", \"samplecode\", \"wine\"],\n",
    "        var_name=\"siglabel\",\n",
    "        value_vars=[\"bcorr\", \"aligned\"],\n",
    "        value_name=\"sig_val\",\n",
    "    )\n",
    "    .pipe(\n",
    "        lambda df: so.Plot(data=df, x=\"mins\", color=\"siglabel\")\n",
    "        .layout(size=(15, 10))\n",
    "        .add(so.Line(), y=\"sig_val\")\n",
    "        .facet(col=\"samplecode\", wrap=2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect peaks in aligned set\n",
    "# aligned peaks data\n",
    "apddata = peaks.peakdetect(\n",
    "    adata, \"samplecode\", signal_label=\"aligned\", col_label=\"peaks_aligned\"\n",
    ")\n",
    "\n",
    "apddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of top 20 peaks in aligned peak set\n",
    "# top aligned peaks data\n",
    "tappdata = apddata.assign(\n",
    "    aligned_top_20=lambda df: df.groupby(\"samplecode\", group_keys=False)[\n",
    "        \"peaks_aligned\"\n",
    "    ].nlargest(20)\n",
    ")\n",
    "tappdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and display peak table\n",
    "\n",
    "aligned_peak_table = tappdata.pipe(\n",
    "    peaks.peak_table,\n",
    "    grouper=[\"samplecode\", \"wine\"],\n",
    "    peaks_label=\"aligned_top_20\",\n",
    "    peak_num_label=\"peak_n\",\n",
    "    peak_idx_label=\"mins\",\n",
    ")\n",
    "aligned_peak_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay signals and top 20 peaks\n",
    "\n",
    "(\n",
    "    tappdata.loc[lambda df: df.mins < 20].pipe(\n",
    "        lambda df: so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "        .add(so.Line(), y=\"aligned\")\n",
    "        .add(so.Dot(), y=\"aligned_top_20\")\n",
    "        .facet(col=\"samplecode\", wrap=2)\n",
    "        .layout(size=(15, 10))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overlay aligned set, unaligned peaks, aligned peaks as dots\n",
    "\n",
    "plot_peak_data = tappdata.melt(\n",
    "    id_vars=[\"mins\", \"samplecode\", \"wine\"],\n",
    "    value_vars=[\"aligned_top_20\", \"top_20_unaligned\"],\n",
    "    var_name=\"peak_label\",\n",
    "    value_name=\"sig\",\n",
    ").dropna()\n",
    "\n",
    "(\n",
    "    tappdata.loc[lambda df: df.mins < 20].pipe(\n",
    "        lambda df: so.Plot(df, x=\"mins\")\n",
    "        .add(so.Line(), y=\"aligned\", color=\"samplecode\")\n",
    "        .add(\n",
    "            so.Dot(),\n",
    "            data=plot_peak_data.loc[lambda df: df.mins < 20],\n",
    "            alpha=\"peak_label\",\n",
    "            color=\"samplecode\",\n",
    "            x=\"mins\",\n",
    "            y=\"sig\",\n",
    "        )\n",
    "        # .facet(col='samplecode',wrap=2)\n",
    "        .layout(size=(15, 10))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above graphic, The majority of the peaks have lost amplitude, and torbreck-struie has straight up lost a peak at 18 mins. We need a better DTW algorithm.\n",
    "\n",
    "Need to develop a more reliable alignment method. But we also should look at whether torbreck-struie itself is the issue. First what happens if we exclude it during warping?\n",
    "\n",
    "Continue this in another book. export the processed data to parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tappdata.loc[:, [\"mins\", \"samplecode\", \"wine\", \"signal\", \"bcorr\"]].to_parquet(\n",
    "    \"/Users/jonathan/mres_thesis/wine_analysis_hplc_uv/src/wine_analysis_hplc_uv/notebooks/processed_data.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "\n",
    "2023-10-19\n",
    "\n",
    "Looking at the effects of DTW over categories. Specifically, we're seeing an uncomfortable amount of warping in the most deviated samples, including significant losses of prominant peaks, which is an unacceptable mutation. Regardless of whether it is specific to that one sample (torbreck-struie) or not, we dont want to see any loss of prominant peaks, or peak mutation whatsoever. We are only expecting movement in the X, not the y.\n",
    "\n",
    "Peak mutation is caused by masking with the warping path, where the data point with a given peaks maxima is excluded, resulting in a peak mutation, generally a decline in peak maxima. What it boils down to is a need for a warping algorithm that is punished for not aligning peak maxima. We could first experiment with the base DTW algorithm but the literature documentation @ethanbass_2023 shows that there are better algorithms out there, I just need to access them. Re: ChromatographR.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
