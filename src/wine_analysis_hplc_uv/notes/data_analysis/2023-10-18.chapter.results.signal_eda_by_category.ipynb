{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Signal EDA by Wine Category\"\n",
    "format: html\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back: [Thesis](src/wine_analysis_hplc_uv/notebooks/thesis.qmd) Subheading Profile Descriptions by Category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will contain a description of the signal profiles of each wine category by detection method. The intent is to provide an intuition into how models will behave when exposed to these categories, and see how similar they may or may not be. The hypothesis is that within each selected category there will be significant correlation, but also sufficient variance to uniquely identify each sample. The working hypothesis is also that DTW with a Sakoe-Chiba band of window size 10 will enable alignment without extraneous mutation.\n",
    "\n",
    "We will start with the varietal category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this method, we need to connect all the separate pipes that resulted in the dataset used to test DTW. As per @bos_2020 [1679], the stages are:\n",
    "\n",
    "1. denoise and smooth\n",
    "2. baseline correction\n",
    "3. retention time alignment\n",
    "4. peak deonvolution and resolution enhancement\n",
    "5. data compression\n",
    "\n",
    "Now, our signals are sufficiently quiet and smooth to skip the first step, and we're not interested yet in stage 4, and stage 5. is achieved through resampling. So, we need:\n",
    "\n",
    "1. get data\n",
    "2. resampling\n",
    "3. baseline correction\n",
    "\n",
    "All relevant methods are in mindex_signal_processing SignalProcessor but no full pipeline method has been established yet. Lets build one with a single sample as the test subject. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from wine_analysis_hplc_uv import definitions\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from wine_analysis_hplc_uv.old_signal_processing.signal_processor import (\n",
    "    SignalProcessor,\n",
    ")\n",
    "\n",
    "sns.set_theme(rc={\"figure.dpi\": 100})\n",
    "import matplotlib.pyplot as plt\n",
    "from wine_analysis_hplc_uv.notebooks import eda_by_category_methods\n",
    "\n",
    "plotter = eda_by_category_methods.Plotting()\n",
    "\n",
    "scipro = SignalProcessor()\n",
    "\n",
    "data = pd.read_parquet(definitions.RAW_PARQ_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pipe(scipro.propipe)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.pipe(scipro.most_correlated, \"blinesub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.pipe(scipro.dynamic_time_warping, \"blinesub\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relplot_df = (\n",
    "    data.loc[:, pd.IndexSlice[:, :, :, [\"blinesub\", \"aligned\"]]]\n",
    "    .melt(ignore_index=False)\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        winelabel=lambda df: df.role + \"_\" + df.samplecode + \"_\" + df.wine,\n",
    "        mins=lambda df: df.mins.dt.total_seconds() / 60,\n",
    "    )\n",
    "    .set_index(\"mins\")\n",
    "    .sort_index()\n",
    "    .loc[0.0:20.0, :]\n",
    ")\n",
    "\n",
    "rp = sns.relplot(\n",
    "    data=relplot_df,\n",
    "    x=\"mins\",\n",
    "    y=\"value\",\n",
    "    hue=\"subsignal\",\n",
    "    col=\"winelabel\",\n",
    "    col_wrap=2,\n",
    "    kind=\"line\",\n",
    "    errorbar=None,\n",
    "    height=3,\n",
    "    aspect=2,\n",
    ")\n",
    "fig, ax = plt.subplots(1)\n",
    "\n",
    "hm = plotter.alignment_heatmap(data, ax=ax, signal_label=\"blinesub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not actually clear if modifying the window size is having an effect on the alignment.. need a mathematical description of the alignment. Lets start with peak detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.signal\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the peak indices for each sample, join the resulting series with the data and add\n",
    "# a sparse column containing values where peaks are detected by boolean masking\n",
    "peaks = (\n",
    "    data\n",
    "    # go to long form df for groupby operations\n",
    "    .melt(ignore_index=False)\n",
    "    # groupby 'samplecode' and select 'value' series\n",
    "    .groupby(\"samplecode\")[\"value\"]\n",
    "    # find peaks based on the given parameters, the peak index array is the zeroth\n",
    "    # element of the returned tuple\n",
    "    .apply(\n",
    "        lambda value: pd.Series(\n",
    "            signal.find_peaks(\n",
    "                value,\n",
    "            )[0]\n",
    "        )\n",
    "    )\n",
    "    # return to a frame\n",
    "    .to_frame(name=\"idx\")\n",
    "    # move 'samplecode' from index to column\n",
    "    .reset_index(\"samplecode\")\n",
    "    # add a column 'ispeak' that contains True values to be used to identify the peak\n",
    "    # elements after the join\n",
    "    .assign(ispeak=True)\n",
    "    # set the index as 'samplecode', 'idx' to prepare for join with the df\n",
    "    .set_index([\"samplecode\", \"idx\"])\n",
    ")\n",
    "display(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I index the original frame with the found peaks to get the peak values, and preferably assign a boolean peak column alongside the signal column?\n",
    "\n",
    "One method could be to massage the df into the same shape as the peak df, i.e. long, indices by group. once thats achieved a `where` call should enable me to mark the values corresponding to the indice and the samplecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    # convert the timedelta index to float for easier plotting\n",
    "    .pipe(lambda df: df.set_axis(axis=0, labels=df.index.total_seconds() / 60))\n",
    "    # slice to the aligned signal from 0 to 16 minutes\n",
    "    .loc[0:16, pd.IndexSlice[:, :, :, \"aligned\"]]\n",
    "    # go long form for groupby operations\n",
    "    .melt(ignore_index=False)\n",
    "    # add a incremental index to match the find_peaks results\n",
    "    .assign(idx=lambda df: df.groupby(\"samplecode\").cumcount())\n",
    "    # set the index as samplecode and the newly formed idx\n",
    "    .reset_index()\n",
    "    .set_index([\"samplecode\", \"idx\"])\n",
    "    # left join based on samplecode and idx\n",
    "    .join(peaks, on=[\"samplecode\", \"idx\"])\n",
    "    # any rows who did not have a corresponding peak element are NaN, now filled with False\n",
    "    .assign(ispeak=lambda df: df.ispeak.fillna(False))\n",
    "    # add a 'peak' column that is equal to the peak value using 'ispeak' as a mask on 'value'\n",
    "    .assign(peak=lambda df: df[\"value\"].loc[df.ispeak])\n",
    "    # go to default index\n",
    "    .reset_index()\n",
    "    # add an overlay plot of the signals and their peaks\n",
    "    .pipe(\n",
    "        lambda df: so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "        .layout()\n",
    "        .add(so.Line(), y=\"value\")\n",
    "        .add(so.Dot(), y=\"peak\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from @scipy_findpeaks_2023: \"a peak or local maximum is defined as any sample whose two direct neighbours have a smaller amplitude.\" It notes that noisy signals can result in errors due to loss of information about local maxima. In these cases they recommend using `find_peaks_cwt`, or exploring smoothing options.\n",
    "\n",
    "Considering the results of above, perhaps it would be a good idea to experiment with smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above prototyped code has been wrapped in a class and placed in 'eda_by_category_methods'\n",
    "\n",
    "processor = eda_by_category_methods.Processing()\n",
    "\n",
    "\n",
    "def find_peaks_1(df, kwargs: None) -> pd.Series:\n",
    "    a = signal.find_peaks(df.signal, **kwargs)[0]\n",
    "    peaks = df.iloc[a].signal\n",
    "    return peaks\n",
    "\n",
    "\n",
    "data = data.loc[lambda df: df.signal_label == \"blinecorr\"].assign(\n",
    "    peaks=lambda df: df.groupby(\"samplecode\", group_keys=False).apply(find_peaks_1)\n",
    ")\n",
    "\n",
    "\n",
    "data.pipe(processor.find_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the documentation of [chromatograpR](https://ethanbass.github.io/chromatographR/articles/chromatographR.html#pre-processing-data) we could use some smoothing, especially on 176, which is coincidentally our selected reference. But what is smoothing? Also, They use parametric time awrping or variable penalty dynamic time warping for alignment. They then use 'complete-linkage hierarchical clustering' to link peaks across samples.\n",
    "\n",
    "Unfortunately there does not seem to be a Python library that implements penalty dynamic time warping, so lets focus on smoothing for now.\n",
    "\n",
    "As per previous studies, first base is a Savitzky-Golay filter, which is implemented by SciPy. @cuadros-rodr√≠guez_2021 used a 5 point window and second order polynomial. The SciPy implementation is a 1D filter that requires the data, a window length and polyorder. There are also a number of other parameters. It returns an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "\n",
    "data = pd.read_parquet(definitions.RAW_PARQ_PATH)\n",
    "f = processor.process(data)\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, no effect. The smoothing necessary to remove those detected peaks will result in unsatisfactory loss of signal information. Ergo better to use constraints in the peak detection algo. Also, remaining in simple long form with no multiindex massively reduces reshaping overhead and makes UDF functions much simpler to define..\n",
    "\n",
    "Now lets add kwargs for peak finder.. added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)\n",
    "pro_data = processor.process(data, find_peak_kwargs=dict(height=6))\n",
    "\n",
    "pro_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so.Plot(pro_data.loc[lambda df: df.mins < 21], x=\"mins\", color=\"samplecode\").add(\n",
    "    so.Line(), y=\"signal\"\n",
    ").add(so.Dot(), y=\"peaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes, we're only interested in the top 10 peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 peaks per sample\n",
    "pro_data = pro_data.assign(\n",
    "    select_peaks=lambda df: df.groupby(\"samplecode\", group_keys=False)[\n",
    "        \"peaks\"\n",
    "    ].nlargest(10)\n",
    ")\n",
    "\n",
    "# plot the peaks on top of the curves\n",
    "(\n",
    "    pro_data.loc[lambda df: df.mins < 21]\n",
    "    .pipe(\n",
    "        lambda df: (\n",
    "            df\n",
    "            if so.Plot(df, x=\"mins\", color=\"samplecode\")\n",
    "            .add(so.Line(), y=\"signal\")\n",
    "            .add(so.Dot(), y=\"select_peaks\")\n",
    "            else df\n",
    "        )\n",
    "    )\n",
    "    # peak table\n",
    "    .pipe(\n",
    "        lambda df: (\n",
    "            df\n",
    "            if display(\n",
    "                df.loc[:, [\"samplecode\", \"wine\", \"mins\", \"select_peaks\"]]\n",
    "                .dropna()\n",
    "                .assign(n_peak=lambda df: df.groupby(\"samplecode\").cumcount())\n",
    "                .pivot(\n",
    "                    columns=[\"samplecode\", \"wine\"],\n",
    "                    index=[\"n_peak\"],\n",
    "                    values=[\"mins\", \"select_peaks\"],\n",
    "                )\n",
    "                .reorder_levels([1, 2, 0], axis=1)\n",
    "                .sort_index(axis=1)\n",
    "            )\n",
    "            else df\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now align, get the top 10 peaks again, and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply dtw\n",
    "\n",
    "# find reference\n",
    "def reference(df):\n",
    "    reference = (\n",
    "        df.corr().mean().loc[lambda df: df == df.max()]\n",
    "        # .pipe(scipro.dynamic_time_warpingi)\n",
    "    )\n",
    "    return reference.index\n",
    "\n",
    "\n",
    "reference = reference(\n",
    "    pro_data.pivot_table(\n",
    "        columns=[\"samplecode\", \"wine\"], index=[\"mins\"], values=\"signal\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align\n",
    "\n",
    "ref_signal = pro_data.loc[\n",
    "    pro_data.samplecode == reference.get_level_values(\"samplecode\")[0]\n",
    "].signal.reset_index(drop=True)\n",
    "\n",
    "(\n",
    "    pro_data.set_index([\"mins\"])\n",
    "    .groupby(\"samplecode\", group_keys=False)[\"signal\"]\n",
    "    .apply(scipro.align_query_to_ref, ref_signal)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
