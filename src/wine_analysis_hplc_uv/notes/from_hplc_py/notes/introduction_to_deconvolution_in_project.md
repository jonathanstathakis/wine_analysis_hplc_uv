---
date: 2024-10-04
---

## Introduction To Deconvolution Project

A HPLC UV/Vis signal is a simple and rapid method of profiling an organic sample. It requires minimal sample preparation or user training and the instrument is robust and inexpensive when compared with similar tools. It produces signals of the individual compounds in a sample with very high SNR.

However, A drawback of the method is that due to a number of factors, reproducibility is low, namely due to baseline drift and peak shifting. Baseline drift amplifies the apparent intensity of signals, and can vary in its shape along the time dimension, and run to run. Peak shift describes the movement of a peak backwards or forewards along the time axis, potentially resulting in peaks swapping their order. A third complication is convolution. As detectors observe the seperated stream from one direction, two peaks closely eluting will overlap, producing a convoluted signal. In practice, this results in an increase in intensity of the individual peaks as well as fronting/tailing as their ends are artificially magnified. Baseline drift and convolution can both be solved by signal processing and numerical analysis.

To study the chromatographic signatures across different samples, we need to be able to identify peaks across samples, and accurately measure their area. The aforementioned phenonmenea render the signals useless in their raw state, and as such we are required to process them until we can isolate each peak. Once the signals are purified and the peaks isolated, we can label them by their presence within a time interval. Across the samples, a peak within an interval is labelled the same. This does however require three assumptions - 1. that peaks will never move further than the length of a bin, 2. that the peaks will not move between bins, and 3. that the peaks will not swap. As we are lacking a means of identifying a peak beyond its retention time, these are the assumptions we have to make. It is not difficult to see that these assumptions introduce the potential for mislabeling of peaks, but presumably, downstream statistical analysis can identify outliers for reassesment.

On the topic of peak labeling, I intend to investigate whether the spetral dimension, once sufficiently resolved, can be used to distinguish compounds. If true, this would enable us to determine whether peak shifting / swap has occured.

Signal processing is the manipulation of a signal through the application of algorithms such as filters and optimizers in order to remove background noise (including baseline adjustment), and improve resolution of features (such as peaks). It tends to consist of a number of stages in a specific order, as a given algorithm might perform better after an enhancement peformed by a prior step. A stage might even be repeated one or more times throughout the process - for example, it is recommended to smooth a signal initially, and again after a resolution enhancement step, as they tend to magnify noise as much as the signal. When processing a chromatographic signal we aim for a signal whose peaks are perfectly resolved from each other, and with a zero value baseline between peaks.

Overall, there is an art to processing complex signals, as each signal has its own character, which makes it difficult to automate. Generally speaking, A number of baseline correction, smoothing, and resolution enhancement stages may be applied prior to user input. It should also be noted that there is a vast number of algorithms for each processing action, due inpart because signals are complex, but also because signal processing is ubi quitous across any field that deals with observations over time or time-like dimensions. Thus time must be spent becoming familiar with the tools available and determining whether they are suitable for the problem at hand.

Of the three processing stages, baseline drift is generally tackled first, as resolution enhancement and deconvolution algorithms benefit (or require) a baseline at or approaching zero. It is generally solved by estimating the background and subtracting it from the instrument signal. A number of methods exist for solving this problem, including manually selecting he baseline, fitting minima with least squares (ALS, BEADS), or smoothing the signal until the signal until peaks are obliterated (SNIP). TODO: continue to provide information regarding baseline corection methods..

Resolution enhancement describes the application of filters to exagerate a peaks center and narrow its sides, producing a peak more gaussian in nature and distinct from the baseline and neighbouring peaks. Methods include: TODO: fill in resolution enhancemnt methods..

Unfortunately, due to the chemical similarity of compounds in a mixture, which is to be expected in real-life samples, some peaks may remain obscured, or convolved. One method of solving this is through the fitting of a model function to the convoluted signal. First a model, such as Gaussian, Lorentzian, or Skew-Normal, is selected based on the shape of an isolated peak. It is necessary that the model can be defined using parameters observable in the signal, such as peak width, height, and location. If we then define a series of convoluted peaks as the sum of peaks of the given model, then by identifying the parameters of the model that produce the observed summation, we can model each peak independently using those derived parameters. For example, the Skew-Normal Distribution has four parameters - location, height, half-height width and skew. If two peaks are convoluted, we are required to fit 8 parameters in total. In real world scenarios it is untenable to find the parameters manually, so typically they are found through least-squares curve fitting. Once each peaks model parameters are found, the peaks are reconstructed from the model, and stored as independent signals. Goodness-of-fit can be assessed by comparing the summation of the individual peak signals and the original signal. As fitting as signal consisting of many peaks is computationally taxing, it is ideal to break the signal up into windows and fitting each one at a time. This has the benefit of speeding the fitting up considerably, allows the user to observe the intermediate results, and lets the algorithm fit signals sharing similar features, meaning the algorithm may not need to search as far in the parameter space.

What follows is a number of exercises getting to grips with the problem of deconvolution and signal processing.
