{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressing Chromatographic Signals through Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is of interest to compress my dataset in order to increase development iterations - progress is proportional to the number of development iterations, the time between iterations is proportional to the size of the dataset. We want to reduce the time between iterations, ergo reduce the size of the dataset whereever possible.\n",
    "\n",
    "One method of signal compression through downsampling, a form of data compression as a reduction in frequency of observation of a signal [@nielsen_2019]. It is necessary to have a high fidelity detector, as without it peaks may only be partially detected - or not detected at all, however during data processing, we don't need every datapoint, just the most important information from a given interval. Thus multiple datapoints can be summarized through an aggregate function such as the average for an interval. This results in a compressed signal while preserving the overall shape of the dataset. A compromise has to reached between compression and granularity. One negative side-effect is that true peak heights can be lost when averaging out, and the overal intensity tends toward the total intensity mean as the number of observations decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wine_analysis_hplc_uv import definitions\n",
    "import duckdb as db\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wine_analysis_hplc_uv.db_methods import get_data, pivot_wine_data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "con = db.connect(definitions.DB_PATH)\n",
    "get_data.get_wine_data(\n",
    "    con, wavelength=(\"450\",), detection=(\"cuprac\",), varietal=(\"shiraz\",)\n",
    ")\n",
    "df = pivot_wine_data.pivot_wine_data(con)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert mins to timedelta\n",
    "\n",
    "First the time axis needs to be of timedelta datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.loc[:, \"154\"]\n",
    "    .stack([\"wine\"])\n",
    "    .drop([\"id\", \"detection\"], axis=1)\n",
    "    .reset_index()\n",
    "    .drop(\"i\", axis=1)\n",
    "    .assign(mins=lambda df: pd.to_timedelta(df[\"mins\"], unit=\"min\").round(\"L\"))\n",
    "    .assign(mins=lambda df: df.mins - df.mins.iloc[0])\n",
    "    .set_index([\"wine\", \"mins\"])\n",
    "    .unstack(\"wine\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Resample API\n",
    "\n",
    "Pandas handles resampling of datasets through the `resample` API which relies on a datetime column to apply binning and aggregation operations to the remaining columns.\n",
    "\n",
    "The resample frequency is controlled by the `rule` argument which takes an offset string / object ([see docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)). These strings are called 'offset aliases' and are defined in the [Time series / date functionality](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases) page. They describe a base level of atomization of a time series, and can accept arithmatic manipulations in the string (need to confirm, at least accept multiplication). For example, `'min'` = `'1min'` is minutely, and will downsample the signal to 1 observation per minute, `'0.5min'` will downsample it to one observation every half minute.\n",
    "\n",
    "## Calculating Sampling Ratios\n",
    "\n",
    "So to for example reduce the 40 minute, 6000 observation signal we start with, (6000 obs/40 min = 150 obs / 1 min = 150 obs per minute) by 20%, resulting in 120 observations per minute, (4800 observations)\n",
    "\n",
    "For example, we could reduce the dataset by a ratio of 1:6, which has been selected because the peak maxima lasts for approximately 6 observations:\n",
    "\n",
    "## Relationship Between Observations per Second and Sampling Frequency\n",
    "\n",
    "Note: converting minutes to seconds then calculating the number of observations per second = frequency (Hz). The reciporical is the frequency in seconds, ergo 1/2.5 = 0.4 seconds = 400 milliseconds. Observe below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean sampling frequency for sample 157 by calculating the reciprocal of the mean difference in time elements\n",
    "\n",
    "mean_sampling_frequency = (\n",
    "    1 / df.reset_index(\"mins\").mins.dt.total_seconds().diff().mean()\n",
    ")\n",
    "mean_sampling_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, it is important to regularize the frequency. Due to systemic error, variations in the frequency can occur. Rectifying these errors can be achieved by calculating the mean sampling frequency, resampling the dataset to that frequency then interpolating the missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample to the mean sampling frequency then interpolate the missing data, then plot\n",
    "\n",
    "df = df.resample(f\"{1/np.round(mean_sampling_frequency,3)}S\").interpolate(\n",
    "    method=\"linear\"\n",
    ")\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to Interval of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per previous discussion, the region of interest is between 0 to 20 minutes, thus we will subset the signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to 0 - 20 mins, plot result\n",
    "df = df.loc[: pd.to_timedelta(20, unit=\"minutes\")]\n",
    "df.plot()\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is a generally much more interesting region of the signal, and provides a vaguely symmetric baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying an Appropriate Target Frequency\n",
    "\n",
    "Generally speaking, choosing a target frequency is an arbitrary choice. Remembering the criteria that we should reduce memory size while preserving the overall shape of the dataset. At a bare minimum, a peak consists of three data points, a start point, a maxima, and an end point. Therefore observing the change in the number of data points making up the maxima peak will indicate the lower limit of sampling frequency of this dataset. Specifically, I will set it as the lowest frequency in which 3 data points are present in the top 10% intensity value of the peak maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties of the original signal are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a statistical description of df, display the points within 90% of the peak maxima\n",
    "\n",
    "display(\n",
    "    df.stack([\"wine\"])\n",
    "    .pipe(lambda df: df.loc[df[\"value\"] >= df[\"value\"].max() * 0.9])\n",
    "    .pipe(\n",
    "        lambda df: df\n",
    "        if df.reset_index()\n",
    "        .assign(mins=lambda df: df.mins.dt.total_seconds() / 60)\n",
    "        .plot.scatter(x=\"mins\", y=\"value\", title=\"points within 90% of maxima\")\n",
    "        else df\n",
    "    )\n",
    "    .pipe(lambda df: df if display(df.describe()) else df)  # display df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling to a frequency of 0.5 Hz, or 1 observation every 2 seconds results in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample signal to 1 observation every 2 seconds, from 1 every 0.4\n",
    "\n",
    "df = df.assign(\n",
    "    value_2S=lambda df: df.groupby([\"wine\"], axis=1)\n",
    "    .resample(\"2S\")\n",
    "    .interpolate(method=\"linear\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_2S.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "display interval of peak maxima within 10% of peak after resampling to 2S intervals,\n",
    "both statistics and a plot\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    df.stack([\"wine\"]).pipe(\n",
    "        lambda df: df.loc[df.value_2S >= df.value_2S.max() * 0.9, \"value_2S\"]\n",
    "        .pipe(\n",
    "            lambda df: df\n",
    "            if df.reset_index()\n",
    "            .assign(mins=lambda df: df.mins.dt.total_seconds() / 60)\n",
    "            .plot.scatter(x=\"mins\", y=\"value_2S\")\n",
    "            else df\n",
    "        )\n",
    "        .pipe(lambda df: df if display(df.describe()) else df)  # display df\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That meets the defined criteria, but how dos it compare to the original signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the original signal and downsampled signal as row-wise subplots\n",
    "\n",
    "fig, axs = plt.subplots(2, 1)\n",
    "ax = df[\"value\"].plot.line(ax=axs[0]).legend(labels=[\"original\"])\n",
    "ax = df[\"value_2S\"].interpolate().plot.line(ax=axs[1]).legend(labels=[\"downsampled\"])\n",
    "plt.suptitle(\"original and downsampled signals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = {\n",
    "    \"original_signal\": (df.value.memory_usage() / 1024).sum(),\n",
    "    \"2S_downsample\": df.value_2S.dropna().memory_usage(index=True) / 1024,\n",
    "}\n",
    "\n",
    "(\n",
    "    pd.DataFrame(_, index=[\"memory_usage (KB)\"])\n",
    "    .assign(\n",
    "        diff=lambda df: df[\"original_signal\"] - df[\"2S_downsample\"],\n",
    "    )\n",
    "    .assign(perc_diff=lambda df: df[\"diff\"] / df.original_signal * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that downsampling to 2S decreases the series memory size by 91.6%, after removing NA rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Change Due to Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is not a visual change, I need to be able to summarize how the signal has changed, or at least the magnitude of difference between the original and downsampled signals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Downsampled Signal To Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: 2023-09-06 13:21:14 this has actually been solved by assigning the downsampled series back to the original frame rather than creating a new index. This section is kept for prosterity, but is now superfluous.\n",
    "\n",
    "To measure the euclidean distance of two signals, I need to perform vector arithmatic. One of the fundamental rules of vector arithmatic is that the vectors cannot be of different length. The shorter vector needs to be mapped to the longer one before a comparison can be made. One method of doing this could be to upsample the downsampled signal back to the original frequency of 0.4 seconds per observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop this method we will use a signal downsampled to a sampling frequency of 1 per 30 seconds in order to have a much more obvious difference between the source and destination signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a Downsampled Series to the Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a comparison of the original time series and the same series downsampled to 30 seconds per observation then upsampled back to 2 seconds per obsveration via linear interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_2S_df = df.value_2S.dropna()\n",
    "upsampled = df.value_2S.resample(\"30S\").interpolate().resample(\"2S\").interpolate()\n",
    "display(value_2S_df)\n",
    "display(upsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(2, 1)\n",
    "\n",
    "value_2S_df.plot(ax=axs2[0]).legend(labels=[\"2 Second sampling f\"])\n",
    "upsampled.plot(ax=axs2[1]).legend(labels=[\"30 second sampling f\"])\n",
    "plt.suptitle(\"Comparison of 2 and 30 second sampling frequencies\")\n",
    "display(pd.concat([value_2S_df.describe(), upsampled.describe()], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            value_2S_df.reset_index()[\"mins\"].agg(min=\"min\", max=\"max\", count=\"count\"),\n",
    "            upsampled.reset_index()[\"mins\"].agg(min=\"min\", max=\"max\", count=\"count\"),\n",
    "        ],\n",
    "        names=[\"a\", \"b\"],\n",
    "        axis=1,\n",
    "    )\n",
    "    .set_axis([\"2S\", \"30S\"], axis=1)\n",
    "    .rename_axis(\"freq\", axis=1)\n",
    "    .rename_axis(\"agg\")\n",
    ")\n",
    "compare[\"diff\"] = compare[\"2S\"] - compare[\"30S\"]\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there appears to be a discrepency in the resulting array lengths due to the positioning of the bins. The lower I downsample, the shorter the resulting resampled time series is. At default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [\"2S\", \"4S\", \"8S\", \"16S\", \"32S\"]\n",
    "length = []\n",
    "for freq in freqs:\n",
    "    length.append(\n",
    "        value_2S_df.resample(freq).interpolate().resample(\"2S\").interpolate().shape[0]\n",
    "    )\n",
    "\n",
    "ax = plt.scatter(freqs, length)\n",
    "plt.xlabel(\"sampling frequency\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.suptitle(\"sampling freq. v. resulting array length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 14 time points are missing on upsampling. Which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_2S_df.index.difference(upsampled.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So its literally the last 30 seconds that have been shaved off. When does that happen? Could it be during the first downsampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    value_2S_df.resample(\"30S\")\n",
    "    .interpolate()\n",
    "    .reset_index()\n",
    "    .agg(\n",
    "        count=(\"mins\", lambda x: x.count()),\n",
    "        first=(\"mins\", lambda x: x.iloc[0]),\n",
    "        last=(\"mins\", lambda x: x.iloc[-1]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the downsampling. Its to do with the binning. the last x is compressed into a single data point on the left side of the bin. I think thats an unavoidable consequence of downsampling. So we will need to instead manually add the remaining data points. Or trim the original signal. Or ffill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.DataFrame.shift()` can be used to move a time series by periods, for example a shift of 15 periods will get the dataframe to 20 minutes. Forming the union of the shifted and original series will produce a time series index that starts at 0 and ends at 20. Reindexing the downsampled series then filling the missing values will result in a downsampled series that has the same number of elements as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_2S_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_upsample(df: pd.DataFrame, new_freq: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downsample then upsample back to the original frequency, shift to match original\n",
    "    timeframe, then fill so that the two time series can be compared.\n",
    "\n",
    "    Takes a dataframe with a time series index and a offset alias new frequency. Outputs\n",
    "    the new sampled dataframe.\n",
    "    \"\"\"\n",
    "    # Time object (in this project typically Second) of input dataframe timeseries index\n",
    "    ofreq = df.index.freq\n",
    "    # downsampled-upsampled time series dataframe.\n",
    "    # downsampled to 'new_freq' then upsampled again to match original dataframe length\n",
    "    downupdf = df.resample(new_freq).interpolate().resample(ofreq.freqstr).interpolate()\n",
    "    # difference between input dataframe.index and updowndf.index\n",
    "    diff = df.index.difference(downupdf.index)\n",
    "    # extend new dataframe index as union of index and difference, reindex new dataframe\n",
    "    # and fill NA data from extension with last value\n",
    "    out_df = downupdf.reindex(downupdf.index.union(diff)).ffill()\n",
    "\n",
    "    return out_df\n",
    "\n",
    "\n",
    "downsample_upsample(df.value, \"30S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create a calibration curve following how the euclidean distance changes for increasing downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [\"2S\", \"4S\", \"8S\", \"16S\", \"32S\", \"64S\"]\n",
    "\n",
    "downsamples = dict()\n",
    "for freq in freqs:\n",
    "    ndf = downsample_upsample(df.value, freq)\n",
    "    downsamples[freq] = ndf\n",
    "\n",
    "downsample_df = pd.concat(downsamples, axis=1, names=[\"freq\"])\n",
    "downsample_df.droplevel(\"wine\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "dist_df = (\n",
    "    downsample_df.droplevel(\"wine\", axis=1)\n",
    "    .stack([\"freq\"])\n",
    "    .groupby(\"freq\")\n",
    "    .agg(\n",
    "        edist=lambda x: euclidean(df.value.values.flatten(), x.values.flatten()),\n",
    "    )\n",
    "    .sort_index()\n",
    "    .pipe(lambda df: df.set_axis(pd.Categorical(df.index, categories=freqs)))\n",
    "    .sort_index()\n",
    "    .rename_axis(\"freq_str\")\n",
    "    .rename_axis(\"labels\", axis=1)\n",
    ")\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df[\"freq_val\"] = dist_df.index.str.replace(\"S\", \"\").astype(\"int\")\n",
    "dist_df = dist_df.sort_values(\"freq_val\")\n",
    "dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "x = dist_df.freq_val.values.reshape(-1, 1)\n",
    "y = dist_df.edist.values.reshape(-1, 1)\n",
    "linreg = LinearRegression(fit_intercept=False).fit(x, y=y)\n",
    "dist_df[\"fit\"] = linreg.predict(x)\n",
    "\n",
    "(\n",
    "    dist_df.reset_index()[\n",
    "        #  .set_index('freq_val')\n",
    "        [\"edist\", \"fit\"]\n",
    "    ].plot(ax=ax)\n",
    ")\n",
    "display(dist_df)\n",
    "\n",
    "(dist_df.reset_index().plot.scatter(x=\"freq_str\", y=\"edist\", ax=ax))\n",
    "plt.suptitle(\"downsampling freq. v. euclidean distance\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an $R^2$ of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there is a vaguely exponential change for increasing levels of downsamples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a stacked display of increasing downsampling rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_cats = pd.Categorical(\n",
    "    downsample_df.columns.get_level_values(\"freq\"), categories=freqs, ordered=True\n",
    ")\n",
    "freq_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "One method of setting freq to categorical is to go to long format, modify it there\n",
    "then going back to tidy. Can see that it worked by reversing the order as defined.\n",
    "\"\"\"\n",
    "\n",
    "downsample_df = (\n",
    "    downsample_df.stack([\"wine\", \"freq\"])\n",
    "    .reset_index(name=\"value\")\n",
    "    .assign(freq=lambda df: pd.Categorical(df.freq, categories=freqs, ordered=True))\n",
    "    .set_index([\"wine\", \"freq\", \"mins\"])\n",
    "    .rename_axis(axis=1, mapper=\"v\")\n",
    "    .unstack([\"wine\", \"freq\"])\n",
    "    .reorder_levels([\"wine\", \"freq\", \"v\"], axis=1)\n",
    "    .sort_index(level=\"freq\", axis=1, ascending=False)\n",
    ")\n",
    "downsample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the value axis to be within 0 and 1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "downsample_df = downsample_df.apply(\n",
    "    lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1, 1)).flatten()\n",
    ")\n",
    "\n",
    "downsample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "add a scalar offset to each freq column so that they will be spread out on a line plot.\n",
    "This is done by first copying downsample_df and then iterate through every freq column \n",
    "and add an incrementing scalar value relative to the column name idx.\n",
    "\"\"\"\n",
    "\n",
    "offset_downsample_df = downsample_df.copy(deep=True).sort_index()\n",
    "\n",
    "for i, col in enumerate(downsample_df.columns.get_level_values(\"freq\")):\n",
    "    offset_downsample_df[\n",
    "        lambda df: (df.columns.get_level_values(\"wine\").values[0], col)\n",
    "    ] = downsample_df[\n",
    "        lambda df: (df.columns.get_level_values(\"wine\").values[0], col)\n",
    "    ] + 0.6 * (i + 1)\n",
    "\n",
    "\n",
    "offset_downsample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "display the offset line plots of increasingly downsampled signal. Dont display\n",
    "x and y axes because the offset causes the y axis to be meaningless, x axis is in\n",
    "wrong units.\n",
    "\"\"\"\n",
    "\n",
    "(\n",
    "    offset_downsample_df.stack([\"wine\", \"freq\"]).pipe(\n",
    "        lambda df: sns.lineplot(df, x=\"mins\", y=\"value\", hue=\"freq\", alpha=0.9).set(\n",
    "            xticklabels=[], yticklabels=[], ylabel=\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "plt.suptitle(\"increasingly downsampled signal from 2S to 64S\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which provides an intuitive understanding of the relationship between downsampling frequency and euclidean distance. This means that euclidean distance is an acceptable (if not truly linear) method of gauging the difference between a signal and its downsampled form. This is a bulk summary though, and does not provide any information about local changes. We need to go further and find methods of measuring local changes such as smoothness, local amplitude, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This study set out with the intention of developing a method of compressing a signal through downsampling, wanted to gauge the positive and negative effects. During this investigation I found that while the mean frequency may be 2.5Hz, the are local variations that require an initial resampling to the mean frequency to smooth out. I also noted that after t=20 mins, the signal has no interest, so subsetting the signal to that point is a good choice, whatsmore it provides a relatively symmetrical baseline for later corrections. After investigation of peak resolution, I identified an appropriate downsample frequency of 2 seconds per observation, and found that there was no visual difference to the signal while saving more than 90% computer memory. Finally, I showed that Euclidean distance is an appropriate measure of the bulk magnitude of change between a source and downsampeld signal, with a weakly exponential positive trend for decreasing frequency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine-analysis-hplc-uv-F-SbhWjO-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
