{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulating Past Experiments\n",
    "\n",
    "continuing from (the last notebook)[./2023-03-28-joining-cellartracker-metadata.ipynb], I have been attempting to analyse how my collection of samples is going, but my records are muddled. I need to form a table that contains information on what samples have been run on the 10cm column, and what was the quality of the chromatogram.\n",
    "\n",
    "For simplicities sake, we will focus on the 255nm spectra, and define qualtiy as a high ratio of peak prominance to area under baseline. It would be useful at this point to tidy up Agilette and add baseline, peak, and correction functions to that codebase.\n",
    "\n",
    "Theoretically, each run should contain the sample id. That should be sufficient to create the table to join.\n",
    "\n",
    "However, I should also scour my logbook to make any other corrections. Best way to do that would be to form a table of notes sorted by date with the contents of each note and a uri link to the note. That should be done in a seperate notebook tho. In this one:\n",
    "\n",
    "1. Load all experiments as a pandas table with:\n",
    "['acq_date', 'sample_name', 'method_name', 'data']\n",
    "2. fit the baseline.\n",
    "3. Calculate the peak prominance / area under baseline, display in table.\n",
    "4. Sort by ratio, see if this is a sensible approach.\n",
    "5. Observe, make conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "parent_path = Path.cwd().parent\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(parent_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading all data files in given path\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39magilette\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magilette_core\u001b[39;00m \u001b[39mimport\u001b[39;00m Library\n\u001b[0;32m----> 3\u001b[0m lib \u001b[39m=\u001b[39m Library(Path(\u001b[39m'\u001b[39;49m\u001b[39m/Users/jonathan/0_jono_data/\u001b[39;49m\u001b[39m'\u001b[39;49m), [\u001b[39m'\u001b[39;49m\u001b[39mall_data_files\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      4\u001b[0m lib\n",
      "File \u001b[0;32m~/wine_analysis_hplc_uv/agilette/agilette_core.py:304\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[0;34m(self, path, runs_to_load)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m runs_to_load:\n\u001b[1;32m    303\u001b[0m     d_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mglob(\u001b[39m'\u001b[39m\u001b[39m**/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name)\n\u001b[0;32m--> 304\u001b[0m     d_path \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(d_path)\n\u001b[1;32m    305\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_runs[d_path\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m Run_Dir(d_path)\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from agilette.agilette_core import Library\n",
    "\n",
    "lib = Library(Path('/Users/jonathan/0_jono_data/'), ['all_data_files'])\n",
    "lib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I currently have an elaborate function called all_data() which is written in such a way so as to handle duplicate .D files. But why? Because I was putting all the names into a dict. that's dumb. No .D file should be a duplicate because they will all have different filepaths, at least. And what's more, their metadata will intrinsically vary.\n",
    "\n",
    "Actually, bugger that. Just assemble a list of Run_Dir objects then assemble the dataframe from that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
