{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Joining Cellartracker Metadata with Sample Tracker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this project is to prototype getting metadata for my wines, and for a list from the sample tracker table, inner join to the sample tracker table, then perform EDA on the join.\n",
    "\n",
    "First, get the sample tracker table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_sheets_api import get_sheets_values_as_df\n",
    "\n",
    "def sample_tracker_df_builder():\n",
    "\n",
    "    df = get_sheets_values_as_df(\n",
    "        spreadsheet_id='15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY',\n",
    "        range='sample_tracker!A1:H200',\n",
    "        creds_parent_path=os.path.join(os.getcwd(),'credentials_tokens'),\n",
    "        )\n",
    "    return df\n",
    "\n",
    "sample_tracker_df = sample_tracker_df_builder()\n",
    "\n",
    "print(sample_tracker_df.columns)\n",
    "\n",
    "sample_tracker_df = sample_tracker_df[['id','vintage', 'name', 'size', 'open_date', 'sample_date', 'variety', 'notes']]\n",
    "\n",
    "import dtale\n",
    "\n",
    "dtale.show(sample_tracker_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should be able to join on the name alone, through the use of a fuzzy match. But first, let's get the cellartracker table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellartracker import cellartracker\n",
    "\n",
    "def get_cellar_tracker_table():\n",
    "\n",
    "    client = cellartracker.CellarTracker('OctaneOolong', 'S74rg4z3r1')\n",
    "\n",
    "    usecols = ['Size', 'Vintage', 'Wine', 'Locale', 'Country', 'Region', 'SubRegion', 'Appellation', 'Producer', 'Type', 'Color', 'Category', 'Varietal']\n",
    "\n",
    "    cellar_tracker_df = pd.DataFrame(client.get_list())\n",
    "\n",
    "    cellar_tracker_df = cellar_tracker_df[usecols]\n",
    "\n",
    "    # clean it up. lower values and columns, replace 1001 with nv, check datatypes\n",
    "\n",
    "    cellar_tracker_df = cellar_tracker_df.apply(lambda x : x.str.lower() if str(x) else x)\n",
    "    cellar_tracker_df.columns = cellar_tracker_df.columns.str.lower()\n",
    "    cellar_tracker_df = cellar_tracker_df.rename({'wine' : 'name'}, axis = 1)\n",
    "\n",
    "    cellar_tracker_df = cellar_tracker_df.replace({'1001' : 'nv'})\n",
    "\n",
    "    return cellar_tracker_df\n",
    "    \n",
    "cellar_tracker_df = get_cellar_tracker_table()\n",
    "\n",
    "dtale.show(cellar_tracker_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! Now to join them. We should form a join col made up of the vintage and name in both tables."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to chatgpt, \"Pr&#333;ter&#333;\" instead of \"prōterō\" is due to being encoded as a 'html character entity', and I can use the HTML module to 'unescape' the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def unescape_html(s):\n",
    "    return html.unescape(s)\n",
    "\n",
    "cellar_tracker_df = cellar_tracker_df.applymap(unescape_html)\n",
    "\n",
    "dtale.show(cellar_tracker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_join_col(df):\n",
    "    df['join_key'] = df['vintage'] + \" \" + df['name']\n",
    "    return df\n",
    "\n",
    "sample_tracker_df = form_join_col(sample_tracker_df)\n",
    "cellar_tracker_df = form_join_col(cellar_tracker_df)\n",
    "sample_tracker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dfs_with_fuzzy(df1, df2):\n",
    "    from fuzzywuzzy import fuzz, process\n",
    "\n",
    "    def fuzzy_match(s1, s2):\n",
    "        return fuzz.token_set_ratio(s1, s2)\n",
    "    \n",
    "    df1['join_key_match'] = df1['join_key'].apply(lambda x: process.extractOne(x, df2['join_key'], scorer = fuzzy_match))\n",
    "\n",
    "    # the above code produces a tuple of: ('matched_string', 'match score', 'matched_string_indice'). Usually it's two return values, but using scorer=fuzzy.token_sort_ratio or scorer=fuzz.token_set_ratio returns the index as well.\n",
    "\n",
    "    df1['join_key_matched'] = df1['join_key_match'].apply(lambda x: x[0])\n",
    "    df1['join_key_similarity'] = df1['join_key_match'].apply(lambda x : x[1])\n",
    "\n",
    "    print(df1['join_key_similarity'][0])\n",
    "\n",
    "    df1.drop(columns = ['join_key_match'], inplace = True)\n",
    "\n",
    "    merged_df = pd.merge(df1, df2, left_on='join_key_matched', right_on='join_key')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "df = join_dfs_with_fuzzy(sample_tracker_df, cellar_tracker_df)\n",
    "\n",
    "# original attempt fails on: 'null mystery', '2021 criante c21 cataratto', '2020 dom. simha paysan fleur sauvage', '2021 billaud d'or bourgogne blanc', 'null empty id, missing wine', '2021 st hugo gsm', '? yangarra 'roux beaute' roussanne'.\n",
    "\n",
    "# I now want to have a look at all the incorrect matches and get theirx similarity scores. This should be simple enough, just need another column to keep the sim scores.\n",
    "\n",
    "pd.options.display.min_rows = 50\n",
    "\n",
    "\n",
    "dtale.show(df[['join_key_similarity','join_key_x', 'join_key_y']].sort_values('join_key_similarity'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, a similarity score below 55 is when the matches became false. Thus a simple fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dfs_with_fuzzy(df1, df2):\n",
    "    from fuzzywuzzy import fuzz, process\n",
    "\n",
    "    def fuzzy_match(s1, s2):\n",
    "        return fuzz.token_set_ratio(s1, s2)\n",
    "    \n",
    "    df1['join_key_match'] = df1['join_key'].apply(lambda x: process.extractOne(x, df2['join_key'], scorer = fuzzy_match))\n",
    "\n",
    "    # the above code produces a tuple of: ('matched_string', 'match score', 'matched_string_indice'). Usually it's two return values, but using scorer=fuzzy.token_sort_ratio or scorer=fuzz.token_set_ratio returns the index as well.\n",
    "\n",
    "    df1['join_key_matched'] = df1['join_key_match'].apply(lambda x: x[0] if x[1] > 65 else None)\n",
    "    df1['join_key_similarity'] = df1['join_key_match'].apply(lambda x : x[1] if x[1] > 65 else None)\n",
    "\n",
    "    df1.drop(columns = ['join_key_match'], inplace = True)\n",
    "\n",
    "    merged_df = pd.merge(df1, df2, left_on='join_key_matched', right_on='join_key', how = 'left')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "df = join_dfs_with_fuzzy(sample_tracker_df, cellar_tracker_df)\n",
    "\n",
    "dtale.show(df[['join_key_similarity','join_key_x', 'join_key_y']].sort_values('join_key_similarity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleanup(df):\n",
    "\n",
    "    df = df.drop(['vintage_x', 'name_x', 'join_key_x', 'join_key_matched', 'join_key_y', 'variety', 'size_y'], axis = 1)\n",
    "\n",
    "    df = df.rename({\n",
    "                    'size_x' : 'size',\n",
    "                    'vintage_y' : 'vintage',\n",
    "                    'name_y' : 'name'\n",
    "                    }, axis = 1)\n",
    "    \n",
    "    ordered_cols = ['id', 'vintage', 'name', 'size']\n",
    "\n",
    "    rest_of_cols = list(df.columns.drop(ordered_cols))\n",
    "\n",
    "    ordered_cols = ordered_cols + rest_of_cols\n",
    "    \n",
    "    df = df[ordered_cols]\n",
    "    \n",
    "    display(df)\n",
    "\n",
    "df_cleanup(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok looking good. Now to chuck it all into a google sheet.\n",
    "\n",
    "Let's call the sheet..\n",
    "\n",
    "sample_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "sheet_url = 'https://docs.google.com/spreadsheets/d/15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY/edit#gid=347137817'\n",
    " \n",
    "\n",
    "sheet_id = Path(sheet_url).parent.name\n",
    "\n",
    "sheet_title = 'sample_overview'\n",
    "\n",
    "creds_parent_path = Path.cwd() / 'credentials_tokens'\n",
    "\n",
    "def make_result_sheet(sheet_id, sheet_title, creds_parent_path):\n",
    "\n",
    "    from google_sheets_api import post_new_sheet\n",
    "\n",
    "    sheet_title = 'sample_overview'\n",
    "\n",
    "    creds_parent_path = Path.cwd() / 'credentials_tokens'\n",
    "\n",
    "    response = post_new_sheet(sheet_id, sheet_title, creds_parent_path)\n",
    "\n",
    "    display(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to post the results into the results sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_results(df, sheet_id, range, creds_parent_path):\n",
    "    from google_sheets_api import post_df_as_sheet_values\n",
    "\n",
    "    response = post_df_as_sheet_values(df, sheet_id, range, creds_parent_path)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "#post_results(df, sheet_id, f\"{sheet_title}!A1\", creds_parent_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, the next thing to do is to identify all samples which have been run on the avantor 10cm column. The most straightforward method would be to go back to your notes to check when you swapped over to it, since you haven't used another column since.\n",
    "\n",
    "On checking, it appears that the 10cm column was installed on the 02/02. Now, from the table above, some of the wines only have a recorded open date, and some only have a recorded sample date.\n",
    "\n",
    "To verify the sampling dates, we should go back to Agilette and see if we can form a table with the sample 'id' as the primary key, then join on that.\n",
    "\n",
    "Regardless, looks like there are a lot of samples dated from after the 2nd. That being said, I think a run of samples before the 20's were all compromised by time before they were run. Definitely need to review the experiments.\n",
    "\n",
    "That's enough in this note. Will continue in [the next notebook](./2023-03-28_tabulating-past-experiments.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
