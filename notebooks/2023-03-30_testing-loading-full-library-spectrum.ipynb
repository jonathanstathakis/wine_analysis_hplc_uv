{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investgating Data Collection Progress Thus Far\n",
    "\n",
    "i.e. 2023-03-30."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [2023-03-30_logbook](../../001_obsidian_vault/2023-03-31_logbook.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Runs are Appropriate for Analysis.\n",
    "\n",
    "Criteria: \n",
    "\n",
    "1. were on the Avantor 10cm column.\n",
    "2. contain UV spectra.\n",
    "3. Not uracil, acetone or coffee runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from agilette.modules.library import Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = Library('/Users/jonathan/0_jono_data')\n",
    "df = lib.metadata_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try selecting only for rows that contain UV data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_df_avantor_wine_subset(df : pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df[(df['uv_filenames']!='') & (df['acq_method'].str.contains('AVANTOR')) & ~(df['name'].str.contains('uracil')) & ~(df['name'].str.contains('coffee')) & ~(df['name'].str.contains('lor'))]\n",
    "\n",
    "    return df\n",
    "\n",
    "avantor_df = metadata_df_avantor_wine_subset(lib.metadata_table)\n",
    "\n",
    "avantor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70 unique samples doesn't seem too bad. There should be more to pull sitting in the instrument as well. It appears that this filter is legitimate, as there is UV data for every remaining row.\n",
    "\n",
    "Now, one sticking point is that we don't have the wine names in this table. To get them we will need to load the sample tracker as a df and merge them on the sample ID., or 'name' in this dataframe. A further complicating factor is that the names are sometimes not consistant with sample tracker, for example '2021-debortoli-cabernet-merlot_avantor`. I dont believe it was ever added, and it makes up 7/107 of the runs. Let's first load sample_tracker then compare their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_tracker() -> pd.DataFrame:\n",
    "\n",
    "    from google_sheets_api import get_sheets_values_as_df\n",
    "\n",
    "    sheet_id = '15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY'\n",
    "    path_to_creds = '/Users/jonathan/wine_analysis_hplc_uv/credentials_tokens'\n",
    "\n",
    "    tracker_df = get_sheets_values_as_df(sheet_id, \"sample_tracker!A1:H200\",path_to_creds)\n",
    "\n",
    "    return tracker_df\n",
    "\n",
    "tracker_df = get_sample_tracker()\n",
    "tracker_df = tracker_df.replace(\"\", np.nan)\n",
    "tracker_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making id/name Column the Same Prior to Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join `sample_tracker` with `metadata_table` I need to do the following:\n",
    "1. Ensure they are the same datatype.\n",
    "2. Clear whitespace.\n",
    "3. Sort.\n",
    "4. Drop duplicates.\n",
    "5. Compare using `equals()`. If not, continue.\n",
    "\n",
    "1. Find common elements using `isin()`.\n",
    "2. filter on `isin()`.\n",
    "3. compare filtered columns using `equals()`.\n",
    "\n",
    "Credits: ChatGPT.\n",
    "\n",
    "I don't quite know how `filter` or `isin` works, so let's check it out.\n",
    "\n",
    "## `isin`\n",
    "\n",
    "`pd.DataFrame.isin` takes `values` as its argument, which can be any iterable, Series, DataFrame or dict, and returns a DataFrame of booleans depending on matches.\n",
    "\n",
    "For the given `values`, if any of the elements match an element of the DataFrame, a value of True is marked in the output DataFrame at that coordinate.\n",
    "\n",
    "The idea is to return a mask that can be applied to the original DataFrame.\n",
    "\n",
    "## `filter`\n",
    "\n",
    "`pd.DataFrame.filter` subsets the DataFrame rows and/or columns with an index-oriented approach, and the `like` keyword argument allows you to match column or row names against substrings and regex patterns for psuedo fuzzy matching.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the 'id' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rename avantor 'name' to 'id' for consistancy.\n",
    "avantor_df = avantor_df.rename({'name' : 'id'}, axis = 1)\n",
    "\n",
    "# clean the columns up, sort and drop duplicates\n",
    "\n",
    "def dataframe_cleaner(df : pd.DataFrame, col_name : str) -> pd.DataFrame:\n",
    "    df[col_name] = df[col_name].str.strip()\n",
    "    df = df.sort_values(col_name)\n",
    "    df = df.drop_duplicates(col_name)\n",
    "    return df\n",
    "    \n",
    "# todo: add subsetter to pipeline.\n",
    "avantor_df = (avantor_df.pipe(dataframe_cleaner, col_name='id'))\n",
    "tracker_df = (tracker_df.pipe(dataframe_cleaner, col_name='id'))\n",
    "\n",
    "tracker_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form the link between `tracker_df` and `avantor_df`, I need to have an understanding of what happened when, and draw a link between sample dates, wine names, and ids. First though we can reconcile the formats of the id numbers. In tracker_df they have the format \"DD\" where D is a digit, but in avantor_df they are either empty, \"DD\" \"00DD\", or other. What methods are there of finding patterns in strings in a column?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Patterns in Strings in a Column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "for idx, row in tracker_df.iterrows():\n",
    "    print(row['id'], end=', ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tracker_df['id']` has pattern \"D\", \"DD\", or \"zD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in avantor_df.iterrows():\n",
    "    print(row['id'], end=', ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avantor DF starts with DDDD, goes to DD, then \"string\", DD, \"string\". Tbh there should be more DD after those, looks like im missing files? Anyway. To rectify this, we need to:\n",
    "1.  [x] pad tracker_df to be DD\n",
    "2. [x] drop the first and last digits on the first 19 rows of avantor df.\n",
    "3. replace the strings with DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 Pad Tracker_DF to be DD. Can use `pd.Series.str.pad`\n",
    "\n",
    "def pad_id(df : pd.DataFrame) -> pd.DataFrame:\n",
    "    return df['id'].str.pad(2, fillchar='0')\n",
    "\n",
    "tracker_df['id'] = pad_id(tracker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. subset first 19 rows of avantor df (identify that group, probs by acq_date), drop first and last digits of id through slicing.\n",
    "\n",
    "four_digit_ids = avantor_df[avantor_df['id'].str.len() ==4].sort_values('acq_date')\n",
    "\n",
    "# ranges from the 02-14 13:18:27 to 02-16 12:45:35. Is this the same as all values within that range?\n",
    "\n",
    "avantor_four_digit_id_range = avantor_df[(avantor_df['acq_date'] >= four_digit_ids.loc[four_digit_ids.index[0],'acq_date']) & (avantor_df['acq_date'] <= four_digit_ids.loc[four_digit_ids.index[-1],'acq_date'])].sort_values('acq_date')\n",
    "\n",
    "avantor_four_digit_id_range.equals(four_digit_ids)\n",
    "\n",
    "# confirmed that for those date ranges, all entries were four digit ids."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. replace the strings with DD.\n",
    "\n",
    "This is the most difficult. First find all which are names rather than numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_mask = avantor_df['id'].str.isdigit()\n",
    "avantor_df[~(avantor_df['id'].str.isdigit())].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_df[tracker_df['name'].str.contains('crawford')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline of Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df_date_sort = avantor_df.sort_values('acq_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "avantor_df_date_sort['id'] = avantor_df_date_sort['id'].fillna('none')\n",
    "\n",
    "fig = px.scatter(avantor_df_date_sort, x = 'acq_date', y = 'id')\n",
    "\n",
    "# show the figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty useful representation. Now we can start renaming the id rows.\n",
    "\n",
    "From this point, we will be disconnecting from the source material. This is not a problem as the filepaths to the data files, but any further work should be done on this table directly rather than reloading from the source material.\n",
    "\n",
    "This introduces the question of how to integrate new data in. A question that will be answered later!\n",
    "\n",
    "For now, let's get the ids reconciled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning IDs Across Tables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27c46f4eb3a1e072bb472673e0f5bc67d135295985dc85bce54a4088e8c57ef4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
