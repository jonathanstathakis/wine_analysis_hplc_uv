{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investgating Data Collection Progress Thus Far\n",
    "\n",
    "i.e. 2023-03-30."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to [2023-03-30_logbook](../../001_obsidian_vault/2023-03-31_logbook.md)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Runs are Appropriate for Analysis.\n",
    "\n",
    "Criteria: \n",
    "\n",
    "1. were on the Avantor 10cm column.\n",
    "2. contain UV spectra.\n",
    "3. Not uracil, acetone or coffee runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "\n",
    "from agilette.modules.library import Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib = Library('/Users/jonathan/0_jono_data')\n",
    "meta_df = lib.metadata_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try selecting only for rows that contain UV data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df = meta_df[(meta_df['uv_filenames']!='') & (meta_df['acq_method'].str.contains('AVANTOR')) & ~(meta_df['name'].str.contains('uracil')) & ~(meta_df['name'].str.contains('coffee')) & ~(meta_df['name'].str.contains('lor'))]\n",
    "\n",
    "avantor_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70 unique samples doesn't seem too bad. There should be more to pull sitting in the instrument as well. It appears that this filter is legitimate, as there is UV data for every remaining row.\n",
    "\n",
    "Now, one sticking point is that we don't have the wine names in this table. To get them we will need to load the sample tracker as a df and merge them on the sample ID., or 'name' in this dataframe. A further complicating factor is that the names are sometimes not consistant with sample tracker, for example '2021-debortoli-cabernet-merlot_avantor`. I dont believe it was ever added, and it makes up 7/107 of the runs. Let's first load sample_tracker then compare their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_tracker() -> pd.DataFrame:\n",
    "\n",
    "    from google_sheets_api import get_sheets_values_as_df\n",
    "\n",
    "    sheet_id = '15S2wm8t6ol2MRwTzgKTjlTcUgaStNlA22wJmFYhcwAY'\n",
    "    path_to_creds = '/Users/jonathan/wine_analysis_hplc_uv/credentials_tokens'\n",
    "\n",
    "    tracker_df = get_sheets_values_as_df(sheet_id, \"sample_tracker!A1:H200\",path_to_creds)\n",
    "\n",
    "    return tracker_df\n",
    "\n",
    "tracker_df = get_sample_tracker()\n",
    "tracker_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making id/name Column the Same Prior to Join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join `sample_tracker` with `metadata_table` I need to do the following:\n",
    "1. Ensure they are the same datatype.\n",
    "2. Clear whitespace.\n",
    "3. Sort.\n",
    "4. Drop duplicates.\n",
    "5. Compare using `equals()`. If not, continue.\n",
    "\n",
    "1. Find common elements using `isin()`.\n",
    "2. filter on `isin()`.\n",
    "3. compare filtered columns using `equals()`.\n",
    "\n",
    "Credits: ChatGPT.\n",
    "\n",
    "I don't quite know how `filter` or `isin` works, so let's check it out.\n",
    "\n",
    "## `isin`\n",
    "\n",
    "`pd.DataFrame.isin` takes `values` as its argument, which can be any iterable, Series, DataFrame or dict, and returns a DataFrame of booleans depending on matches.\n",
    "\n",
    "For the given `values`, if any of the elements match an element of the DataFrame, a value of True is marked in the output DataFrame at that coordinate.\n",
    "\n",
    "The idea is to return a mask that can be applied to the original DataFrame.\n",
    "\n",
    "## `filter`\n",
    "\n",
    "`pd.DataFrame.filter` subsets the DataFrame rows and/or columns with an index-oriented approach, and the `like` keyword argument allows you to match column or row names against substrings and regex patterns for psuedo fuzzy matching.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the 'id' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rename avantor 'name' to 'id' for consistancy.\n",
    "avantor_df = avantor_df.rename({'name' : 'id'}, axis = 1)\n",
    "\n",
    "# clean the columns up, sort and drop duplicates\n",
    "\n",
    "def dataframe_cleaner(df : pd.DataFrame, col_name : str) -> pd.DataFrame:\n",
    "    df[col_name] = df[col_name].str.strip()\n",
    "    df[col_name] = df.sort_values(col_name)\n",
    "    df[col_name] = df.drop_duplicates(col_name)\n",
    "    return df\n",
    "    \n",
    "avantor_df = (avantor_df.pipe(dataframe_cleaner, col_name='id'))\n",
    "# avantor_df['id'] = avantor_df['id'].str.strip()\n",
    "# avantor_df = avantor_df.sort_values('id').drop_duplicates('id')\n",
    "\n",
    "avantor_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a number of NaN's in the avantor_df, which is surprising. Presumably if there is no name in the xml file it is recorded as NaN after forming the dataframe. How many?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df[avantor_df['id'].isna()].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So out of 101, 31 have names that are are NaN, all the way through the dataset.\n",
    "\n",
    "I need an overview to get what's going on. A visualisation would be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avantor_df_date_sort = avantor_df.sort_values('acq_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "avantor_df_date_sort['id'] = avantor_df_date_sort['id'].fillna('none')\n",
    "\n",
    "fig = px.scatter(avantor_df_date_sort, x = 'acq_date', y = 'id')\n",
    "\n",
    "# show the figure\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty useful representation. Now we can start renaming the id rows.\n",
    "\n",
    "From this point, we will be disconnecting from the source material. This is not a problem as the filepaths to the data files, but any further work should be done on this table directly rather than reloading from the source material.\n",
    "\n",
    "This introduces the question of how to integrate new data in. A question that will be answered later!\n",
    "\n",
    "For now, let's get the ids reconciled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning IDs Across Tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 105\n",
    "tracker_df['id']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27c46f4eb3a1e072bb472673e0f5bc67d135295985dc85bce54a4088e8c57ef4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
